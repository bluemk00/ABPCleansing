{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38dfd7ac",
   "metadata": {},
   "source": [
    "#### 원저자의 cost 함수를 수정함 maximize p(x|z) 대신 minimize MSE 사용: lib.models_varia\n",
    "#### 원저자의 cost 함수는 lib.models\n",
    "\n",
    "#####  missing values 로 mask 되어있는 hat_Values만 선택되어 mse 및 map 가 적용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65af1ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "WARNING:tensorflow:From C:\\Users\\USER\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\mixed_precision\\loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from absl import app\n",
    "from absl import flags\n",
    "sys.path.append(\"..\")\n",
    "from lib.models_varia_mse_y_only_for_gp import *\n",
    "\n",
    "# Enable mixed precision for performance\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "\n",
    "## GPU selection\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "outdir = './results/GPVAE_newver2/'\n",
    "# DATA = np.load(\"../../2.ProcessedData/mimic_30s_sample.npy\")\n",
    "checkpoint_prefix = os.path.join(outdir, \"ckpt\")\n",
    "\n",
    "data_type = 'hmnist'\n",
    "testing = False\n",
    "num_steps = 0 # 'Number of training steps: If non-zero it overwrites num_epochs'\n",
    "num_epochs = 10000\n",
    "batch_size = 1500\n",
    "print_interval = 0\n",
    "TrRate = 0.8\n",
    "LatDim = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd9c0d6",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "360bd20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(283380, 3000) (70844, 3000)\n"
     ]
    }
   ],
   "source": [
    "Data = np.load(\"D:/Dropbox/AICleansing_ver2/2.ProcessedData/train_mimic_orgscale.npy\")\n",
    "# ValData = np.load(\"D:/Dropbox/AICleansing_ver2/2.ProcessedData/valid_vitaldb_orgscale.npy\")\n",
    "np.random.shuffle(Data)\n",
    "data_len = len(Data)\n",
    "ValData = Data[:data_len//5]\n",
    "TrData = Data[data_len//5:]\n",
    "print(TrData.shape, ValData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c9a21e",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74e7a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ver2\n",
    "\n",
    "TrDataFrame = tf.signal.frame(TrData.astype('float32'), 50, 50).numpy()\n",
    "ValDataFrame = tf.signal.frame(ValData.astype('float32'), 50, 50).numpy()\n",
    "\n",
    "np.random.shuffle(TrDataFrame)\n",
    "np.random.shuffle(ValDataFrame)\n",
    "\n",
    "m_train_miss = np.random.choice([1,0], size=TrDataFrame.shape, p=[0.1,0.9])\n",
    "m_val_miss = np.random.choice([1,0], size=ValDataFrame.shape, p=[0.1,0.9])\n",
    "\n",
    "Tr_X = TrDataFrame.copy()\n",
    "Tr_Y = (TrDataFrame - 20.0) / (220.0 - 20.0)\n",
    "# 평균 80, 표준 편차 25인 정규 분포에서 값을 뽑아서 채울 배열 생성\n",
    "random_values = np.random.normal(loc=80, scale=25, size=TrDataFrame.shape)\n",
    "# m_train_miss가 1인 위치에 뽑은 값을 할당\n",
    "Tr_X[m_train_miss == 1] = random_values[m_train_miss == 1]\n",
    "Tr_X = (Tr_X - 20.0) / (220.0 - 20.0)\n",
    "\n",
    "m_train_miss[:,-10:,:] = 1\n",
    "Tr_X[:,-10:,:] = Tr_X[:,-10:,:] + np.random.normal(loc=0.0, scale=0.05, size=Tr_X[:,-10:,:].shape)\n",
    "\n",
    "Tr_X = np.clip(Tr_X, 0.0, 1.0)\n",
    "\n",
    "Val_X = ValDataFrame.copy()\n",
    "Val_Y = (ValDataFrame - 20.0) / (220.0 - 20.0)\n",
    "# 평균 80, 표준 편차 25인 정규 분포에서 값을 뽑아서 채울 배열 생성\n",
    "random_values = np.random.normal(loc=80, scale=25, size=ValDataFrame.shape)\n",
    "# m_train_miss가 1인 위치에 뽑은 값을 할당\n",
    "Val_X[m_val_miss == 1] = random_values[m_val_miss == 1]\n",
    "Val_X = (Val_X - 20.0) / (220.0 - 20.0)\n",
    "\n",
    "m_val_miss[:,-10:,:] = 1\n",
    "Val_X[:,-10:,:] = Val_X[:,-10:,:] + np.random.normal(loc=0.0, scale=0.05, size=Val_X[:,-10:,:].shape)\n",
    "\n",
    "Val_X = np.clip(Val_X, 0.0, 1.0)\n",
    "\n",
    "data_dim = TrDataFrame.shape[-1]\n",
    "time_length = TrDataFrame.shape[1]\n",
    "tr_sig_nb = len(TrDataFrame)\n",
    "\n",
    "del random_values\n",
    "del TrDataFrame\n",
    "del ValDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c38de903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f74d2f7f10>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Tr_X[0].ravel())\n",
    "plt.plot(Tr_Y[0].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f45e91b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "tf_x_train_miss = (\n",
    "    tf.data.Dataset.from_tensor_slices((Tr_X, Tr_Y, m_train_miss))\n",
    "    .shuffle(tr_sig_nb)\n",
    "    .batch(batch_size)\n",
    "    .repeat()\n",
    "    .prefetch(AUTOTUNE)  # Add prefetching.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa190d1c",
   "metadata": {},
   "source": [
    "### Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f2f6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = BandedJointEncoderGRU\n",
    "decoder = GaussianDecoder\n",
    "\n",
    "\n",
    "model = GP_VAE(latent_dim=LatDim, \n",
    "               data_dim=data_dim, \n",
    "               time_length=time_length,\n",
    "               encoder_sizes=[100, 80, 60], encoder=encoder,\n",
    "               decoder_sizes=[60,80,100], decoder=decoder,\n",
    "               kernel='cauchy', \n",
    "               sigma=1.,\n",
    "               length_scale=0.1, \n",
    "               kernel_scales = 1,\n",
    "               #image_preprocessor=image_preprocessor, \n",
    "               window_size=10,\n",
    "               beta=0.1, M=1, K=1, \n",
    "               #data_type=data_type     \n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eb9002c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_2 (GRU)                  (1, 60, 100)              45600     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (1, 60, 80)               8080      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (1, 60, 60)               4860      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (1, 60, 30)               1830      \n",
      "=================================================================\n",
      "Total params: 60,370\n",
      "Trainable params: 60,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Encoder:  None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (1, 60, 60)               660       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (1, 60, 80)               4880      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (1, 60, 100)              8100      \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (1, 60, 50)               22800     \n",
      "=================================================================\n",
      "Total params: 36,440\n",
      "Trainable params: 36,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Decoder:  None\n"
     ]
    }
   ],
   "source": [
    "_ = tf.compat.v1.train.get_or_create_global_step()\n",
    "trainable_vars = model.get_trainable_vars()\n",
    "# optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3)\n",
    "\n",
    "print(\"Encoder: \", model.encoder.net.summary())\n",
    "print(\"Decoder: \", model.decoder.net.summary())\n",
    "\n",
    "# For global step\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "gradient_clip = 1e4\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# Initialize optimizer and learning rate scheduler\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Checkpoints\n",
    "checkpoint = {\n",
    "    \"optimizer\": optimizer,\n",
    "    \"encoder\": model.encoder.net,\n",
    "    \"decoder\": model.decoder.net,\n",
    "    \"global_step\": global_step\n",
    "}\n",
    "\n",
    "if model.preprocessor is not None:\n",
    "    print(\"Preprocessor: \", model.preprocessor.net.summary())\n",
    "    checkpoint[\"preprocessor\"] = model.preprocessor.net\n",
    "\n",
    "saver = tf.train.Checkpoint(**checkpoint)\n",
    "\n",
    "# TensorBoard\n",
    "summary_writer = tf.summary.create_file_writer(outdir+'log/', flush_millis=10000)\n",
    "\n",
    "# Compute steps and intervals\n",
    "if num_steps == 0:\n",
    "    num_steps = num_epochs * tr_sig_nb // batch_size\n",
    "else:\n",
    "    num_steps = num_steps\n",
    "\n",
    "if print_interval == 0:\n",
    "    print_interval = num_steps // num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28796ca8",
   "metadata": {},
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0444a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.encoder.net.load_weights(outdir+'encoder2.hdf5')\n",
    "# model.decoder.net.load_weights(outdir+'decoder2.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc60ace",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed422378",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Function for a single training step\n",
    "# @tf.function\n",
    "# def train_step(x_seq, y_seq, m_seq, model, optimizer):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         loss = model.compute_loss(x_seq, y_seq, m_mask=m_seq)\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#     # Gradient Clipping\n",
    "#     clipped_grads = [tf.clip_by_value(grad, -gradient_clip, gradient_clip) for grad in gradients]\n",
    "#     optimizer.apply_gradients(zip(clipped_grads, model.trainable_variables))\n",
    "#     return loss\n",
    "\n",
    "# # To store losses\n",
    "# losses_train = []\n",
    "# losses_val = []\n",
    "\n",
    "# # For best model saving\n",
    "# val_loss_check = np.inf\n",
    "\n",
    "# t0 = time.time()\n",
    "\n",
    "# # For TensorBoard visualization\n",
    "# summary_writer = tf.summary.create_file_writer(\"/path/to/log_dir\")\n",
    "\n",
    "# with summary_writer.as_default():\n",
    "#     for i, (x_seq, y_seq, m_seq) in enumerate(tf_x_train_miss.take(num_steps)):\n",
    "#         loss = train_step(x_seq, y_seq, m_seq, model, optimizer)\n",
    "#         losses_train.append(loss.numpy())\n",
    "\n",
    "#         if i % print_interval == 0:\n",
    "#             print(\"================================================\")\n",
    "#             print(\"Learning rate: {} | Global gradient norm: {:.2f}\".format(\n",
    "#                 optimizer._decayed_lr('float32'), \n",
    "#                 tf.linalg.global_norm(model.trainable_variables))\n",
    "#             )\n",
    "#             print(\"Step {}) Time = {:2f}\".format(i, time.time() - t0))\n",
    "#             loss, mse, kl = model.compute_loss(x_seq, y_seq, m_mask=m_seq, return_parts=True)\n",
    "#             print(\"Train loss = {:.5f} | mse = {:.5f} | KL = {:.5f}\".format(loss, mse, kl))\n",
    "            \n",
    "#             tf.summary.scalar(\"loss_train\", loss, step=i)\n",
    "#             tf.summary.scalar(\"kl_train\", kl, step=i)\n",
    "#             tf.summary.scalar(\"mse_train\", mse, step=i)\n",
    "\n",
    "#             # Validation\n",
    "#             random_indices = np.random.choice(len(Val_X), size=batch_size, replace=False)\n",
    "#             random_batch_X = Val_X[random_indices]\n",
    "#             random_batch_Y = Val_Y[random_indices]\n",
    "#             random_batch_m = m_val_miss[random_indices]\n",
    "#             val_loss, val_mse, val_kl = model.compute_loss(random_batch_X, random_batch_Y, m_mask=random_batch_m, return_parts=True)\n",
    "#             losses_val.append(val_loss.numpy())\n",
    "            \n",
    "#             print(\"Validation loss = {:.5f} | mse = {:.5f} | KL = {:.5f}\".format(val_loss, val_mse, val_kl))\n",
    "\n",
    "#             tf.summary.scalar(\"loss_val\", val_loss, step=i)\n",
    "#             tf.summary.scalar(\"kl_val\", val_kl, step=i)\n",
    "#             tf.summary.scalar(\"mse_val\", val_mse, step=i)\n",
    "\n",
    "#             if val_loss_check > val_loss:\n",
    "#                 val_loss_check = val_loss\n",
    "#                 model.encoder.net.save_weights(outdir+'encoder_gpvae_1.hdf5')\n",
    "#                 model.decoder.net.save_weights(outdir+'decoder_gpvae_1.hdf5')\n",
    "#                 # Stop training if val_mse goes below 0.00043\n",
    "#                 if val_mse < 0.00043:\n",
    "#                     print(\"Stopping training as val_mse reached below 0.00043.\")\n",
    "#                     break\n",
    "\n",
    "#             t0 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f412e4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Function for a single training step\n",
    "# @tf.function\n",
    "# def train_step(x_seq, y_seq, m_seq, model, optimizer):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         loss = model.compute_loss(x_seq, y_seq, m_mask=m_seq)\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#     # Gradient Clipping\n",
    "#     clipped_grads = [tf.clip_by_value(grad, -gradient_clip, gradient_clip) for grad in gradients]\n",
    "#     optimizer.apply_gradients(zip(clipped_grads, model.trainable_variables))\n",
    "#     return loss\n",
    "\n",
    "# # To store losses\n",
    "# losses_train = []\n",
    "# losses_val = []\n",
    "\n",
    "# # For best model saving\n",
    "# val_loss_check = 0.00123\n",
    "\n",
    "# t0 = time.time()\n",
    "\n",
    "# # For TensorBoard visualization\n",
    "# summary_writer = tf.summary.create_file_writer(\"/path/to/log_dir\")\n",
    "\n",
    "# with summary_writer.as_default():\n",
    "#     for i, (x_seq, y_seq, m_seq) in enumerate(tf_x_train_miss.take(num_steps)):\n",
    "#         loss = train_step(x_seq, y_seq, m_seq, model, optimizer)\n",
    "#         losses_train.append(loss.numpy())\n",
    "\n",
    "#         if i % print_interval == 0:\n",
    "#             print(\"================================================\")\n",
    "#             print(\"Learning rate: {} | Global gradient norm: {:.2f}\".format(\n",
    "#                 optimizer._decayed_lr('float32'), \n",
    "#                 tf.linalg.global_norm(model.trainable_variables))\n",
    "#             )\n",
    "#             print(\"Step {}) Time = {:2f}\".format(i, time.time() - t0))\n",
    "#             loss, mse, kl = model.compute_loss(x_seq, y_seq, m_mask=m_seq, return_parts=True)\n",
    "#             print(\"Train loss = {:.5f} | mse = {:.5f} | KL = {:.5f}\".format(loss, mse, kl))\n",
    "            \n",
    "#             tf.summary.scalar(\"loss_train\", loss, step=i)\n",
    "#             tf.summary.scalar(\"kl_train\", kl, step=i)\n",
    "#             tf.summary.scalar(\"mse_train\", mse, step=i)\n",
    "\n",
    "#             # Validation\n",
    "#             random_indices = np.random.choice(len(Val_X), size=batch_size, replace=False)\n",
    "#             random_batch_X = Val_X[random_indices]\n",
    "#             random_batch_Y = Val_Y[random_indices]\n",
    "#             random_batch_m = m_val_miss[random_indices]\n",
    "#             val_loss, val_mse, val_kl = model.compute_loss(random_batch_X, random_batch_Y, m_mask=random_batch_m, return_parts=True)\n",
    "#             losses_val.append(val_loss.numpy())\n",
    "            \n",
    "#             print(\"Validation loss = {:.5f} | mse = {:.5f} | KL = {:.5f}\".format(val_loss, val_mse, val_kl))\n",
    "\n",
    "#             tf.summary.scalar(\"loss_val\", val_loss, step=i)\n",
    "#             tf.summary.scalar(\"kl_val\", val_kl, step=i)\n",
    "#             tf.summary.scalar(\"mse_val\", val_mse, step=i)\n",
    "\n",
    "#             if val_loss_check > val_loss:\n",
    "#                 val_loss_check = val_loss\n",
    "#                 model.encoder.net.save_weights(outdir+'encoder_gpvae_val'+str(val_loss.numpy())+'_valmse'+str(val_mse.numpy())+'.hdf5')\n",
    "#                 model.decoder.net.save_weights(outdir+'decoder_gpvae_val'+str(val_loss.numpy())+'_valmse'+str(val_mse.numpy())+'.hdf5')\n",
    "# #                 # Stop training if val_mse goes below 0.00043\n",
    "# #                 if val_mse < 0.00043:\n",
    "# #                     print(\"Stopping training as val_mse reached below 0.00043.\")\n",
    "# #                     break\n",
    "\n",
    "#             t0 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6b3b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.net.load_weights(outdir+'encoder_gpvae_val0.00092348515_valmse0.00028393653.hdf5')\n",
    "model.decoder.net.load_weights(outdir+'decoder_gpvae_val0.00092348515_valmse0.00028393653.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a386533c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Learning rate: 0.0004999947268515825 | Global gradient norm: 141.60\n",
      "Step 0) Time = 81.041226\n",
      "Train loss = 0.03281 | mse = 0.03211 | KL = 0.00070\n",
      "Validation loss = 0.03191 | mse = 0.03121 | KL = 0.00070\n",
      "================================================\n",
      "Learning rate: 0.0004990053130313754 | Global gradient norm: 141.58\n",
      "Step 188) Time = 181.478992\n",
      "Train loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0004980178782716393 | Global gradient norm: 141.58\n",
      "Step 376) Time = 209.651948\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0004970324225723743 | Global gradient norm: 141.58\n",
      "Step 564) Time = 232.882617\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0004960488877259195 | Global gradient norm: 141.59\n",
      "Step 752) Time = 236.855197\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0004950672737322748 | Global gradient norm: 141.59\n",
      "Step 940) Time = 235.155958\n",
      "Train loss = 0.00095 | mse = 0.00030 | KL = 0.00064\n",
      "Validation loss = 0.00095 | mse = 0.00031 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0004940876387991011 | Global gradient norm: 141.60\n",
      "Step 1128) Time = 231.785568\n",
      "Train loss = 0.00095 | mse = 0.00031 | KL = 0.00064\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0004931099829263985 | Global gradient norm: 141.60\n",
      "Step 1316) Time = 223.942120\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0004921341896988451 | Global gradient norm: 141.61\n",
      "Step 1504) Time = 203.724849\n",
      "Train loss = 0.00096 | mse = 0.00031 | KL = 0.00064\n",
      "Validation loss = 0.00096 | mse = 0.00032 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0004911603173241019 | Global gradient norm: 141.61\n",
      "Step 1692) Time = 193.072110\n",
      "Train loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00031 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0004901883658021688 | Global gradient norm: 141.62\n",
      "Step 1880) Time = 181.040719\n",
      "Train loss = 0.00100 | mse = 0.00035 | KL = 0.00065\n",
      "Validation loss = 0.00099 | mse = 0.00034 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004892183933407068 | Global gradient norm: 141.62\n",
      "Step 2068) Time = 188.923024\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00048825034173205495 | Global gradient norm: 141.63\n",
      "Step 2256) Time = 179.442814\n",
      "Train loss = 0.00104 | mse = 0.00040 | KL = 0.00064\n",
      "Validation loss = 0.00105 | mse = 0.00041 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00048728418187238276 | Global gradient norm: 141.63\n",
      "Step 2444) Time = 184.056909\n",
      "Train loss = 0.00095 | mse = 0.00030 | KL = 0.00064\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00048631991376169026 | Global gradient norm: 141.63\n",
      "Step 2632) Time = 187.679612\n",
      "Train loss = 0.00098 | mse = 0.00033 | KL = 0.00065\n",
      "Validation loss = 0.00097 | mse = 0.00032 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004853575665038079 | Global gradient norm: 141.64\n",
      "Step 2820) Time = 186.984052\n",
      "Train loss = 0.00096 | mse = 0.00032 | KL = 0.00065\n",
      "Validation loss = 0.00096 | mse = 0.00032 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004843971400987357 | Global gradient norm: 141.64\n",
      "Step 3008) Time = 182.824318\n",
      "Train loss = 0.00096 | mse = 0.00032 | KL = 0.00065\n",
      "Validation loss = 0.00098 | mse = 0.00033 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004834386345464736 | Global gradient norm: 141.65\n",
      "Step 3196) Time = 182.508772\n",
      "Train loss = 0.00095 | mse = 0.00031 | KL = 0.00065\n",
      "Validation loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00048248202074319124 | Global gradient norm: 141.65\n",
      "Step 3384) Time = 188.599027\n",
      "Train loss = 0.00096 | mse = 0.00032 | KL = 0.00065\n",
      "Validation loss = 0.00096 | mse = 0.00032 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00048152724048122764 | Global gradient norm: 141.66\n",
      "Step 3572) Time = 181.119101\n",
      "Train loss = 0.00095 | mse = 0.00031 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004805743810720742 | Global gradient norm: 141.66\n",
      "Step 3760) Time = 174.792815\n",
      "Train loss = 0.00100 | mse = 0.00035 | KL = 0.00065\n",
      "Validation loss = 0.00098 | mse = 0.00033 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00047962344251573086 | Global gradient norm: 141.67\n",
      "Step 3948) Time = 170.168418\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004786743375007063 | Global gradient norm: 141.67\n",
      "Step 4136) Time = 174.766825\n",
      "Train loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00047772712423466146 | Global gradient norm: 141.68\n",
      "Step 4324) Time = 174.106027\n",
      "Train loss = 0.00097 | mse = 0.00032 | KL = 0.00065\n",
      "Validation loss = 0.00096 | mse = 0.00032 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00047678177361376584 | Global gradient norm: 141.68\n",
      "Step 4512) Time = 170.335542\n",
      "Train loss = 0.00098 | mse = 0.00033 | KL = 0.00065\n",
      "Validation loss = 0.00098 | mse = 0.00034 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00047583828563801944 | Global gradient norm: 141.69\n",
      "Step 4700) Time = 172.475680\n",
      "Train loss = 0.00097 | mse = 0.00033 | KL = 0.00065\n",
      "Validation loss = 0.00097 | mse = 0.00033 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004748967185150832 | Global gradient norm: 141.70\n",
      "Step 4888) Time = 172.868794\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004739569849334657 | Global gradient norm: 141.70\n",
      "Step 5076) Time = 176.979301\n",
      "Train loss = 0.00098 | mse = 0.00034 | KL = 0.00065\n",
      "Validation loss = 0.00098 | mse = 0.00033 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004730191139969975 | Global gradient norm: 141.71\n",
      "Step 5264) Time = 172.715754\n",
      "Train loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "Validation loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.000472083076601848 | Global gradient norm: 141.71\n",
      "Step 5452) Time = 172.313519\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004711489600595087 | Global gradient norm: 141.72\n",
      "Step 5640) Time = 169.698777\n",
      "Train loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00096 | mse = 0.00032 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004702166188508272 | Global gradient norm: 141.72\n",
      "Step 5828) Time = 175.911377\n",
      "Train loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.000469286140287295 | Global gradient norm: 141.73\n",
      "Step 6016) Time = 173.302548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.00101 | mse = 0.00037 | KL = 0.00065\n",
      "Validation loss = 0.00102 | mse = 0.00037 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004683574952650815 | Global gradient norm: 141.74\n",
      "Step 6204) Time = 177.180178\n",
      "Train loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "Validation loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004674307128880173 | Global gradient norm: 141.75\n",
      "Step 6392) Time = 172.775655\n",
      "Train loss = 0.00095 | mse = 0.00031 | KL = 0.00065\n",
      "Validation loss = 0.00097 | mse = 0.00032 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00046650576405227184 | Global gradient norm: 141.75\n",
      "Step 6580) Time = 179.376045\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004655826196540147 | Global gradient norm: 141.76\n",
      "Step 6768) Time = 177.246445\n",
      "Train loss = 0.00112 | mse = 0.00047 | KL = 0.00065\n",
      "Validation loss = 0.00112 | mse = 0.00047 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00046466130879707634 | Global gradient norm: 141.77\n",
      "Step 6956) Time = 176.805502\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00046374183148145676 | Global gradient norm: 141.78\n",
      "Step 7144) Time = 189.809965\n",
      "Train loss = 0.00099 | mse = 0.00035 | KL = 0.00065\n",
      "Validation loss = 0.00100 | mse = 0.00035 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00046282418770715594 | Global gradient norm: 141.78\n",
      "Step 7332) Time = 184.113043\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.000461908319266513 | Global gradient norm: 141.79\n",
      "Step 7520) Time = 189.309436\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004609942843671888 | Global gradient norm: 141.80\n",
      "Step 7708) Time = 188.927531\n",
      "Train loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004600820830091834 | Global gradient norm: 141.81\n",
      "Step 7896) Time = 182.469495\n",
      "Train loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00045917165698483586 | Global gradient norm: 141.81\n",
      "Step 8084) Time = 184.881246\n",
      "Train loss = 0.00097 | mse = 0.00031 | KL = 0.00065\n",
      "Validation loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004582630645018071 | Global gradient norm: 141.82\n",
      "Step 8272) Time = 187.043104\n",
      "Train loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004573562473524362 | Global gradient norm: 141.84\n",
      "Step 8460) Time = 190.127087\n",
      "Train loss = 0.00099 | mse = 0.00034 | KL = 0.00065\n",
      "Validation loss = 0.00099 | mse = 0.00034 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00045645120553672314 | Global gradient norm: 141.85\n",
      "Step 8648) Time = 184.097264\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00045554793905466795 | Global gradient norm: 141.86\n",
      "Step 8836) Time = 187.014628\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.000454646535217762 | Global gradient norm: 141.87\n",
      "Step 9024) Time = 185.834088\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00045374687761068344 | Global gradient norm: 141.88\n",
      "Step 9212) Time = 189.510106\n",
      "Train loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004528489662334323 | Global gradient norm: 141.88\n",
      "Step 9400) Time = 189.694441\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00045195285929366946 | Global gradient norm: 141.90\n",
      "Step 9588) Time = 188.812130\n",
      "Train loss = 0.00100 | mse = 0.00035 | KL = 0.00065\n",
      "Validation loss = 0.00099 | mse = 0.00034 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004510585276875645 | Global gradient norm: 141.91\n",
      "Step 9776) Time = 185.695338\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004501659714151174 | Global gradient norm: 141.92\n",
      "Step 9964) Time = 184.301224\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00044927519047632813 | Global gradient norm: 141.92\n",
      "Step 10152) Time = 185.341660\n",
      "Train loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004483861557673663 | Global gradient norm: 141.94\n",
      "Step 10340) Time = 186.422152\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00044749886728823185 | Global gradient norm: 141.95\n",
      "Step 10528) Time = 179.998199\n",
      "Train loss = 0.00095 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004466133832465857 | Global gradient norm: 141.96\n",
      "Step 10716) Time = 177.168184\n",
      "Train loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00044572961633093655 | Global gradient norm: 141.94\n",
      "Step 10904) Time = 172.368443\n",
      "Train loss = 0.00097 | mse = 0.00031 | KL = 0.00065\n",
      "Validation loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004448475665412843 | Global gradient norm: 141.96\n",
      "Step 11092) Time = 175.196978\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00044396729208528996 | Global gradient norm: 141.97\n",
      "Step 11280) Time = 170.798144\n",
      "Train loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00044308873475529253 | Global gradient norm: 141.98\n",
      "Step 11468) Time = 176.300302\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00044221198186278343 | Global gradient norm: 141.99\n",
      "Step 11656) Time = 177.783409\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004413369169924408 | Global gradient norm: 142.01\n",
      "Step 11844) Time = 171.458152\n",
      "Train loss = 0.00096 | mse = 0.00032 | KL = 0.00065\n",
      "Validation loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004404635983519256 | Global gradient norm: 142.02\n",
      "Step 12032) Time = 172.074789\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00043959199683740735 | Global gradient norm: 142.03\n",
      "Step 12220) Time = 177.502597\n",
      "Train loss = 0.00097 | mse = 0.00032 | KL = 0.00065\n",
      "Validation loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004387220833450556 | Global gradient norm: 142.04\n",
      "Step 12408) Time = 175.468910\n",
      "Train loss = 0.00098 | mse = 0.00033 | KL = 0.00065\n",
      "Validation loss = 0.00097 | mse = 0.00032 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00043785397429019213 | Global gradient norm: 142.05\n",
      "Step 12596) Time = 174.927480\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00043698755325749516 | Global gradient norm: 142.06\n",
      "Step 12784) Time = 179.249115\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004361228202469647 | Global gradient norm: 142.07\n",
      "Step 12972) Time = 175.968244\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00043525980436243117 | Global gradient norm: 142.08\n",
      "Step 13160) Time = 176.083403\n",
      "Train loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "Validation loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00043439853470772505 | Global gradient norm: 142.09\n",
      "Step 13348) Time = 201.572402\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00043353892397135496 | Global gradient norm: 142.10\n",
      "Step 13536) Time = 182.682514\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00043268100125715137 | Global gradient norm: 142.12\n",
      "Step 13724) Time = 190.294896\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004318248247727752 | Global gradient norm: 142.13\n",
      "Step 13912) Time = 193.172433\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.000430970307206735 | Global gradient norm: 142.15\n",
      "Step 14100) Time = 190.679037\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004301175067666918 | Global gradient norm: 142.16\n",
      "Step 14288) Time = 193.401019\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004292663943488151 | Global gradient norm: 142.17\n",
      "Step 14476) Time = 195.472251\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004284169408492744 | Global gradient norm: 142.18\n",
      "Step 14664) Time = 190.507131\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004275691753719002 | Global gradient norm: 142.19\n",
      "Step 14852) Time = 188.299570\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00042672312702052295 | Global gradient norm: 142.20\n",
      "Step 15040) Time = 190.541967\n",
      "Train loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004258787084836513 | Global gradient norm: 142.22\n",
      "Step 15228) Time = 193.252020\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004250359779689461 | Global gradient norm: 142.23\n",
      "Step 15416) Time = 188.424196\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00042419490637257695 | Global gradient norm: 142.24\n",
      "Step 15604) Time = 188.253122\n",
      "Train loss = 0.00098 | mse = 0.00034 | KL = 0.00065\n",
      "Validation loss = 0.00099 | mse = 0.00034 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00042335549369454384 | Global gradient norm: 142.25\n",
      "Step 15792) Time = 192.820984\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004225177690386772 | Global gradient norm: 142.27\n",
      "Step 15980) Time = 189.855420\n",
      "Train loss = 0.00103 | mse = 0.00038 | KL = 0.00065\n",
      "Validation loss = 0.00103 | mse = 0.00038 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00042168167419731617 | Global gradient norm: 142.28\n",
      "Step 16168) Time = 191.034666\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00042084723827429116 | Global gradient norm: 142.30\n",
      "Step 16356) Time = 191.846612\n",
      "Train loss = 0.00097 | mse = 0.00032 | KL = 0.00065\n",
      "Validation loss = 0.00097 | mse = 0.00033 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004200144321657717 | Global gradient norm: 142.31\n",
      "Step 16544) Time = 192.262099\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004191833140794188 | Global gradient norm: 142.32\n",
      "Step 16732) Time = 193.728577\n",
      "Train loss = 0.00097 | mse = 0.00032 | KL = 0.00065\n",
      "Validation loss = 0.00097 | mse = 0.00032 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00041835385491140187 | Global gradient norm: 142.33\n",
      "Step 16920) Time = 186.645489\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004175259964540601 | Global gradient norm: 142.35\n",
      "Step 17108) Time = 184.184111\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00041669976781122386 | Global gradient norm: 142.36\n",
      "Step 17296) Time = 173.962753\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004158751980867237 | Global gradient norm: 142.38\n",
      "Step 17484) Time = 176.895881\n",
      "Train loss = 0.00098 | mse = 0.00033 | KL = 0.00065\n",
      "Validation loss = 0.00096 | mse = 0.00032 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00041505228728055954 | Global gradient norm: 142.39\n",
      "Step 17672) Time = 176.769507\n",
      "Train loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00041423094808124006 | Global gradient norm: 142.40\n",
      "Step 17860) Time = 174.815939\n",
      "Train loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00041341123869642615 | Global gradient norm: 142.42\n",
      "Step 18048) Time = 184.176810\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004125931882299483 | Global gradient norm: 142.42\n",
      "Step 18236) Time = 178.285222\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00041177673847414553 | Global gradient norm: 142.44\n",
      "Step 18424) Time = 180.307867\n",
      "Train loss = 0.00097 | mse = 0.00032 | KL = 0.00065\n",
      "Validation loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004109619476366788 | Global gradient norm: 142.46\n",
      "Step 18612) Time = 174.115951\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004101486993022263 | Global gradient norm: 142.47\n",
      "Step 18800) Time = 177.434254\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00040933708078227937 | Global gradient norm: 142.48\n",
      "Step 18988) Time = 176.120812\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00040852706297300756 | Global gradient norm: 142.49\n",
      "Step 19176) Time = 175.768257\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004077186749782413 | Global gradient norm: 142.50\n",
      "Step 19364) Time = 179.230758\n",
      "Train loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "Validation loss = 0.00097 | mse = 0.00032 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004069118876941502 | Global gradient norm: 142.52\n",
      "Step 19552) Time = 174.975388\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004061067011207342 | Global gradient norm: 142.53\n",
      "Step 19740) Time = 169.385465\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004053030861541629 | Global gradient norm: 142.55\n",
      "Step 19928) Time = 173.502440\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004045010718982667 | Global gradient norm: 142.56\n",
      "Step 20116) Time = 178.460043\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00040370060014538467 | Global gradient norm: 142.57\n",
      "Step 20304) Time = 174.337759\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00040290175820700824 | Global gradient norm: 142.58\n",
      "Step 20492) Time = 173.615610\n",
      "Train loss = 0.00097 | mse = 0.00032 | KL = 0.00065\n",
      "Validation loss = 0.00098 | mse = 0.00033 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004021044878754765 | Global gradient norm: 142.60\n",
      "Step 20680) Time = 174.014278\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004013087891507894 | Global gradient norm: 142.61\n",
      "Step 20868) Time = 171.739048\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0004005146911367774 | Global gradient norm: 142.63\n",
      "Step 21056) Time = 175.854719\n",
      "Train loss = 0.00097 | mse = 0.00033 | KL = 0.00065\n",
      "Validation loss = 0.00098 | mse = 0.00033 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00039972213562577963 | Global gradient norm: 142.64\n",
      "Step 21244) Time = 175.694935\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003989311517216265 | Global gradient norm: 142.65\n",
      "Step 21432) Time = 172.915097\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003981417394243181 | Global gradient norm: 142.67\n",
      "Step 21620) Time = 184.270169\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00039735386963002384 | Global gradient norm: 142.68\n",
      "Step 21808) Time = 186.786102\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003965676005464047 | Global gradient norm: 142.70\n",
      "Step 21996) Time = 187.936308\n",
      "Train loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00031 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003957828739657998 | Global gradient norm: 142.70\n",
      "Step 22184) Time = 195.517294\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003949996898882091 | Global gradient norm: 142.72\n",
      "Step 22372) Time = 189.805147\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003942180483136326 | Global gradient norm: 142.73\n",
      "Step 22560) Time = 192.627575\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00039343800744973123 | Global gradient norm: 142.75\n",
      "Step 22748) Time = 197.695482\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00039265945088118315 | Global gradient norm: 142.76\n",
      "Step 22936) Time = 188.346261\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003918824077118188 | Global gradient norm: 142.77\n",
      "Step 23124) Time = 189.980005\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003911069652531296 | Global gradient norm: 142.79\n",
      "Step 23312) Time = 178.959494\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003903330070897937 | Global gradient norm: 142.80\n",
      "Step 23500) Time = 187.742017\n",
      "Train loss = 0.00098 | mse = 0.00033 | KL = 0.00065\n",
      "Validation loss = 0.00098 | mse = 0.00033 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003895606496371329 | Global gradient norm: 142.82\n",
      "Step 23688) Time = 188.581692\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00038878980558365583 | Global gradient norm: 142.83\n",
      "Step 23876) Time = 187.758271\n",
      "Train loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "Validation loss = 0.00097 | mse = 0.00033 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003880204167217016 | Global gradient norm: 142.84\n",
      "Step 24064) Time = 188.989293\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003872525994665921 | Global gradient norm: 142.86\n",
      "Step 24252) Time = 179.098531\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003864862665068358 | Global gradient norm: 142.87\n",
      "Step 24440) Time = 194.715463\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003857215342577547 | Global gradient norm: 142.88\n",
      "Step 24628) Time = 191.363174\n",
      "Train loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00031 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003849582572001964 | Global gradient norm: 142.90\n",
      "Step 24816) Time = 185.415411\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003841964644379914 | Global gradient norm: 142.91\n",
      "Step 25004) Time = 189.682755\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00038343624328263104 | Global gradient norm: 142.93\n",
      "Step 25192) Time = 188.095459\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00038267747731879354 | Global gradient norm: 142.93\n",
      "Step 25380) Time = 187.257216\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003819202247541398 | Global gradient norm: 142.95\n",
      "Step 25568) Time = 194.870991\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003811644855886698 | Global gradient norm: 142.96\n",
      "Step 25756) Time = 188.404319\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003804102016147226 | Global gradient norm: 142.98\n",
      "Step 25944) Time = 188.323647\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003796574310399592 | Global gradient norm: 142.99\n",
      "Step 26132) Time = 184.182160\n",
      "Train loss = 0.00097 | mse = 0.00032 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00031 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003789061738643795 | Global gradient norm: 143.00\n",
      "Step 26320) Time = 187.701841\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00037815640098415315 | Global gradient norm: 143.01\n",
      "Step 26508) Time = 190.338771\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00037740811239928007 | Global gradient norm: 143.03\n",
      "Step 26696) Time = 187.808936\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00037666124990209937 | Global gradient norm: 143.04\n",
      "Step 26884) Time = 188.503217\n",
      "Train loss = 0.00098 | mse = 0.00033 | KL = 0.00065\n",
      "Validation loss = 0.00098 | mse = 0.00034 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003759159299079329 | Global gradient norm: 143.06\n",
      "Step 27072) Time = 191.327882\n",
      "Train loss = 0.00103 | mse = 0.00038 | KL = 0.00065\n",
      "Validation loss = 0.00103 | mse = 0.00038 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003751720651052892 | Global gradient norm: 143.07\n",
      "Step 27260) Time = 186.057885\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003744296554941684 | Global gradient norm: 143.08\n",
      "Step 27448) Time = 188.764904\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003736887010745704 | Global gradient norm: 143.09\n",
      "Step 27636) Time = 192.927692\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003729492600541562 | Global gradient norm: 143.11\n",
      "Step 27824) Time = 185.190190\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00037221130332909524 | Global gradient norm: 143.12\n",
      "Step 28012) Time = 173.073321\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00037147474358789623 | Global gradient norm: 143.13\n",
      "Step 28200) Time = 174.531591\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003707396681420505 | Global gradient norm: 143.15\n",
      "Step 28388) Time = 175.600283\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00037000601878389716 | Global gradient norm: 143.16\n",
      "Step 28576) Time = 182.172655\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003692738537210971 | Global gradient norm: 143.17\n",
      "Step 28764) Time = 184.499421\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003685431438498199 | Global gradient norm: 143.19\n",
      "Step 28952) Time = 182.414134\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00036781386006623507 | Global gradient norm: 143.20\n",
      "Step 29140) Time = 188.310032\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003670860023703426 | Global gradient norm: 143.21\n",
      "Step 29328) Time = 185.052003\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.000366359599865973 | Global gradient norm: 143.23\n",
      "Step 29516) Time = 186.425642\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00036563468165695667 | Global gradient norm: 143.24\n",
      "Step 29704) Time = 186.354129\n",
      "Train loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003649111313279718 | Global gradient norm: 143.25\n",
      "Step 29892) Time = 186.416790\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003641890361905098 | Global gradient norm: 143.27\n",
      "Step 30080) Time = 184.881819\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00036346836714074016 | Global gradient norm: 143.28\n",
      "Step 30268) Time = 185.768036\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003627491241786629 | Global gradient norm: 143.29\n",
      "Step 30456) Time = 188.672911\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00036203133640810847 | Global gradient norm: 143.31\n",
      "Step 30644) Time = 184.856829\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00036131494562141597 | Global gradient norm: 143.32\n",
      "Step 30832) Time = 184.926182\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003605999518185854 | Global gradient norm: 143.34\n",
      "Step 31020) Time = 191.850457\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003598863841034472 | Global gradient norm: 143.35\n",
      "Step 31208) Time = 192.702123\n",
      "Train loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "Validation loss = 0.00096 | mse = 0.00031 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00035917427157983184 | Global gradient norm: 143.36\n",
      "Step 31396) Time = 193.271130\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00035846352693624794 | Global gradient norm: 143.37\n",
      "Step 31584) Time = 186.547343\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.000357754179276526 | Global gradient norm: 143.39\n",
      "Step 31772) Time = 188.073793\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003570462286006659 | Global gradient norm: 143.39\n",
      "Step 31960) Time = 194.758888\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003563397331163287 | Global gradient norm: 143.41\n",
      "Step 32148) Time = 176.464159\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003556345764081925 | Global gradient norm: 143.42\n",
      "Step 32336) Time = 176.049022\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003549308457877487 | Global gradient norm: 143.44\n",
      "Step 32524) Time = 173.024671\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00035422848304733634 | Global gradient norm: 143.45\n",
      "Step 32712) Time = 174.354193\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00035352754639461637 | Global gradient norm: 143.46\n",
      "Step 32900) Time = 173.707796\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003528279485180974 | Global gradient norm: 143.47\n",
      "Step 33088) Time = 173.794454\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00035212980583310127 | Global gradient norm: 143.49\n",
      "Step 33276) Time = 172.602063\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00035143300192430615 | Global gradient norm: 143.50\n",
      "Step 33464) Time = 173.162983\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00091 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003507375658955425 | Global gradient norm: 143.51\n",
      "Step 33652) Time = 174.010035\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003500435268506408 | Global gradient norm: 143.53\n",
      "Step 33840) Time = 173.890117\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003493508556857705 | Global gradient norm: 143.54\n",
      "Step 34028) Time = 175.384100\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003486595524009317 | Global gradient norm: 143.55\n",
      "Step 34216) Time = 178.181373\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003479696169961244 | Global gradient norm: 143.56\n",
      "Step 34404) Time = 173.511141\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00034728102036751807 | Global gradient norm: 143.58\n",
      "Step 34592) Time = 173.869811\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00034659382072277367 | Global gradient norm: 143.59\n",
      "Step 34780) Time = 174.634730\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00034590798895806074 | Global gradient norm: 143.60\n",
      "Step 34968) Time = 175.814189\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003452234959695488 | Global gradient norm: 143.62\n",
      "Step 35156) Time = 174.290119\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00034454037086106837 | Global gradient norm: 143.63\n",
      "Step 35344) Time = 178.612537\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003438585845287889 | Global gradient norm: 143.64\n",
      "Step 35532) Time = 177.774354\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00030 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00034317816607654095 | Global gradient norm: 143.66\n",
      "Step 35720) Time = 120.924819\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003424990572966635 | Global gradient norm: 143.67\n",
      "Step 35908) Time = 177.843684\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00034182131639681756 | Global gradient norm: 143.68\n",
      "Step 36096) Time = 188.017239\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003411449433770031 | Global gradient norm: 143.69\n",
      "Step 36284) Time = 196.000023\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003404698509257287 | Global gradient norm: 143.71\n",
      "Step 36472) Time = 199.612163\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003397961554583162 | Global gradient norm: 143.72\n",
      "Step 36660) Time = 193.955825\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00033912374055944383 | Global gradient norm: 143.73\n",
      "Step 36848) Time = 191.741977\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003384526935406029 | Global gradient norm: 143.75\n",
      "Step 37036) Time = 189.707125\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003377829270903021 | Global gradient norm: 143.76\n",
      "Step 37224) Time = 194.291479\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003371145576238632 | Global gradient norm: 143.77\n",
      "Step 37412) Time = 195.298928\n",
      "Train loss = 0.00095 | mse = 0.00031 | KL = 0.00064\n",
      "Validation loss = 0.00096 | mse = 0.00031 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003364474687259644 | Global gradient norm: 143.78\n",
      "Step 37600) Time = 187.902783\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00095 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003357816895004362 | Global gradient norm: 143.80\n",
      "Step 37788) Time = 199.514658\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003351172199472785 | Global gradient norm: 143.81\n",
      "Step 37976) Time = 194.727964\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003344541182741523 | Global gradient norm: 143.82\n",
      "Step 38164) Time = 191.783257\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00033379229716956615 | Global gradient norm: 143.84\n",
      "Step 38352) Time = 203.927574\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003331317566335201 | Global gradient norm: 143.85\n",
      "Step 38540) Time = 189.760509\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003324725548736751 | Global gradient norm: 143.86\n",
      "Step 38728) Time = 194.252906\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003318146336823702 | Global gradient norm: 143.87\n",
      "Step 38916) Time = 197.631085\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003311580512672663 | Global gradient norm: 143.88\n",
      "Step 39104) Time = 194.665145\n",
      "Train loss = 0.00095 | mse = 0.00031 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003305027785245329 | Global gradient norm: 143.89\n",
      "Step 39292) Time = 191.556234\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00065\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003298487572465092 | Global gradient norm: 143.91\n",
      "Step 39480) Time = 178.843002\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00032919601653702557 | Global gradient norm: 143.92\n",
      "Step 39668) Time = 181.640568\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003285446437075734 | Global gradient norm: 143.94\n",
      "Step 39856) Time = 183.215170\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00032789446413517 | Global gradient norm: 143.95\n",
      "Step 40044) Time = 177.479988\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.000327245652442798 | Global gradient norm: 143.96\n",
      "Step 40232) Time = 177.363158\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00032659806311130524 | Global gradient norm: 143.98\n",
      "Step 40420) Time = 179.299203\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.000325951783452183 | Global gradient norm: 143.99\n",
      "Step 40608) Time = 182.144897\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003253067843616009 | Global gradient norm: 144.00\n",
      "Step 40796) Time = 177.480741\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003246630949433893 | Global gradient norm: 144.01\n",
      "Step 40984) Time = 180.246112\n",
      "Train loss = 0.00102 | mse = 0.00038 | KL = 0.00064\n",
      "Validation loss = 0.00103 | mse = 0.00038 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00032402059878222644 | Global gradient norm: 144.02\n",
      "Step 41172) Time = 185.299650\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.0003233794413972646 | Global gradient norm: 144.04\n",
      "Step 41360) Time = 182.677798\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003227395354770124 | Global gradient norm: 144.05\n",
      "Step 41548) Time = 183.900450\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00032210088102146983 | Global gradient norm: 144.06\n",
      "Step 41736) Time = 180.501607\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00032146350713446736 | Global gradient norm: 144.08\n",
      "Step 41924) Time = 177.964270\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00032082738471217453 | Global gradient norm: 144.09\n",
      "Step 42112) Time = 176.341400\n",
      "Train loss = 0.00091 | mse = 0.00026 | KL = 0.00065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003201925428584218 | Global gradient norm: 144.10\n",
      "Step 42300) Time = 179.031284\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003195589524693787 | Global gradient norm: 144.11\n",
      "Step 42488) Time = 180.469158\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00065\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00031892655533738434 | Global gradient norm: 144.13\n",
      "Step 42676) Time = 178.746082\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.000318295496981591 | Global gradient norm: 144.14\n",
      "Step 42864) Time = 178.930641\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00031766563188284636 | Global gradient norm: 144.15\n",
      "Step 43052) Time = 182.879062\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00031703701824881136 | Global gradient norm: 144.16\n",
      "Step 43240) Time = 184.791487\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00031640968518331647 | Global gradient norm: 144.18\n",
      "Step 43428) Time = 191.704877\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00031578357447870076 | Global gradient norm: 144.19\n",
      "Step 43616) Time = 190.541106\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00031515868613496423 | Global gradient norm: 144.20\n",
      "Step 43804) Time = 195.860453\n",
      "Train loss = 0.00103 | mse = 0.00039 | KL = 0.00064\n",
      "Validation loss = 0.00103 | mse = 0.00039 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00031453504925593734 | Global gradient norm: 144.21\n",
      "Step 43992) Time = 193.488865\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003139126056339592 | Global gradient norm: 144.23\n",
      "Step 44180) Time = 190.202453\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00031329147168435156 | Global gradient norm: 144.24\n",
      "Step 44368) Time = 189.163942\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003126715309917927 | Global gradient norm: 144.25\n",
      "Step 44556) Time = 188.367028\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003120527835562825 | Global gradient norm: 144.26\n",
      "Step 44744) Time = 202.204583\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00065\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.000311435287585482 | Global gradient norm: 144.28\n",
      "Step 44932) Time = 197.189728\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00031081901397556067 | Global gradient norm: 144.29\n",
      "Step 45120) Time = 188.653811\n",
      "Train loss = 0.00095 | mse = 0.00030 | KL = 0.00064\n",
      "Validation loss = 0.00095 | mse = 0.00030 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00031020399183034897 | Global gradient norm: 144.30\n",
      "Step 45308) Time = 189.695732\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00030959013383835554 | Global gradient norm: 144.32\n",
      "Step 45496) Time = 195.079083\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003089774982072413 | Global gradient norm: 144.33\n",
      "Step 45684) Time = 188.662733\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003083661140408367 | Global gradient norm: 144.34\n",
      "Step 45872) Time = 189.778564\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00030775589402765036 | Global gradient norm: 144.35\n",
      "Step 46060) Time = 194.382268\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00094 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00030714692547917366 | Global gradient norm: 144.37\n",
      "Step 46248) Time = 194.181171\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00030653912108391523 | Global gradient norm: 144.38\n",
      "Step 46436) Time = 194.557379\n",
      "Train loss = 0.00095 | mse = 0.00031 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00030593256815336645 | Global gradient norm: 144.39\n",
      "Step 46624) Time = 188.776113\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.000305327121168375 | Global gradient norm: 144.40\n",
      "Step 46812) Time = 178.630521\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00030472298385575414 | Global gradient norm: 144.41\n",
      "Step 47000) Time = 177.314949\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003041199524886906 | Global gradient norm: 144.43\n",
      "Step 47188) Time = 181.373676\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00030351817258633673 | Global gradient norm: 144.44\n",
      "Step 47376) Time = 181.105306\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003029175859410316 | Global gradient norm: 144.46\n",
      "Step 47564) Time = 176.807480\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00030231813434511423 | Global gradient norm: 144.47\n",
      "Step 47752) Time = 177.073147\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00030171990511007607 | Global gradient norm: 144.48\n",
      "Step 47940) Time = 176.707431\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00030112286913208663 | Global gradient norm: 144.49\n",
      "Step 48128) Time = 180.669647\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0003005270264111459 | Global gradient norm: 144.50\n",
      "Step 48316) Time = 178.936429\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00029993231873959303 | Global gradient norm: 144.52\n",
      "Step 48504) Time = 175.000393\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00029933880432508886 | Global gradient norm: 144.53\n",
      "Step 48692) Time = 179.482242\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00029874645406380296 | Global gradient norm: 144.54\n",
      "Step 48880) Time = 175.764259\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002981552970595658 | Global gradient norm: 144.55\n",
      "Step 49068) Time = 179.405879\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002975653042085469 | Global gradient norm: 144.57\n",
      "Step 49256) Time = 174.289817\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002969765046145767 | Global gradient norm: 144.58\n",
      "Step 49444) Time = 180.330221\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002963888109661639 | Global gradient norm: 144.59\n",
      "Step 49632) Time = 182.079570\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002958023105747998 | Global gradient norm: 144.60\n",
      "Step 49820) Time = 178.721327\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002952170034404844 | Global gradient norm: 144.62\n",
      "Step 50008) Time = 177.162117\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00029463277314789593 | Global gradient norm: 144.63\n",
      "Step 50196) Time = 182.099992\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002940497943200171 | Global gradient norm: 144.64\n",
      "Step 50384) Time = 185.687393\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00029346789233386517 | Global gradient norm: 144.65\n",
      "Step 50572) Time = 196.008428\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00029288718360476196 | Global gradient norm: 144.66\n",
      "Step 50760) Time = 197.545188\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00029230760992504656 | Global gradient norm: 144.67\n",
      "Step 50948) Time = 202.256277\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00029172920039854944 | Global gradient norm: 144.69\n",
      "Step 51136) Time = 196.920956\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00029115189681760967 | Global gradient norm: 144.70\n",
      "Step 51324) Time = 193.288762\n",
      "Train loss = 0.00094 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00029057575738988817 | Global gradient norm: 144.71\n",
      "Step 51512) Time = 196.451050\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00029000078211538494 | Global gradient norm: 144.72\n",
      "Step 51700) Time = 195.231930\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002894269418902695 | Global gradient norm: 144.74\n",
      "Step 51888) Time = 194.248757\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00028885420761071146 | Global gradient norm: 144.75\n",
      "Step 52076) Time = 196.263798\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002882826083805412 | Global gradient norm: 144.76\n",
      "Step 52264) Time = 200.098681\n",
      "Train loss = 0.00098 | mse = 0.00034 | KL = 0.00064\n",
      "Validation loss = 0.00098 | mse = 0.00034 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00028771214419975877 | Global gradient norm: 144.77\n",
      "Step 52452) Time = 194.054870\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00028714281506836414 | Global gradient norm: 144.78\n",
      "Step 52640) Time = 191.899273\n",
      "Train loss = 0.00097 | mse = 0.00033 | KL = 0.00064\n",
      "Validation loss = 0.00096 | mse = 0.00031 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00028657462098635733 | Global gradient norm: 144.79\n",
      "Step 52828) Time = 202.501503\n",
      "Train loss = 0.00108 | mse = 0.00043 | KL = 0.00064\n",
      "Validation loss = 0.00109 | mse = 0.00045 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002860075328499079 | Global gradient norm: 144.81\n",
      "Step 53016) Time = 200.318360\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002854415506590158 | Global gradient norm: 144.82\n",
      "Step 53204) Time = 196.058849\n",
      "Train loss = 0.00106 | mse = 0.00042 | KL = 0.00064\n",
      "Validation loss = 0.00107 | mse = 0.00043 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002848767035175115 | Global gradient norm: 144.83\n",
      "Step 53392) Time = 193.290914\n",
      "Train loss = 0.00093 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00028431302052922547 | Global gradient norm: 144.84\n",
      "Step 53580) Time = 201.772714\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002837503852788359 | Global gradient norm: 144.85\n",
      "Step 53768) Time = 190.194606\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002831889141816646 | Global gradient norm: 144.86\n",
      "Step 53956) Time = 182.059575\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002826285199262202 | Global gradient norm: 144.88\n",
      "Step 54144) Time = 180.560446\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00028206928982399404 | Global gradient norm: 144.89\n",
      "Step 54332) Time = 178.722493\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00028151110745966434 | Global gradient norm: 144.90\n",
      "Step 54520) Time = 178.408231\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00028095406014472246 | Global gradient norm: 144.91\n",
      "Step 54708) Time = 178.539004\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002803980896715075 | Global gradient norm: 144.92\n",
      "Step 54896) Time = 181.493528\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00027984322514384985 | Global gradient norm: 144.93\n",
      "Step 55084) Time = 179.901758\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00027928949566558003 | Global gradient norm: 144.94\n",
      "Step 55272) Time = 183.108747\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00027873681392520666 | Global gradient norm: 144.95\n",
      "Step 55460) Time = 179.913401\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002781852672342211 | Global gradient norm: 144.97\n",
      "Step 55648) Time = 177.729308\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002776347391773015 | Global gradient norm: 144.98\n",
      "Step 55836) Time = 182.631745\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002770853752736002 | Global gradient norm: 144.99\n",
      "Step 56024) Time = 177.581585\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002765370882116258 | Global gradient norm: 145.00\n",
      "Step 56212) Time = 179.049435\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002759898779913783 | Global gradient norm: 145.01\n",
      "Step 56400) Time = 180.971946\n",
      "Train loss = 0.00091 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002754437446128577 | Global gradient norm: 145.02\n",
      "Step 56588) Time = 179.754953\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00027489865897223353 | Global gradient norm: 145.03\n",
      "Step 56776) Time = 179.336667\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002743547083809972 | Global gradient norm: 145.04\n",
      "Step 56964) Time = 178.214959\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002738117764238268 | Global gradient norm: 145.06\n",
      "Step 57152) Time = 177.101285\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002732700086198747 | Global gradient norm: 145.07\n",
      "Step 57340) Time = 190.914469\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00027272923034615815 | Global gradient norm: 145.08\n",
      "Step 57528) Time = 188.789501\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00027218955801799893 | Global gradient norm: 145.09\n",
      "Step 57716) Time = 194.098714\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00027165093342773616 | Global gradient norm: 145.10\n",
      "Step 57904) Time = 189.406971\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00027111341478303075 | Global gradient norm: 145.11\n",
      "Step 58092) Time = 195.961191\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00027057688566856086 | Global gradient norm: 145.13\n",
      "Step 58280) Time = 196.427451\n",
      "Train loss = 0.00096 | mse = 0.00032 | KL = 0.00064\n",
      "Validation loss = 0.00095 | mse = 0.00031 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002700414916034788 | Global gradient norm: 145.14\n",
      "Step 58468) Time = 187.620758\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002695071161724627 | Global gradient norm: 145.15\n",
      "Step 58656) Time = 190.948904\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002689738175831735 | Global gradient norm: 145.16\n",
      "Step 58844) Time = 191.518423\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00026844156673178077 | Global gradient norm: 145.17\n",
      "Step 59032) Time = 197.482980\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00026791036361828446 | Global gradient norm: 145.18\n",
      "Step 59220) Time = 187.707491\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002673802082426846 | Global gradient norm: 145.19\n",
      "Step 59408) Time = 194.975667\n",
      "Train loss = 0.00103 | mse = 0.00039 | KL = 0.00064\n",
      "Validation loss = 0.00104 | mse = 0.00040 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002668511006049812 | Global gradient norm: 145.20\n",
      "Step 59596) Time = 190.973163\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00026632306980900466 | Global gradient norm: 145.21\n",
      "Step 59784) Time = 190.355155\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002657960867509246 | Global gradient norm: 145.22\n",
      "Step 59972) Time = 194.731932\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00026527009322308004 | Global gradient norm: 145.23\n",
      "Step 60160) Time = 192.508608\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002647451765369624 | Global gradient norm: 145.24\n",
      "Step 60348) Time = 190.461324\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002642213075887412 | Global gradient norm: 145.25\n",
      "Step 60536) Time = 191.052522\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00026369845727458596 | Global gradient norm: 145.26\n",
      "Step 60724) Time = 184.688757\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002631766255944967 | Global gradient norm: 145.27\n",
      "Step 60912) Time = 181.403167\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002626558707561344 | Global gradient norm: 145.29\n",
      "Step 61100) Time = 176.922974\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002621361054480076 | Global gradient norm: 145.30\n",
      "Step 61288) Time = 182.846182\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002616174169816077 | Global gradient norm: 145.31\n",
      "Step 61476) Time = 179.020800\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002610997180454433 | Global gradient norm: 145.32\n",
      "Step 61664) Time = 177.667385\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002605830377433449 | Global gradient norm: 145.33\n",
      "Step 61852) Time = 179.260050\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00026006740517914295 | Global gradient norm: 145.35\n",
      "Step 62040) Time = 181.225559\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00025955276214517653 | Global gradient norm: 145.35\n",
      "Step 62228) Time = 181.691490\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002590391377452761 | Global gradient norm: 145.37\n",
      "Step 62416) Time = 177.214825\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00025852653197944164 | Global gradient norm: 145.38\n",
      "Step 62604) Time = 178.646426\n",
      "Train loss = 0.00121 | mse = 0.00057 | KL = 0.00064\n",
      "Validation loss = 0.00121 | mse = 0.00056 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002580150030553341 | Global gradient norm: 145.38\n",
      "Step 62792) Time = 178.578981\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00025750440545380116 | Global gradient norm: 145.39\n",
      "Step 62980) Time = 180.436746\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002569948846939951 | Global gradient norm: 145.40\n",
      "Step 63168) Time = 177.878414\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00025648632436059415 | Global gradient norm: 145.41\n",
      "Step 63356) Time = 177.252404\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00025597881176508963 | Global gradient norm: 145.42\n",
      "Step 63544) Time = 178.384233\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002554722595959902 | Global gradient norm: 145.43\n",
      "Step 63732) Time = 175.763399\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002549667260609567 | Global gradient norm: 145.44\n",
      "Step 63920) Time = 177.802909\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002544621529523283 | Global gradient norm: 145.46\n",
      "Step 64108) Time = 184.284479\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00025395865668542683 | Global gradient norm: 145.47\n",
      "Step 64296) Time = 180.637203\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00025345609174109995 | Global gradient norm: 145.48\n",
      "Step 64484) Time = 190.847770\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002529545745346695 | Global gradient norm: 145.49\n",
      "Step 64672) Time = 194.941635\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002524539886508137 | Global gradient norm: 145.50\n",
      "Step 64860) Time = 193.689150\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002519544796086848 | Global gradient norm: 145.51\n",
      "Step 65048) Time = 195.189787\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002514558727853 | Global gradient norm: 145.52\n",
      "Step 65236) Time = 192.168857\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002509583136998117 | Global gradient norm: 145.53\n",
      "Step 65424) Time = 190.580398\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00025046171504072845 | Global gradient norm: 145.54\n",
      "Step 65612) Time = 195.268938\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002499660768080503 | Global gradient norm: 145.55\n",
      "Step 65800) Time = 194.864146\n",
      "Train loss = 0.00092 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00024947142810560763 | Global gradient norm: 145.56\n",
      "Step 65988) Time = 189.122120\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002489777689334005 | Global gradient norm: 145.57\n",
      "Step 66176) Time = 192.908068\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002484850992914289 | Global gradient norm: 145.58\n",
      "Step 66364) Time = 194.950362\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002479933900758624 | Global gradient norm: 145.60\n",
      "Step 66552) Time = 197.704449\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002475026703905314 | Global gradient norm: 145.61\n",
      "Step 66740) Time = 190.357173\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00024701288202777505 | Global gradient norm: 145.62\n",
      "Step 66928) Time = 190.458844\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002465240831952542 | Global gradient norm: 145.63\n",
      "Step 67116) Time = 193.595526\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00024603624478913844 | Global gradient norm: 145.64\n",
      "Step 67304) Time = 193.767388\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002455493959132582 | Global gradient norm: 145.65\n",
      "Step 67492) Time = 191.130561\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.000245063507463783 | Global gradient norm: 145.66\n",
      "Step 67680) Time = 191.467412\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002445786085445434 | Global gradient norm: 145.67\n",
      "Step 67868) Time = 182.371500\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00024409462639596313 | Global gradient norm: 145.68\n",
      "Step 68056) Time = 180.387692\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00024361159012187272 | Global gradient norm: 145.69\n",
      "Step 68244) Time = 179.279550\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00024312951427418739 | Global gradient norm: 145.70\n",
      "Step 68432) Time = 178.466152\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00024264839885290712 | Global gradient norm: 145.71\n",
      "Step 68620) Time = 181.972821\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00024216827296186239 | Global gradient norm: 145.72\n",
      "Step 68808) Time = 178.869012\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00024168906384147704 | Global gradient norm: 145.73\n",
      "Step 68996) Time = 177.704498\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00024121080059558153 | Global gradient norm: 145.74\n",
      "Step 69184) Time = 177.168887\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00024073348322417587 | Global gradient norm: 145.75\n",
      "Step 69372) Time = 178.612135\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00024025711172726005 | Global gradient norm: 145.76\n",
      "Step 69560) Time = 177.840995\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002397817006567493 | Global gradient norm: 145.77\n",
      "Step 69748) Time = 180.379529\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00023930720635689795 | Global gradient norm: 145.78\n",
      "Step 69936) Time = 180.530944\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00023883365793153644 | Global gradient norm: 145.79\n",
      "Step 70124) Time = 177.046109\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00023836104082874954 | Global gradient norm: 145.80\n",
      "Step 70312) Time = 180.452443\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002378893841523677 | Global gradient norm: 145.81\n",
      "Step 70500) Time = 183.487695\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00023741861514281482 | Global gradient norm: 145.82\n",
      "Step 70688) Time = 181.911140\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00023694883566349745 | Global gradient norm: 145.83\n",
      "Step 70876) Time = 182.002143\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.000236479943851009 | Global gradient norm: 145.84\n",
      "Step 71064) Time = 179.193652\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00023601202701684088 | Global gradient norm: 145.85\n",
      "Step 71252) Time = 124.967754\n",
      "Train loss = 0.00090 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002355449687456712 | Global gradient norm: 145.86\n",
      "Step 71440) Time = 183.099084\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00023507888545282185 | Global gradient norm: 145.88\n",
      "Step 71628) Time = 198.597345\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00023461368982680142 | Global gradient norm: 145.89\n",
      "Step 71816) Time = 200.581937\n",
      "Train loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00023414944007527083 | Global gradient norm: 145.89\n",
      "Step 72004) Time = 197.642735\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00023368612164631486 | Global gradient norm: 145.90\n",
      "Step 72192) Time = 200.062195\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00023322369088418782 | Global gradient norm: 145.92\n",
      "Step 72380) Time = 189.066017\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00023276216234080493 | Global gradient norm: 145.92\n",
      "Step 72568) Time = 198.252094\n",
      "Train loss = 0.00095 | mse = 0.00031 | KL = 0.00064\n",
      "Validation loss = 0.00094 | mse = 0.00030 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002323015796719119 | Global gradient norm: 145.94\n",
      "Step 72756) Time = 199.889243\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00023184188466984779 | Global gradient norm: 145.95\n",
      "Step 72944) Time = 197.893739\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00023138310643844306 | Global gradient norm: 145.96\n",
      "Step 73132) Time = 195.080241\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00023092525952961296 | Global gradient norm: 145.97\n",
      "Step 73320) Time = 196.801618\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00023046830028761178 | Global gradient norm: 145.98\n",
      "Step 73508) Time = 193.191725\n",
      "Train loss = 0.00091 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00023001222871243954 | Global gradient norm: 145.99\n",
      "Step 73696) Time = 201.403060\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00022955707390792668 | Global gradient norm: 146.00\n",
      "Step 73884) Time = 206.319105\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002291028358740732 | Global gradient norm: 146.01\n",
      "Step 74072) Time = 189.281313\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002286495000589639 | Global gradient norm: 146.02\n",
      "Step 74260) Time = 193.033004\n",
      "Train loss = 0.00092 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00022819703735876828 | Global gradient norm: 146.03\n",
      "Step 74448) Time = 200.531647\n",
      "Train loss = 0.00090 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00022774549142923206 | Global gradient norm: 146.04\n",
      "Step 74636) Time = 203.861595\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00022729481861460954 | Global gradient norm: 146.05\n",
      "Step 74824) Time = 199.844667\n",
      "Train loss = 0.00091 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00022684503346681595 | Global gradient norm: 146.06\n",
      "Step 75012) Time = 196.392330\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002263961359858513 | Global gradient norm: 146.07\n",
      "Step 75200) Time = 183.206786\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00022594815527554601 | Global gradient norm: 146.08\n",
      "Step 75388) Time = 178.763436\n",
      "Train loss = 0.00090 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00022550103312823921 | Global gradient norm: 146.09\n",
      "Step 75576) Time = 183.480526\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002250548277515918 | Global gradient norm: 146.10\n",
      "Step 75764) Time = 184.439619\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00022460948093794286 | Global gradient norm: 146.11\n",
      "Step 75952) Time = 183.471796\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00022416500723920763 | Global gradient norm: 146.12\n",
      "Step 76140) Time = 183.812608\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00022372142120730132 | Global gradient norm: 146.13\n",
      "Step 76328) Time = 180.986885\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00022327872284222394 | Global gradient norm: 146.14\n",
      "Step 76516) Time = 178.979015\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00089 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002228369121439755 | Global gradient norm: 146.15\n",
      "Step 76704) Time = 181.056567\n",
      "Train loss = 0.00091 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002223959454568103 | Global gradient norm: 146.17\n",
      "Step 76892) Time = 181.214808\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00022195588098838925 | Global gradient norm: 146.18\n",
      "Step 77080) Time = 179.580845\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.000221516631427221 | Global gradient norm: 146.19\n",
      "Step 77268) Time = 183.164656\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00022107831318862736 | Global gradient norm: 146.20\n",
      "Step 77456) Time = 182.449499\n",
      "Train loss = 0.00094 | mse = 0.00031 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00030 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00022064082440920174 | Global gradient norm: 146.21\n",
      "Step 77644) Time = 176.812175\n",
      "Train loss = 0.00090 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00022020422329660505 | Global gradient norm: 146.22\n",
      "Step 77832) Time = 182.944762\n",
      "Train loss = 0.00090 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00021976848074700683 | Global gradient norm: 146.23\n",
      "Step 78020) Time = 178.117607\n",
      "Train loss = 0.00092 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00021933361131232232 | Global gradient norm: 146.24\n",
      "Step 78208) Time = 180.360128\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00021889957133680582 | Global gradient norm: 146.25\n",
      "Step 78396) Time = 182.059453\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00021846643358003348 | Global gradient norm: 146.25\n",
      "Step 78584) Time = 184.853490\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00021803411073051393 | Global gradient norm: 146.27\n",
      "Step 78772) Time = 202.430205\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00021760266099590808 | Global gradient norm: 146.28\n",
      "Step 78960) Time = 194.680308\n",
      "Train loss = 0.00090 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00021717208437621593 | Global gradient norm: 146.28\n",
      "Step 79148) Time = 199.568950\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002167423372156918 | Global gradient norm: 146.29\n",
      "Step 79336) Time = 198.617544\n",
      "Train loss = 0.00089 | mse = 0.00025 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00021631343406625092 | Global gradient norm: 146.30\n",
      "Step 79524) Time = 196.258481\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00021588537492789328 | Global gradient norm: 146.31\n",
      "Step 79712) Time = 194.881394\n",
      "Train loss = 0.00091 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00021545817435253412 | Global gradient norm: 146.32\n",
      "Step 79900) Time = 196.776841\n",
      "Train loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00021503184689208865 | Global gradient norm: 146.33\n",
      "Step 80088) Time = 202.379041\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00021460631978698075 | Global gradient norm: 146.34\n",
      "Step 80276) Time = 196.016974\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00021418165124487132 | Global gradient norm: 146.35\n",
      "Step 80464) Time = 196.784596\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00021375782671384513 | Global gradient norm: 146.36\n",
      "Step 80652) Time = 199.668599\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00021333486074581742 | Global gradient norm: 146.37\n",
      "Step 80840) Time = 196.619603\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00021291269513312727 | Global gradient norm: 146.38\n",
      "Step 81028) Time = 197.462630\n",
      "Train loss = 0.00089 | mse = 0.00025 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002124913880834356 | Global gradient norm: 146.39\n",
      "Step 81216) Time = 199.617649\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002120708959409967 | Global gradient norm: 146.40\n",
      "Step 81404) Time = 204.115268\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002116512623615563 | Global gradient norm: 146.41\n",
      "Step 81592) Time = 200.489802\n",
      "Train loss = 0.00093 | mse = 0.00030 | KL = 0.00064\n",
      "Validation loss = 0.00091 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00021123242913745344 | Global gradient norm: 146.42\n",
      "Step 81780) Time = 199.298244\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00027 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002108144253725186 | Global gradient norm: 146.43\n",
      "Step 81968) Time = 204.218459\n",
      "Train loss = 0.00092 | mse = 0.00029 | KL = 0.00064\n",
      "Validation loss = 0.00093 | mse = 0.00029 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.000210397265618667 | Global gradient norm: 146.44\n",
      "Step 82156) Time = 182.221582\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00089 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00020998093532398343 | Global gradient norm: 146.45\n",
      "Step 82344) Time = 176.891144\n",
      "Train loss = 0.00096 | mse = 0.00032 | KL = 0.00064\n",
      "Validation loss = 0.00095 | mse = 0.00031 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00020956540538463742 | Global gradient norm: 146.45\n",
      "Step 82532) Time = 186.685860\n",
      "Train loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "Validation loss = 0.00092 | mse = 0.00028 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.00020915070490445942 | Global gradient norm: 146.46\n",
      "Step 82720) Time = 201.578774\n",
      "Train loss = 0.00091 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002087368629872799 | Global gradient norm: 146.47\n",
      "Step 82908) Time = 222.942997\n",
      "Train loss = 0.00090 | mse = 0.00027 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "================================================\n",
      "Learning rate: 0.0002083238068735227 | Global gradient norm: 146.48\n",
      "Step 83096) Time = 235.572234\n",
      "Train loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n",
      "Validation loss = 0.00090 | mse = 0.00026 | KL = 0.00064\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function for a single training step\n",
    "@tf.function\n",
    "def train_step(x_seq, y_seq, m_seq, model, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = model.compute_loss(x_seq, y_seq, m_mask=m_seq)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Gradient Clipping\n",
    "    clipped_grads = [tf.clip_by_value(grad, -gradient_clip, gradient_clip) for grad in gradients]\n",
    "    optimizer.apply_gradients(zip(clipped_grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# To store losses\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "\n",
    "# For best model saving\n",
    "val_loss_check = 0.00092348515\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# For TensorBoard visualization\n",
    "summary_writer = tf.summary.create_file_writer(\"/path/to/log_dir\")\n",
    "\n",
    "with summary_writer.as_default():\n",
    "    for i, (x_seq, y_seq, m_seq) in enumerate(tf_x_train_miss.take(num_steps)):\n",
    "        loss = train_step(x_seq, y_seq, m_seq, model, optimizer)\n",
    "        losses_train.append(loss.numpy())\n",
    "\n",
    "        if i % print_interval == 0:\n",
    "            print(\"================================================\")\n",
    "            print(\"Learning rate: {} | Global gradient norm: {:.2f}\".format(\n",
    "                optimizer._decayed_lr('float32'), \n",
    "                tf.linalg.global_norm(model.trainable_variables))\n",
    "            )\n",
    "            print(\"Step {}) Time = {:2f}\".format(i, time.time() - t0))\n",
    "            loss, mse, kl = model.compute_loss(x_seq, y_seq, m_mask=m_seq, return_parts=True)\n",
    "            print(\"Train loss = {:.5f} | mse = {:.5f} | KL = {:.5f}\".format(loss, mse, kl))\n",
    "            \n",
    "            tf.summary.scalar(\"loss_train\", loss, step=i)\n",
    "            tf.summary.scalar(\"kl_train\", kl, step=i)\n",
    "            tf.summary.scalar(\"mse_train\", mse, step=i)\n",
    "\n",
    "            # Validation\n",
    "            random_indices = np.random.choice(len(Val_X), size=batch_size, replace=False)\n",
    "            random_batch_X = Val_X[random_indices]\n",
    "            random_batch_Y = Val_Y[random_indices]\n",
    "            random_batch_m = m_val_miss[random_indices]\n",
    "            val_loss, val_mse, val_kl = model.compute_loss(random_batch_X, random_batch_Y, m_mask=random_batch_m, return_parts=True)\n",
    "            losses_val.append(val_loss.numpy())\n",
    "            \n",
    "            print(\"Validation loss = {:.5f} | mse = {:.5f} | KL = {:.5f}\".format(val_loss, val_mse, val_kl))\n",
    "\n",
    "            tf.summary.scalar(\"loss_val\", val_loss, step=i)\n",
    "            tf.summary.scalar(\"kl_val\", val_kl, step=i)\n",
    "            tf.summary.scalar(\"mse_val\", val_mse, step=i)\n",
    "\n",
    "            if val_loss_check > val_loss:\n",
    "                val_loss_check = val_loss\n",
    "                model.encoder.net.save_weights(outdir+'encoder_gpvae_val'+str(val_loss.numpy())+'_valmse'+str(val_mse.numpy())+'.hdf5')\n",
    "                model.decoder.net.save_weights(outdir+'decoder_gpvae_val'+str(val_loss.numpy())+'_valmse'+str(val_mse.numpy())+'.hdf5')\n",
    "#                 # Stop training if val_mse goes below 0.00043\n",
    "#                 if val_mse < 0.00043:\n",
    "#                     print(\"Stopping training as val_mse reached below 0.00043.\")\n",
    "#                     break\n",
    "\n",
    "            t0 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "065427ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.encoder.net.save_weights(outdir+'encoder_gpvae_val'+str(val_loss.numpy())+'_valmse'+str(val_mse.numpy())+'.hdf5')\n",
    "# model.decoder.net.save_weights(outdir+'decoder_gpvae_val'+str(val_loss.numpy())+'_valmse'+str(val_mse.numpy())+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af388001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3dd632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dbc0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a1be961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\linalg\\linear_operator_lower_triangular.py:159: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n",
      "WARNING:tensorflow:From C:\\Users\\USER\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:334: calling TransformedDistribution.__init__ (from tensorflow_probability.python.distributions.transformed_distribution) with batch_shape is deprecated and will be removed after 2020-06-01.\n",
      "Instructions for updating:\n",
      "`batch_shape` and `event_shape` args are deprecated. Please use `tfd.Sample`, `tfd.Independent`, and broadcasted parameters of the base distribution instead. For example, replace `tfd.TransformedDistribution(tfd.Normal(0., 1.), tfb.Exp(), batch_shape=[2, 3], event_shape=[4])` with `tfd.TransformedDistrbution(tfd.Sample(tfd.Normal(tf.zeros([2, 3]), 1.),sample_shape=[4]), tfb.Exp())` or `tfd.TransformedDistribution(tfd.Independent(tfd.Normal(tf.zeros([2, 3, 4]), 1.), reinterpreted_batch_ndims=1), tfb.Exp())`.\n",
      "WARNING:tensorflow:From C:\\Users\\USER\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:334: calling TransformedDistribution.__init__ (from tensorflow_probability.python.distributions.transformed_distribution) with event_shape is deprecated and will be removed after 2020-06-01.\n",
      "Instructions for updating:\n",
      "`batch_shape` and `event_shape` args are deprecated. Please use `tfd.Sample`, `tfd.Independent`, and broadcasted parameters of the base distribution instead. For example, replace `tfd.TransformedDistribution(tfd.Normal(0., 1.), tfb.Exp(), batch_shape=[2, 3], event_shape=[4])` with `tfd.TransformedDistrbution(tfd.Sample(tfd.Normal(tf.zeros([2, 3]), 1.),sample_shape=[4]), tfb.Exp())` or `tfd.TransformedDistribution(tfd.Independent(tfd.Normal(tf.zeros([2, 3, 4]), 1.), reinterpreted_batch_ndims=1), tfb.Exp())`.\n",
      "================================================\n",
      "Learning rate: 0.001 | Global gradient norm: 26.98\n",
      "Step 0) Time = 22.744952\n",
      "Train loss = 13.90182 | mse = 0.10523 | KL = 13.79659\n",
      "Validation loss = 13.89319 | mse = 0.11085 | KL = 13.78234\n",
      "================================================\n",
      "Learning rate: 0.001 | Global gradient norm: 29.86\n",
      "Step 236) Time = 171.759154\n",
      "Train loss = 0.03111 | mse = 0.01206 | KL = 0.01905\n",
      "Validation loss = 0.02672 | mse = 0.00978 | KL = 0.01694\n",
      "================================================\n",
      "Learning rate: 0.001 | Global gradient norm: 31.59\n",
      "Step 472) Time = 208.640807\n",
      "Train loss = 0.02351 | mse = 0.01390 | KL = 0.00961\n",
      "Validation loss = 0.01984 | mse = 0.01225 | KL = 0.00759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x000001CDAC2A4550>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USER\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 534, in __del__\n",
      "    gen_dataset_ops.delete_iterator(\n",
      "  File \"C:\\Users\\USER\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 1262, in delete_iterator\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m summary_writer\u001b[38;5;241m.\u001b[39mas_default(), tf\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mrecord_if(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (x_seq, y_seq, m_seq) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tf_x_train_miss\u001b[38;5;241m.\u001b[39mtake(num_steps)):\n\u001b[1;32m---> 20\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m         losses_train\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m# Print intermediate results\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name) \u001b[38;5;28;01mas\u001b[39;00m tm:\n\u001b[1;32m--> 828\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    829\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m   new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:855\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    852\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    853\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 855\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    857\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    858\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2940\u001b[0m   (graph_function,\n\u001b[0;32m   2941\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1914\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1916\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1917\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1918\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1920\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m     args,\n\u001b[0;32m   1922\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1923\u001b[0m     executing_eagerly)\n\u001b[0;32m   1924\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 555\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    562\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    564\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    567\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    568\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# @tf.function\n",
    "# def train_step(x_seq, y_seq, m_seq, model, optimizer):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         loss = model.compute_loss(x_seq, y_seq, m_mask=m_seq)\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "#     return loss\n",
    "\n",
    "# gradient_clip = 1e4\n",
    "# debug= True\n",
    "\n",
    "# losses_train = []\n",
    "# losses_val = []\n",
    "\n",
    "# val_loss_check = np.inf\n",
    "\n",
    "# t0 = time.time()\n",
    "# with summary_writer.as_default(), tf.summary.record_if(lambda: True):\n",
    "#     for i, (x_seq, y_seq, m_seq) in enumerate(tf_x_train_miss.take(num_steps)):\n",
    "#         loss = train_step(x_seq, y_seq, m_seq, model, optimizer)\n",
    "#         losses_train.append(loss.numpy())\n",
    "            \n",
    "#         # Print intermediate results\n",
    "#         if i % print_interval == 0:\n",
    "#             print(\"================================================\")\n",
    "#             print(\"Learning rate: {} | Global gradient norm: {:.2f}\".format(optimizer._lr, tf.linalg.global_norm(model.trainable_variables)))\n",
    "#             print(\"Step {}) Time = {:2f}\".format(i, time.time() - t0))\n",
    "#             loss, mse, kl = model.compute_loss(x_seq, y_seq, m_mask=m_seq, return_parts=True)\n",
    "#             print(\"Train loss = {:.5f} | mse = {:.5f} | KL = {:.5f}\".format(loss, mse, kl))\n",
    "            \n",
    "#             tf.summary.scalar(\"loss_train\", loss, step=i)\n",
    "#             tf.summary.scalar(\"kl_train\", kl, step=i)\n",
    "#             tf.summary.scalar(\"mse_train\", mse, step=i)\n",
    "\n",
    "#             # Validation loss\n",
    "#             random_indices = np.random.choice(len(Val_X), size=batch_size, replace=False)\n",
    "#             random_batch_X = Val_X[random_indices]\n",
    "#             random_batch_Y = Val_Y[random_indices]\n",
    "#             random_batch_m = m_val_miss[random_indices]\n",
    "#             val_loss, val_mse, val_kl = model.compute_loss(random_batch_X, random_batch_Y, m_mask=random_batch_m, return_parts=True)\n",
    "#             losses_val.append(val_loss.numpy())\n",
    "            \n",
    "            \n",
    "# #             for x_val_batch, y_val_batch, m_val_batch in tf_x_val_miss.take(1): \n",
    "# #                 val_loss, val_mse, val_kl = model.compute_loss(x_val_batch, y_val_batch, m_mask=m_val_batch, return_parts=True)\n",
    "# #                 losses_val.append(val_loss.numpy())\n",
    "\n",
    "#             print(\"Validation loss = {:.5f} | mse = {:.5f} | KL = {:.5f}\".format(val_loss, val_mse, val_kl))\n",
    "\n",
    "#             tf.summary.scalar(\"loss_val\", val_loss, step=i)\n",
    "#             tf.summary.scalar(\"kl_val\", val_kl, step=i)\n",
    "#             tf.summary.scalar(\"mse_val\", val_mse, step=i)\n",
    "\n",
    "#             if val_loss_check > val_loss:\n",
    "#                 val_loss_check = val_loss\n",
    "#                 model.encoder.net.save_weights(outdir+'encoder_gpvae_1.hdf5')\n",
    "#                 model.decoder.net.save_weights(outdir+'decoder_gpvae_1.hdf5')\n",
    "\n",
    "\n",
    "#             # Update learning rate (used only for physionet with decay=0.5)\n",
    "#             if i > 0 and i % (10*print_interval) == 0:\n",
    "#                 optimizer._lr = max(0.99 * optimizer._lr, 0.1 * learning_rate)\n",
    "                \n",
    "#             t0 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46081684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
