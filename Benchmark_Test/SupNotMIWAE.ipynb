{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4094c82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\ai\\lib\\site-packages\\scipy\\__init__.py:173: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.19.2)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79df5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc85b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = np.load('D:/Dropbox/AICleansing_ver2/2.ProcessedData/train_mimic_orgscale.npy')\n",
    "# TrData = np.load('D:/Dropbox/AICleansing_ver2/2.ProcessedData/train_mimic_orgscale.npy')\n",
    "# ValData = np.load('D:/Dropbox/AICleansing_ver2/2.ProcessedData/valid_vitaldb_orgscale.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc92dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(Data)\n",
    "split_idx = len(Data)//5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16c77ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70844"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1f3ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrData = Data[split_idx:]\n",
    "ValData = Data[:split_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb25a300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((283380, 3000), (70844, 3000))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrData.shape, ValData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b8e542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ver2\n",
    "\n",
    "TrDataFrame = tf.signal.frame(TrData.astype('float32'), 50, 50).numpy()\n",
    "ValDataFrame = tf.signal.frame(ValData.astype('float32'), 50, 50).numpy()\n",
    "\n",
    "# np.random.shuffle(TrDataFrame)\n",
    "# np.random.shuffle(ValDataFrame)\n",
    "\n",
    "TrMask = np.random.choice([0,1], size=TrDataFrame.shape, p=[0.1,0.9])\n",
    "ValMask = np.random.choice([0,1], size=ValDataFrame.shape, p=[0.1,0.9])\n",
    "\n",
    "Tr_X = TrDataFrame.copy()\n",
    "Tr_Y = (TrDataFrame - 20.0) / (220.0 - 20.0)\n",
    "# 평균 80, 표준 편차 25인 정규 분포에서 값을 뽑아서 채울 배열 생성\n",
    "random_values = np.random.normal(loc=80, scale=25, size=TrDataFrame.shape)\n",
    "# TrMask가 1인 위치에 뽑은 값을 할당\n",
    "Tr_X[TrMask == 0] = random_values[TrMask == 0]\n",
    "Tr_X = (Tr_X - 20.0) / (220.0 - 20.0)\n",
    "\n",
    "TrMask[:,-10:,:] = 0\n",
    "Tr_X[:,-10:,:] = Tr_X[:,-10:,:] + np.random.normal(loc=0.0, scale=0.05, size=Tr_X[:,-10:,:].shape)\n",
    "\n",
    "Tr_X = np.clip(Tr_X, 0.0, 1.0)\n",
    "\n",
    "Val_X = ValDataFrame.copy()\n",
    "Val_Y = (ValDataFrame - 20.0) / (220.0 - 20.0)\n",
    "# 평균 80, 표준 편차 25인 정규 분포에서 값을 뽑아서 채울 배열 생성\n",
    "random_values = np.random.normal(loc=80, scale=25, size=ValDataFrame.shape)\n",
    "# TrMask가 1인 위치에 뽑은 값을 할당\n",
    "Val_X[ValMask == 0] = random_values[ValMask == 0]\n",
    "Val_X = (Val_X - 20.0) / (220.0 - 20.0)\n",
    "\n",
    "ValMask[:,-10:,:] = 0\n",
    "Val_X[:,-10:,:] = Val_X[:,-10:,:] + np.random.normal(loc=0.0, scale=0.05, size=Val_X[:,-10:,:].shape)\n",
    "\n",
    "Val_X = np.clip(Val_X, 0.0, 1.0)\n",
    "\n",
    "del Data\n",
    "del random_values\n",
    "del TrDataFrame\n",
    "del ValDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf1dd0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283380, 60, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tr_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c531b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow.keras import layers, initializers, regularizers, constraints, activations\n",
    "\n",
    "CONSTANT_INIT = initializers.Constant(0.05)\n",
    "InterpolateInput = namedtuple(\"InterpolateInput\", [\"values\", \"mask\", \"times\"])\n",
    "InterpolateState = namedtuple(\"InterpolateState\", [\"x_keep\", \"t_keep\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34129672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_column(tensor, replacement, column_idx):\n",
    "    left = tensor[:, :column_idx]\n",
    "    right = tensor[:, column_idx+1:]\n",
    "    new_tensor = tf.concat([left, tf.expand_dims(replacement, axis=1), right], axis=1)\n",
    "    return new_tensor\n",
    "\n",
    "def compute_last_observed_and_mean(x, mask):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - x: A tensor of shape [batch_size, time_len, feature_dim]\n",
    "    - mask: A tensor of shape [batch_size, time_len, feature_dim]\n",
    "    \n",
    "    Returns:\n",
    "    - last_observed: A tensor representing the last observed value for each feature over time.\n",
    "    - mean: A tensor representing the mean value for each feature over time.\n",
    "    \"\"\"\n",
    "\n",
    "    # x shape: [batch_size, time_len, feature_dim]\n",
    "    # mask shape: [batch_size, time_len, feature_dim]\n",
    "    # print('clom-1',x)\n",
    "    # print('clom-2',mask)\n",
    "\n",
    "    # Initialize last_observed tensor with zeros\n",
    "    last_observed = tf.zeros_like(x)\n",
    "    # last_observed shape: [batch_size, time_len, feature_dim]\n",
    "    # print('clom-3',last_observed)\n",
    "\n",
    "    # Initialize last_values tensor with zeros\n",
    "    dynamic_shape = tf.shape(x)\n",
    "    last_values = tf.zeros([dynamic_shape[0], OrigDim])\n",
    "\n",
    "    # last_values shape: [batch_size, feature_dim]\n",
    "    # print('clom-4',last_values)\n",
    "\n",
    "    # Iteratively compute the last observed values\n",
    "    for t in range(x.shape[1]):\n",
    "        last_values = mask[:, t] * x[:, t] + (1 - mask[:, t]) * last_values\n",
    "        last_observed = replace_column(last_observed, last_values, t)\n",
    "    # last_observed shape: [batch_size, time_len, feature_dim]\n",
    "    # print('clom-5',last_observed)\n",
    "\n",
    "    # Compute mean across the time axis and account for the mask\n",
    "    mean = tf.math.reduce_sum(x, axis=1) / tf.math.reduce_sum(mask, axis=1)\n",
    "    # print('clom-6',mean)\n",
    "    # mean shape: [batch_size, feature_dim]\n",
    "\n",
    "    mean = tf.expand_dims(mean, axis=1)\n",
    "    # mean shape: [batch_size, 1, feature_dim]\n",
    "    # print('clom-7',mean)\n",
    "\n",
    "    mean = tf.tile(mean, [1, x.shape[1], 1])\n",
    "    # mean shape: [batch_size, time_len, feature_dim]\n",
    "    # print('clom-8',mean)\n",
    "\n",
    "    return last_observed, mean\n",
    "\n",
    "def compute_delta_t(s):\n",
    "    \"\"\"\n",
    "    Compute the time differences between consecutive missing values based on the mask s.\n",
    "    \n",
    "    Args:\n",
    "    - s: A tensor of shape [batch_size, time_len, feature_dim] containing binary values (1 for observed and 0 for missing).\n",
    "    \n",
    "    Returns:\n",
    "    A tensor of shape [batch_size, time_len, feature_dim] containing time differences between consecutive missing values.\n",
    "    \"\"\"\n",
    "    # s shape: [batch_size, time_len, feature_dim]\n",
    "\n",
    "    # Find positions of missing values\n",
    "    missing_positions = tf.where(s == 0)\n",
    "    # missing_positions shape: [num_missing, 3] where num_missing is the number of zeros in s\n",
    "\n",
    "    # Compute cumulative sum of s and use difference to get distances\n",
    "    cumsum_s = tf.cumsum(s, axis=1)\n",
    "    # cumsum_s shape: [batch_size, time_len, feature_dim]\n",
    "\n",
    "    zeros_tensor = tf.zeros((tf.shape(s)[0], 1, tf.shape(s)[2]), dtype=s.dtype)\n",
    "    # zeros_tensor shape: [batch_size, 1, feature_dim]\n",
    "\n",
    "    distances = tf.concat([zeros_tensor, cumsum_s[:, :-1]], axis=1)\n",
    "    # distances shape after concat: [batch_size, time_len, feature_dim]\n",
    "\n",
    "    distances = cumsum_s - distances\n",
    "    # distances shape: [batch_size, time_len, feature_dim]\n",
    "\n",
    "    # Create a delta_t tensor initialized with distances for missing values\n",
    "    delta_t = tf.where(s == 0, distances, 0)\n",
    "    # delta_t shape: [batch_size, time_len, feature_dim]\n",
    "\n",
    "    # For initial missing values, fill with their index + 1\n",
    "    initial_missing = tf.math.cumprod(s, axis=1) == 0\n",
    "    # initial_missing shape: [batch_size, time_len, feature_dim]\n",
    "\n",
    "    arange_tensor = tf.reshape(tf.range(1, tf.shape(s)[1] + 1, dtype=s.dtype), (1, -1, 1))\n",
    "    # arange_tensor shape after reshape: [1, time_len, 1]\n",
    "\n",
    "    arange_tensor = tf.broadcast_to(arange_tensor, tf.shape(s))\n",
    "    # arange_tensor shape after broadcast: [batch_size, time_len, feature_dim]\n",
    "\n",
    "    delta_t = tf.where(initial_missing, arange_tensor, delta_t)\n",
    "    # delta_t shape: [batch_size, time_len, feature_dim]\n",
    "    \n",
    "    return delta_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55557903",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\n",
    "    \"GRUDCell\",\n",
    "    \"GRUD\",\n",
    "    \"GRUDInput\",\n",
    "    \"GRUDState\",\n",
    "]\n",
    "\n",
    "CONSTANT_INIT = initializers.Constant(0.05)\n",
    "IMPUTATION_TYPES = [\"miwae\", \"decay\", \"forward\", \"zero\", \"raw\"]\n",
    "\n",
    "GRUDInput = namedtuple(\"GRUDInput\", [\"values\", \"mask\", \"times\"])\n",
    "GRUDState = namedtuple(\"GRUDState\", [\"h\", \"x_keep\", \"s_prev\"])\n",
    "\n",
    "\n",
    "def exp_relu(x):\n",
    "    return K.exp(-K.relu(x))\n",
    "\n",
    "def exp_softplus(x):\n",
    "    return K.exp(-K.softplus(x))\n",
    "\n",
    "\n",
    "class GRUDCell(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        x_imputation=\"decay\",\n",
    "        input_decay=\"exp_relu\",\n",
    "        hidden_decay=\"exp_relu\",\n",
    "        masking_decay=None,\n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"hard_sigmoid\",\n",
    "        use_bias=True,\n",
    "        use_decay_bias=True,\n",
    "        feed_masking=True,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        recurrent_initializer=\"orthogonal\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        decay_initializer=CONSTANT_INIT,\n",
    "        kernel_regularizer=None,\n",
    "        recurrent_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        decay_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        recurrent_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        decay_constraint=None,\n",
    "        dropout=0.,\n",
    "        recurrent_dropout=0.,\n",
    "        reset_after=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        if units < 0:\n",
    "            raise ValueError(\n",
    "                f\"Received an invalid value for argument `units`, expected a positive integer, got {units}.\"\n",
    "            )\n",
    "\n",
    "        if x_imputation not in IMPUTATION_TYPES:\n",
    "            raise ValueError(f\"Unsupported imputation method: {x_imputation}\")\n",
    "\n",
    "        if kwargs.pop(\"implementation\", 1) != 1:\n",
    "            raise ValueError(f\"GRUDCell only supports implementation=1\")\n",
    "\n",
    "        if reset_after is True:  # TODO: Check if this is correct.\n",
    "            raise ValueError(\"GRUDCell only support reset_after=False\")\n",
    "\n",
    "        # NOTE: Disable caching device for GRUDCell to avoid potential errors.\n",
    "        # By default use cached variable under v2 mode, see b/143699808.\n",
    "        # if tf.compat.v1.executing_eagerly_outside_functions():\n",
    "        #     self._enable_caching_device = kwargs.pop(\"enable_caching_device\", True)\n",
    "        # else:\n",
    "        #     self._enable_caching_device = kwargs.pop(\"enable_caching_device\", False)\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.x_imputation = x_imputation\n",
    "        self.use_decay_bias = use_decay_bias\n",
    "        self.feed_masking = feed_masking\n",
    "\n",
    "        self.units = units\n",
    "        self.decay_initializer = initializers.get(decay_initializer)\n",
    "        self.decay_regularizer = regularizers.get(decay_regularizer)\n",
    "        self.decay_constraint = constraints.get(decay_constraint)\n",
    "\n",
    "        if x_imputation not in [\"decay\", \"miwae\"]:\n",
    "            input_decay = None\n",
    "\n",
    "        with tf.keras.utils.custom_object_scope({\"exp_relu\": exp_relu, \"exp_softplus\": exp_softplus}):\n",
    "            self.input_decay = None if input_decay is None else activations.get(input_decay)\n",
    "            self.hidden_decay = None if hidden_decay is None else activations.get(hidden_decay)\n",
    "\n",
    "            if self.feed_masking:\n",
    "                self.masking_decay = None if masking_decay is None else activations.get(masking_decay)\n",
    "                self._masking_dropout_mask = None\n",
    "            else:\n",
    "                self.masking_decay = None\n",
    "\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        self.dropout = min(1.0, max(0.0, dropout))\n",
    "        self.recurrent_dropout = min(1.0, max(0.0, recurrent_dropout))\n",
    "\n",
    "        self.reset_after = False\n",
    "        self.output_size = self.units\n",
    "\n",
    "        self._input_dim = None\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return (self.units, self._input_dim, self._input_dim)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "        self._input_dim = input_dim = input_shape[0][-1]\n",
    "\n",
    "        # NOTE: Disable caching device for GRUDCell to avoid potential errors.\n",
    "        # default_caching_device = rnn_utils.caching_device(self)\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_dim, self.units * 3),\n",
    "            name=\"kernel\",\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            # caching_device=default_caching_device,\n",
    "        )\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units * 3),\n",
    "            name=\"recurrent_kernel\",\n",
    "            initializer=self.recurrent_initializer,\n",
    "            regularizer=self.recurrent_regularizer,\n",
    "            constraint=self.recurrent_constraint,\n",
    "            # caching_device=default_caching_device,\n",
    "        )\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                shape=(3 * self.units,),\n",
    "                name=\"bias\",\n",
    "                initializer=self.bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "                # caching_device=default_caching_device,\n",
    "            )\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        if self.input_decay is not None:\n",
    "            self.input_decay_kernel = self.add_weight(\n",
    "                shape=(input_dim,),\n",
    "                name=\"input_decay_kernel\",\n",
    "                initializer=self.decay_initializer,\n",
    "                regularizer=self.decay_regularizer,\n",
    "                constraint=self.decay_constraint,\n",
    "                # caching_device=default_caching_device,\n",
    "            )\n",
    "            if self.use_decay_bias:\n",
    "                self.input_decay_bias = self.add_weight(\n",
    "                    shape=(input_dim,),\n",
    "                    name=\"input_decay_bias\",\n",
    "                    initializer=self.bias_initializer,\n",
    "                    regularizer=self.bias_regularizer,\n",
    "                    constraint=self.bias_constraint,\n",
    "                    # caching_device=default_caching_device,\n",
    "                )\n",
    "            else:\n",
    "                self.input_decay_bias = None\n",
    "        else:\n",
    "            self.input_decay_kernel = None\n",
    "\n",
    "        if self.hidden_decay is not None:\n",
    "            self.hidden_decay_kernel = self.add_weight(\n",
    "                shape=(self._input_dim, self.units),\n",
    "                name=\"hidden_decay_kernel\",\n",
    "                initializer=self.decay_initializer,\n",
    "                regularizer=self.decay_regularizer,\n",
    "                constraint=self.decay_constraint,\n",
    "                # caching_device=default_caching_device,\n",
    "            )\n",
    "            if self.use_decay_bias:\n",
    "                self.hidden_decay_bias = self.add_weight(\n",
    "                    shape=(self.units,),\n",
    "                    name=\"hidden_decay_bias\",\n",
    "                    initializer=self.bias_initializer,\n",
    "                    regularizer=self.bias_regularizer,\n",
    "                    constraint=self.bias_constraint,\n",
    "                    # caching_device=default_caching_device,\n",
    "                )\n",
    "            else:\n",
    "                self.hidden_decay_bias = None\n",
    "        else:\n",
    "            self.hidden_decay_kernel = None\n",
    "\n",
    "        if self.feed_masking:\n",
    "            self.masking_kernel = self.add_weight(\n",
    "                shape=(self._input_dim, self.units * 3),\n",
    "                name=\"masking_kernel\",\n",
    "                initializer=self.kernel_initializer,\n",
    "                regularizer=self.kernel_regularizer,\n",
    "                constraint=self.kernel_constraint,\n",
    "                # caching_device=default_caching_device,\n",
    "            )\n",
    "            if self.masking_decay is not None:\n",
    "                self.masking_decay_kernel = self.add_weight(\n",
    "                    shape=(self._input_dim,),\n",
    "                    name=\"masking_decay_kernel\",\n",
    "                    initializer=self.decay_initializer,\n",
    "                    regularizer=self.decay_regularizer,\n",
    "                    constraint=self.decay_constraint,\n",
    "                    # caching_device=default_caching_device,\n",
    "                )\n",
    "                if self.use_decay_bias:\n",
    "                    self.masking_decay_bias = self.add_weight(\n",
    "                        shape=(self._input_dim,),\n",
    "                        name=\"masking_decay_bias\",\n",
    "                        initializer=self.bias_initializer,\n",
    "                        regularizer=self.bias_regularizer,\n",
    "                        constraint=self.bias_constraint,\n",
    "                        # caching_device=default_caching_device,\n",
    "                    )\n",
    "                else:\n",
    "                    self.masking_decay_bias = None\n",
    "            else:\n",
    "                self.masking_decay_kernel = None\n",
    "        else:\n",
    "            self.masking_kernel = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "        \n",
    "    def get_dropout_mask_for_cell(self, inputs, training, count=3):\n",
    "        if training:\n",
    "            masks = []\n",
    "            for i in range(count):\n",
    "                masks.append(tf.nn.dropout(tf.ones_like(inputs[i]), self.dropout))\n",
    "            return masks\n",
    "        else:\n",
    "            return [tf.ones_like(inputs[i]) for i in range(count)]\n",
    "\n",
    "\n",
    "    def get_recurrent_dropout_mask_for_cell(self, h_tm1, training, count=1):\n",
    "        if training:\n",
    "            return [tf.nn.dropout(tf.ones_like(h_tm1), self.recurrent_dropout) for _ in range(count)]\n",
    "        else:\n",
    "            return [tf.ones_like(h_tm1) for _ in range(count)]\n",
    "\n",
    "    \n",
    "    def call(self, inputs, states, training=None):\n",
    "        # print('grudc-1',inputs)  # values:[None, feature_dim], mask:[None, feature_dim], times:[None,1]\n",
    "        # print('grudc-2',states)  # [None, n_hidden], [None, feature_dim], [None, feature_dim] \n",
    "        input_x, input_m, input_t = inputs\n",
    "        h_tm1, x_keep_tm1, t_prev_tm1 = states\n",
    "\n",
    "        delta_t = input_t - t_prev_tm1\n",
    "        # print('grudc-3',delta_t)  # [None, feature_dim]\n",
    "\n",
    "        dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=3)\n",
    "        # print('grudc-4',dp_mask)  # [None, feature_dim], [None, feature_dim], [None, 1]\n",
    "        rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(h_tm1, training, count=3)\n",
    "        # print('grudc-5',rec_dp_mask)  # [None, n_hidden], [None, n_hidden], [None, n_hidden]\n",
    "\n",
    "        if self.feed_masking:\n",
    "            if 0. < self.dropout < 1. and self._masking_dropout_mask is None:\n",
    "                self._masking_dropout_mask = self._create_dropout_mask(\n",
    "                    tf.ones_like(input_m, dtype=tf.float32), training=training, count=3,\n",
    "                )\n",
    "\n",
    "            m_dp_mask = self._masking_dropout_mask\n",
    "            # print('m_dp_mask',m_dp_mask)  # None\n",
    "        \n",
    "        if self.input_decay is not None:\n",
    "            gamma_di = delta_t * self.input_decay_kernel\n",
    "            if self.use_decay_bias:\n",
    "                gamma_di = K.bias_add(gamma_di, self.input_decay_bias)\n",
    "            gamma_di = self.input_decay(gamma_di)\n",
    "            # print('gamma_di',gamma_di)  # [None, feature_dim]\n",
    "\n",
    "        if self.hidden_decay is not None:\n",
    "            gamma_dh = K.dot(delta_t, self.hidden_decay_kernel)\n",
    "            if self.use_decay_bias:\n",
    "                gamma_dh = K.bias_add(gamma_dh, self.hidden_decay_bias)\n",
    "            gamma_dh = self.hidden_decay(gamma_dh)\n",
    "            # print('gamma_dh',gamma_dh)  # [None, n_hidden]\n",
    "\n",
    "        if self.feed_masking and self.masking_decay is not None:\n",
    "            gamma_dm = delta_t * self.masking_decay_kernel\n",
    "            if self.use_decay_bias:\n",
    "                gamma_dm = K.bias_add(gamma_dm, self.masking_decay_bias)\n",
    "            gamma_dm = self.masking_decay(gamma_dm)\n",
    "            # print('gamma_dm',gamma_dm)\n",
    "\n",
    "        # weighted sum between decayed last observation and grown 0 (empirical mean)\n",
    "#         x_keep_t = tf.where(input_m, input_x, x_keep_tm1)\n",
    "        x_keep_t = tf.where(input_m > 0.5, input_x, x_keep_tm1)\n",
    "        # print('grudc-6',x_keep_t)  # [None, feature_dim]\n",
    "#         x_t = tf.where(input_m, input_x, gamma_di * x_keep_t)\n",
    "        x_t = tf.where(input_m > 0.5, input_x, gamma_di * x_keep_t)\n",
    "        # print('grudc-7',x_t)  # [None, feature_dim]\n",
    "\n",
    "        if self.hidden_decay is not None:\n",
    "            h_tm1d = gamma_dh * h_tm1\n",
    "        else:\n",
    "            h_tm1d = h_tm1\n",
    "        # print('grudc-8',h_tm1d)  # [None, n_hidden]\n",
    "\n",
    "        if self.feed_masking:\n",
    "            m_t = 1. - tf.cast(input_m, tf.float32)\n",
    "            if self.masking_decay is not None:\n",
    "                m_t = gamma_dm * m_t\n",
    "            # print('m_t',m_t)  # [None, feature_dim]\n",
    "\n",
    "        x_z, x_r, x_h = x_t, x_t, x_t\n",
    "        # print('pass1d')\n",
    "\n",
    "        if 0. < self.dropout < 1.:\n",
    "            x_z *= dp_mask[0]\n",
    "            x_r *= dp_mask[1]\n",
    "            x_h *= dp_mask[2]\n",
    "\n",
    "            if self.feed_masking:\n",
    "                m_z, m_r, m_h = m_t * m_dp_mask[0], m_t * m_dp_mask[1], m_t * m_dp_mask[2]\n",
    "        else:\n",
    "            if self.feed_masking:\n",
    "                m_z, m_r, m_h = m_t, m_t, m_t\n",
    "\n",
    "        h_tm1_z, h_tm1_r = h_tm1d, h_tm1d\n",
    "        # print('pass2d')\n",
    "\n",
    "        if 0. < self.recurrent_dropout < 1.:\n",
    "            h_tm1_z *= rec_dp_mask[0]\n",
    "            h_tm1_r *= rec_dp_mask[1]\n",
    "\n",
    "        z_t = K.dot(x_z, self.kernel[:, : self.units])\n",
    "        r_t = K.dot(x_r, self.kernel[:, self.units : self.units * 2])\n",
    "        hh_t = K.dot(x_h, self.kernel[:, self.units * 2 :])\n",
    "\n",
    "        z_t = z_t + K.dot(h_tm1_z, self.recurrent_kernel[:, : self.units])\n",
    "        r_t = r_t + K.dot(h_tm1_r, self.recurrent_kernel[:, self.units : self.units * 2])\n",
    "        # print('pass3d')\n",
    "\n",
    "        if self.feed_masking:\n",
    "            z_t += K.dot(m_z, self.masking_kernel[:, : self.units])\n",
    "            r_t += K.dot(m_r, self.masking_kernel[:, self.units : self.units * 2])\n",
    "            hh_t += K.dot(m_h, self.masking_kernel[:, self.units * 2 :])\n",
    "\n",
    "        if self.use_bias:\n",
    "            z_t = K.bias_add(z_t, self.bias[: self.units])\n",
    "            r_t = K.bias_add(r_t, self.bias[self.units : self.units * 2])\n",
    "            hh_t = K.bias_add(hh_t, self.bias[self.units * 2 :])\n",
    "\n",
    "        z_t = self.recurrent_activation(z_t)\n",
    "        r_t = self.recurrent_activation(r_t)\n",
    "        # print('pass4d')\n",
    "\n",
    "        if 0. < self.recurrent_dropout < 1.:\n",
    "            h_tm1_h = r_t * h_tm1d * rec_dp_mask[2]\n",
    "        else:\n",
    "            h_tm1_h = r_t * h_tm1d\n",
    "\n",
    "        hh_t = self.activation(hh_t + K.dot(h_tm1_h, self.recurrent_kernel[:, self.units * 2 :]))\n",
    "        # print('pass5d')\n",
    "        h_t = z_t * h_tm1 + (1 - z_t) * hh_t\n",
    "        # print('pass6d')\n",
    "#         t_prev_t = tf.where(input_m, K.tile(input_t, [1, self.state_size[-1]]), t_prev_tm1)\n",
    "        t_prev_t = tf.where(input_m > 0.5, K.tile(input_t, [1, self.state_size[-1]]), t_prev_tm1)\n",
    "        # print('pass7d')\n",
    "        # print('grudc-9',h_t)  # [None, n_hidden]\n",
    "        # print('grudc-10',x_keep_t)  # [None, feature_dim]\n",
    "        # print('grudc-11',t_prev_t)  # [None, feature_dim]\n",
    "        return h_t, (h_t, x_keep_t, t_prev_t)\n",
    "\n",
    "    def reset_masking_dropout_mask(self):\n",
    "        self._masking_dropout_mask = None\n",
    "\n",
    "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "        dtype = dtype or self.dtype\n",
    "        if inputs is None:\n",
    "            return (\n",
    "                tf.zeros([batch_size, self.units], dtype=dtype),\n",
    "                tf.zeros([batch_size, self._input_dim], dtype=dtype),\n",
    "                tf.zeros([batch_size, self._input_dim], dtype=dtype)\n",
    "            )\n",
    "        else:\n",
    "            if self.go_backwards:\n",
    "                initial_t = tf.reduce_max(inputs[2], axis=1, keepdims=True)\n",
    "            else:\n",
    "                initial_t = tf.expand_dims(inputs[2][:, 0], axis=1)\n",
    "\n",
    "            input_dim = inputs[0].shape[-1]\n",
    "\n",
    "            return (\n",
    "                tf.zeros([batch_size, self.units], dtype=dtype),\n",
    "                tf.zeros([batch_size, input_dim], dtype=dtype),\n",
    "                tf.tile(initial_t, [1, input_dim])\n",
    "            )\n",
    "\n",
    "\n",
    "class GRUD(layers.RNN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        x_imputation=\"decay\",\n",
    "        input_decay=\"exp_relu\",\n",
    "        hidden_decay=\"exp_relu\",\n",
    "        masking_decay=None,\n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"hard_sigmoid\",\n",
    "        use_bias=True,\n",
    "        use_decay_bias=True,\n",
    "        feed_masking=True,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        recurrent_initializer=\"orthogonal\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        decay_initializer=CONSTANT_INIT,\n",
    "        kernel_regularizer=None,\n",
    "        recurrent_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        decay_regularizer=None,\n",
    "        activity_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        recurrent_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        decay_constraint=None,\n",
    "        dropout=0.,\n",
    "        recurrent_dropout=0.,\n",
    "        return_sequences=False,\n",
    "        return_state=False,\n",
    "        go_backwards=False,\n",
    "        stateful=False,\n",
    "        unroll=False,\n",
    "        time_major=False,\n",
    "        reset_after=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        cell = GRUDCell(\n",
    "            units,\n",
    "            activation=activation,\n",
    "            recurrent_activation=recurrent_activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            recurrent_initializer=recurrent_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            recurrent_regularizer=recurrent_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            recurrent_constraint=recurrent_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            reset_after=reset_after,\n",
    "            x_imputation=x_imputation,\n",
    "            input_decay=input_decay,\n",
    "            hidden_decay=hidden_decay,\n",
    "            use_decay_bias=use_decay_bias,\n",
    "            feed_masking=feed_masking,\n",
    "            masking_decay=masking_decay,\n",
    "            decay_initializer=decay_initializer,\n",
    "            decay_regularizer=decay_regularizer,\n",
    "            decay_constraint=decay_constraint,\n",
    "            dtype=kwargs.get(\"dtype\"),\n",
    "            trainable=kwargs.get(\"trainable\", True),\n",
    "        )\n",
    "        super().__init__(\n",
    "            cell,\n",
    "            return_sequences=return_sequences,\n",
    "            return_state=return_state,\n",
    "            go_backwards=go_backwards,\n",
    "            stateful=stateful,\n",
    "            unroll=unroll,\n",
    "            time_major=time_major,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "    def call(self, inputs, mask=None, training=None, initial_state=None):\n",
    "        # print('grud-1',inputs)  # [None, time_len, feature_dim]\n",
    "        # print('grud-2',mask)  # None\n",
    "        def reset_dropout_mask():\n",
    "            self.cell.dropout_mask = None\n",
    "            \n",
    "        def reset_recurrent_dropout_mask():\n",
    "            self.cell.recurrent_dropout_mask = None\n",
    "            \n",
    "        def reset_masking_dropout_mask():\n",
    "            self.cell.masking_dropout_mask = None\n",
    "\n",
    "        reset_dropout_mask()\n",
    "        reset_recurrent_dropout_mask()\n",
    "        reset_masking_dropout_mask()\n",
    "        # print('pass1')\n",
    "        \n",
    "        # The input should be dense, padded with zeros. If a ragged input is fed\n",
    "        # into the layer, it is padded and the row lengths are used for masking.\n",
    "        if isinstance(inputs, tf.RaggedTensor):\n",
    "            is_ragged_input = True\n",
    "            inputs, row_lengths = inputs.to_tensor(), inputs.row_lengths()\n",
    "        else:\n",
    "            is_ragged_input = False\n",
    "            row_lengths = None\n",
    "\n",
    "        self._validate_args_if_ragged(is_ragged_input, mask)\n",
    "        \n",
    "        # GRU does not support constants. Ignore it during process.\n",
    "        inputs, initial_state, _ = self._process_inputs(\n",
    "            inputs, initial_state, None\n",
    "        )\n",
    "        \n",
    "        # print('pass2')\n",
    "        if isinstance(mask, list):\n",
    "            mask = mask[0]\n",
    "        # print('pass3')\n",
    "        input_shape = K.int_shape(inputs[0])\n",
    "        # print('grud-3',input_shape)  # (None, time_len, feature_dim)\n",
    "        timesteps = input_shape[0] if self.time_major else input_shape[1]\n",
    "        # print('grud-4',timesteps)  # time_len\n",
    "\n",
    "        kwargs = {\"training\": training}\n",
    "        # print('pass4')\n",
    "        self._maybe_reset_cell_dropout_mask(self.cell)\n",
    "        # print('pass5')\n",
    "\n",
    "        def step(cell_inputs, cell_states):\n",
    "            return self.cell(cell_inputs, cell_states, **kwargs)\n",
    "\n",
    "        last_output, outputs, states = K.rnn(\n",
    "            step,\n",
    "            inputs,\n",
    "            initial_state,\n",
    "            constants=None,\n",
    "            go_backwards=self.go_backwards,\n",
    "            mask=mask,\n",
    "            unroll=self.unroll,\n",
    "            input_length=(row_lengths if row_lengths is not None else timesteps),\n",
    "            time_major=self.time_major,\n",
    "            zero_output_for_mask=self.zero_output_for_mask,\n",
    "#             return_all_outputs=self.return_sequences,\n",
    "        )\n",
    "        # print('grud-5',last_output)  # (None, n_hidden)\n",
    "        # print('grud-6',outputs)  # (None, time_len, n_hidden)\n",
    "        # print('grud-7',states)  # (None,n_hidden), (None,feature_dim), (None,feature_dim)\n",
    "        if self.stateful:\n",
    "            updates = [self.states[0].assign(states[0])]\n",
    "            self.add_update(updates)\n",
    "        # print('pass6')\n",
    "        if self.return_sequences:\n",
    "            if is_ragged_input:\n",
    "                output = tf.RaggedTensor.from_tensor(outputs, lengths=row_lengths)\n",
    "            else:\n",
    "                output = outputs\n",
    "        else:\n",
    "            output = last_output\n",
    "        # print('grud-8',output)  # (None, time_len, n_hidden)\n",
    "        if self.return_state:\n",
    "            return [output] + list(states)\n",
    "        else:\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a6d5701",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSTANT_INIT = initializers.Constant(0.05)\n",
    "InterpolateInput = namedtuple(\"InterpolateInput\", [\"values\", \"mask\", \"times\"])\n",
    "InterpolateState = namedtuple(\"InterpolateState\", [\"x_keep\", \"t_keep\"])\n",
    "\n",
    "\n",
    "class DecayCell(layers.AbstractRNNCell):\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_bias=True,\n",
    "        decay=\"exp_softplus\",\n",
    "        decay_initializer=\"zeros\",\n",
    "        bias_initializer=CONSTANT_INIT,\n",
    "        decay_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        decay_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.use_bias = use_bias\n",
    "        with tf.keras.utils.custom_object_scope({\"exp_relu\": exp_relu, \"exp_softplus\": exp_softplus}):\n",
    "            self.decay = None if decay is None else activations.get(decay)\n",
    "\n",
    "        self.decay_initializer = initializers.get(decay_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.decay_regularizer = regularizers.get(decay_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.decay_constraint = constraints.get(decay_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._input_dim = input_shape[0][-1]\n",
    "\n",
    "        self.decay_kernel = self.add_weight(\n",
    "            shape=(self._input_dim,),\n",
    "            name=\"decay_kernel\",\n",
    "            initializer=self.decay_initializer,\n",
    "            regularizer=self.decay_regularizer,\n",
    "            constraint=self.decay_constraint,\n",
    "        )\n",
    "        if self.use_bias:\n",
    "            self.decay_bias = self.add_weight(\n",
    "                shape=(self._input_dim,),\n",
    "                name=\"decay_bias\",\n",
    "                initializer=self.bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "            )\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        # print('dc-1',inputs)  # [None, feature_dim], [None, feature_dim], [None, 1]\n",
    "        # print('dc-2',states)  # [None, feature_dim], [None, feature_dim]\n",
    "        x_input, m_input, t_input = inputs\n",
    "        x_last, t_last = states\n",
    "        t_delta = t_input - t_last\n",
    "        # print('dc-3',t_delta)  # [None, feature_dim]\n",
    "\n",
    "        gamma_di = t_delta * self.decay_kernel\n",
    "        # print('dc-4',gamma_di)  # [None, feature_dim]\n",
    "        if self.use_bias:\n",
    "            gamma_di = K.bias_add(gamma_di, self.decay_bias)\n",
    "        gamma_di = self.decay(gamma_di)\n",
    "        # print('dc-5',gamma_di)  # [None, feature_dim]\n",
    "\n",
    "        m_input = tf.cast(m_input, tf.bool)\n",
    "        # print('m',m_input)\n",
    "        # print('x',x_input)\n",
    "        # print('g',gamma_di)\n",
    "        # print('x_l',x_last)\n",
    "        x_t    = tf.where(m_input, x_input, gamma_di * x_last + (1 - gamma_di) * x_input)\n",
    "        # print('dc-6',x_t)  # [None, feature_dim]\n",
    "        # x_t    = tf.where(m_input, x_input, gamma_di * x_last)\n",
    "        x_keep = tf.where(m_input, x_input, x_last)\n",
    "        # print('dc-7',x_keep)  # [None, feature_dim]\n",
    "        t_keep = tf.where(m_input, t_input, t_last)\n",
    "        # print('dc-8',t_keep)  # [None, feature_dim]\n",
    "        return x_t, InterpolateState(x_keep, t_keep)\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return InterpolateState(x_keep=self._input_dim, t_keep=self._input_dim)\n",
    "\n",
    "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "        if inputs is None:\n",
    "            return InterpolateState(\n",
    "                tf.zeros((batch_size, self._input_dim), dtype=dtype),\n",
    "                tf.zeros((batch_size, self._input_dim), dtype=dtype),\n",
    "            )\n",
    "        else:\n",
    "            return InterpolateState(\n",
    "                tf.zeros((batch_size, self._input_dim), dtype=dtype),\n",
    "                inputs.times[:, 0, :],\n",
    "            )\n",
    "\n",
    "\n",
    "class ForwardCell(layers.AbstractRNNCell):\n",
    "    def build(self, input_shape):\n",
    "        self._input_dim = input_shape[0][-1]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        x_input, m_input, t_input = inputs\n",
    "        x_last, t_last = states\n",
    "\n",
    "        x_keep = tf.where(m_input, x_input, x_last)\n",
    "        t_keep = tf.where(m_input, t_input, t_last)\n",
    "        state = InterpolateState(x_keep, t_keep)\n",
    "        return state, state\n",
    "\n",
    "    def state_size(self):\n",
    "        return InterpolateState(x_keep=self._input_dim, t_keep=self._input_dim)\n",
    "\n",
    "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "        if inputs is None:\n",
    "            return InterpolateState(\n",
    "                tf.zeros((batch_size, self._input_dim), dtype=dtype),\n",
    "                tf.zeros((batch_size, self._input_dim), dtype=dtype),\n",
    "            )\n",
    "        else:\n",
    "            return InterpolateState(\n",
    "                tf.zeros((batch_size, self._input_dim), dtype=dtype),\n",
    "                tf.reduce_max(inputs.times, axis=1) if self.go_backwards else inputs.times[:, 0, :],\n",
    "            )\n",
    "\n",
    "\n",
    "class LinearScan(layers.RNN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unroll=False,\n",
    "        go_backwards=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        cell = ForwardCell(\n",
    "            dtype=kwargs.get(\"dtype\"),\n",
    "            trainable=kwargs.get(\"trainable\", False),\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            cell,\n",
    "            return_sequences=True,\n",
    "            return_state=False,\n",
    "            go_backwards=go_backwards,\n",
    "            stateful=False,\n",
    "            unroll=unroll,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "class DecayInterpolate(layers.RNN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_bias=True,\n",
    "        decay=\"exp_relu\",\n",
    "        decay_initializer=\"zeros\",\n",
    "        bias_initializer=CONSTANT_INIT,\n",
    "        decay_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        decay_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        unroll=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        cell = DecayCell(\n",
    "            use_bias=use_bias,\n",
    "            decay=decay,\n",
    "            decay_initializer=decay_initializer,\n",
    "            decay_regularizer=decay_regularizer,\n",
    "            decay_constraint=decay_constraint,\n",
    "            bias_initializer=bias_initializer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            bias_constraint=bias_constraint,\n",
    "            dtype=kwargs.get(\"dtype\"),\n",
    "            trainable=kwargs.get(\"trainable\", True),\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            cell,\n",
    "            return_sequences=True,\n",
    "            return_state=False,\n",
    "            go_backwards=False,\n",
    "            stateful=False,\n",
    "            unroll=unroll,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "class LinearInterpolate(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unroll=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.forward_scan  = LinearScan(unroll=unroll, go_backwards=False, **kwargs)\n",
    "        self.backward_scan = LinearScan(unroll=unroll, go_backwards=True,  **kwargs)\n",
    "\n",
    "    def call(self, inputs, mask=None, training=None):\n",
    "        forwards  = self.forward_scan(inputs, mask=mask, training=training)\n",
    "        backwards = self.backward_scan(inputs, mask=mask, training=training)\n",
    "\n",
    "        x_t, t = inputs.values, inputs.times\n",
    "        x_last, t_last = forwards.x_keep, forwards.t_keep\n",
    "        x_next = tf.reverse(backwards.x_keep, axis=[1])\n",
    "        t_next = tf.reverse(backwards.t_keep, axis=[1])\n",
    "\n",
    "        # Linear interpolation. See https://en.wikipedia.org/wiki/Linear_interpolation\n",
    "        x_itp = (x_last * (t_next - t) + x_next * (t - t_last)) / (t_next - t_last)\n",
    "        x_itp = tf.where(tf.math.is_nan(x_itp), 0., x_itp)\n",
    "        x_itp = tf.where(tf.math.is_inf(x_itp), 0., x_itp)\n",
    "\n",
    "        return tf.where(inputs.mask, x_t, x_itp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac859494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupNotMIWAE(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_hidden:int=128, \n",
    "        n_train_latents:int = 10, \n",
    "        n_train_samples:int=1, \n",
    "        observe_dropout=None, \n",
    "        return_sequences=True, \n",
    "        z_dim:int = 32,\n",
    "        min_latent_sigma: float = 0.0001,\n",
    "        min_sigma: float = 0.001,\n",
    "    ):\n",
    "        super(SupNotMIWAE, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.feature_dim = None\n",
    "        self.n_train_samples = n_train_samples\n",
    "        self.n_train_latents = n_train_latents\n",
    "        self.return_sequences = return_sequences\n",
    "        self.z_dim = z_dim\n",
    "        self.min_latent_sigma = min_latent_sigma\n",
    "        self.min_sigma = min_sigma\n",
    "        \n",
    "        if observe_dropout is None:\n",
    "            self.observe_dropout = tf.constant(0., dtype=tf.float32)\n",
    "        else:\n",
    "            self.observe_dropout = observe_dropout\n",
    "\n",
    "        # Encoder with GRU-D\n",
    "        self.encoder = GRUD(self.n_hidden, return_sequences=True, x_imputation=\"decay\", feed_masking=True, name=\"encoder\")\n",
    "        min_softplus = lambda x: self.min_latent_sigma + (1 - self.min_latent_sigma) * tf.nn.softplus(x)\n",
    "        self.encoder_mu    = layers.Dense(self.z_dim, activation=None, name=\"encoder/mu\")\n",
    "        self.encoder_sigma = layers.Dense(self.z_dim, activation=min_softplus, name=\"encoder/sigma\")\n",
    "\n",
    "        # Prior\n",
    "        self.prior_gru = layers.GRU(self.z_dim, return_sequences=True,   name=\"prior/gru\")\n",
    "        self.prior_mu  = layers.Dense(self.z_dim, activation=None,         name=\"prior/mu\")\n",
    "        self.prior_std = layers.Dense(self.z_dim, activation=min_softplus, name=\"prior/std\")\n",
    "        \n",
    "        # Decoder with GRU\n",
    "        self.decoder = layers.GRU(self.n_hidden, return_sequences=True, name=\"decoder\")\n",
    "        min_softplus = lambda x: self.min_sigma + (1 - self.min_sigma) * tf.nn.softplus(x)\n",
    "        self.decoder_mu    = layers.Dense(OrigDim, activation=None,         name=\"decoder/mu\")\n",
    "        self.decoder_sigma = layers.Dense(OrigDim, activation=min_softplus, name=\"decoder/sigma\")\n",
    "        \n",
    "        # Impute\n",
    "        self.interpolator = DecayInterpolate(name=\"interpolator\")\n",
    "\n",
    "    def encode(self, times, x_obs, missing_mask):\n",
    "        # print('enc-1',times)  # [batch, time_len]\n",
    "        # print('enc-2',x_obs)  # [batch, time_len, feature_dim]\n",
    "        # print('enc-3',missing_mask)  # [batch, time_len, feature_dim]\n",
    "        times = tf.expand_dims(times, axis=-1)\n",
    "        # print('enc-4',times)  # [batch, time_len, 1]\n",
    "    \n",
    "        r = self.encoder(GRUDInput(x_obs, missing_mask, times))\n",
    "        # print('enc-5',r)  # [batch, time_len, n_hidden]\n",
    "        z_mu = self.encoder_mu(r)\n",
    "        # print('enc-6',z_mu)  # [batch, time_len, z_dim]\n",
    "        z_sigma = self.encoder_sigma(r)\n",
    "        # print('enc-7',z_sigma)  # [batch, time_len, z_dim]\n",
    "        \n",
    "        q_z = tfd.Normal(loc=z_mu, scale=z_sigma)\n",
    "        # print('enc-8',q_z)  # batch_shape=[?, time_len, z_dim]\n",
    "        return q_z   \n",
    "        \n",
    "    def prior(self, z_samples):\n",
    "        # print('pr-1',z_samples)  # [n_latents, batch, time_len, z_dim]\n",
    "        shape = tf.shape(z_samples)\n",
    "        # print('pr-2',shape)\n",
    "        z_samples = tf.reshape(z_samples, shape=(-1, Length, self.z_dim))\n",
    "        # print('pr-3',z_samples)  # [n_latents*batch, time_len, z_dim]\n",
    "        r = self.prior_gru(z_samples[:,1:,:],initial_state=z_samples[:,0,:])\n",
    "        # print('pr-4',r)  # [n_latents*batch, time_len-1, z_dim]\n",
    "        r = tf.reshape(r, shape=(self.n_train_latents, -1, Length-1, self.z_dim))\n",
    "        # print('pr-5',r)  # [n_latents, batch, time_len-1, z_dim]\n",
    "        p_z_mu = self.prior_mu(r)\n",
    "        # print('pr-6',p_z_mu)  # [n_latents, batch, time_len-1, z_dim]\n",
    "        p_z_sigma = self.prior_std(r)\n",
    "        # print('pr-7',p_z_sigma)  # [n_latents, batch, time_len-1, z_dim]\n",
    "        p_z_mu    = tf.pad(p_z_mu,    [[0, 0], [0, 0], [1, 0], [0, 0]], constant_values=0)\n",
    "        # print('pr-8',p_z_mu)  # [n_latents, batch, time_len, z_dim]\n",
    "        p_z_sigma = tf.pad(p_z_sigma, [[0, 0], [0, 0], [1, 0], [0, 0]], constant_values=1)\n",
    "        # print('pr-9',p_z_sigma)  # [n_latents, batch, time_len, z_dim]\n",
    "        p_z = tfd.Normal(loc=p_z_mu, scale=p_z_sigma)\n",
    "        # print('pr-10',p_z)  # batch_shape=[n_latents, ?, time_len, z_dim]\n",
    "        \n",
    "        return p_z\n",
    "\n",
    "    def decode(self, times, z):\n",
    "        # print('dec-1',times)  # [batch, time_len]\n",
    "        # print('dec-2',z)  # [n_latents, batch, time_len, z_dim]\n",
    "        shape = tf.shape(z)\n",
    "        z = tf.reshape(z, shape=(-1, Length, self.z_dim))\n",
    "        # print('dec-3',z)  # [n_latents*batch, time_len, z_dim]\n",
    "        h = self.decoder(z)\n",
    "        # print('dec-4',h)  # [n_latents*batch, time_len, n_hidden]\n",
    "        h = tf.reshape(h, shape=(self.n_train_latents, -1, Length, self.n_hidden))\n",
    "        # print('dec-5',h)  # [n_latents, batch, time_len, n_hidden]\n",
    "        x_tilde_mu = self.decoder_mu(h)\n",
    "        # print('dec-6',x_tilde_mu)  # [n_latents, batch, time_len, feature_dim]\n",
    "        x_tilde_sigma = self.decoder_sigma(h)\n",
    "        # print('dec-7',x_tilde_sigma)  # [n_latents, batch, time_len, feature_dim]\n",
    "        p_x_tilde = tfd.Normal(loc=x_tilde_mu, scale=x_tilde_sigma)\n",
    "        # print('dec-8',p_x_tilde)  # batch_shape=[n_latents, ?, time_len, feature_dim]\n",
    "                \n",
    "        return p_x_tilde\n",
    "\n",
    "    def missing_model_func(self, x):\n",
    "        return self.missing_model(x)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        x_obs, missing_mask = inputs\n",
    "        # print('main-1',x_obs)  # x_obs shape: [batch, time_len, feature_dim]\n",
    "        # print('main-2',missing_mask)  # missing_mask shape: [batch, time_len, feature_dim]\n",
    "        \n",
    "        last_observed, mean = compute_last_observed_and_mean(x_obs, missing_mask)\n",
    "        # print('main-3',last_observed)  # last_observed shape: [batch, time_len, feature_dim]\n",
    "        # print('main-4',mean)  # mean shape: [batch, time_len]\n",
    "        \n",
    "        delta_t = compute_delta_t(missing_mask)\n",
    "        # print('main-5',delta_t)  # delta_t shape: [batch, time_len, feature_dim]\n",
    "        \n",
    "        single_time_array = tf.range(0, x_obs.shape[1] * 0.5, 0.5, dtype=tf.float32)\n",
    "        dynamic_shape = tf.shape(x_obs)\n",
    "        times = tf.broadcast_to(single_time_array, [dynamic_shape[0], dynamic_shape[1]])\n",
    "        # print('main-6',times)  # times shape: [batch, time_len]\n",
    "        \n",
    "        # Set feature_dim based on x_obs if not already set\n",
    "        if self.feature_dim is None:\n",
    "            self.feature_dim = x_obs.shape[-1]\n",
    "            \n",
    "        n_samples = self.n_train_samples\n",
    "        n_latents = self.n_train_latents\n",
    "        \n",
    "        # === VAE ===\n",
    "\n",
    "        # Encoder\n",
    "        q_z = self.encode(times, x_obs, missing_mask)\n",
    "        # print('main-7',q_z)  # batch_shape=[?, time_len, z_dim]\n",
    "        \n",
    "        # Latent\n",
    "        z_samples = q_z.sample(n_latents)\n",
    "        # print('main-8',z_samples)  # [n_latents, batch, time_len, z_dim]\n",
    "        \n",
    "        # Prior\n",
    "        p_z = self.prior(z_samples)\n",
    "        # print('main-9',p_z)  # batch_shape=[n_latents, ?, time_len, z_dim]\n",
    "        \n",
    "        # Decoder\n",
    "        p_x_tilde = self.decode(times, z_samples)\n",
    "        # print('main-10',p_x_tilde)  # batch_shape=[n_latents, ?, time_len, feature_dim]\n",
    "        \n",
    "        # Impute\n",
    "                \n",
    "        # log p(xᵒ | zₖ)\n",
    "        log_p_x_obs_given_z = tf.reduce_sum(tf.where(\n",
    "            tf.cast(missing_mask, tf.bool),\n",
    "            p_x_tilde.log_prob(x_obs),\n",
    "            0.\n",
    "        ), axis=-1)\n",
    "        # print('main-11',log_p_x_obs_given_z)  # [n_latents, batch, time_len]\n",
    "\n",
    "        log_p_z = p_z.log_prob(z_samples)\n",
    "        # print('main-12',log_p_z)  # [n_latents, batch, time_len, z_dim]\n",
    "        log_q_z_given_x_obs = q_z.log_prob(z_samples)\n",
    "        # print('main-13',log_q_z_given_x_obs)  # [n_latents, batch, time_len, z_dim]\n",
    "        \n",
    "        if self.return_sequences:\n",
    "            log_p_x_obs_given_z = tf.cumsum(log_p_x_obs_given_z, axis=-1)\n",
    "            log_p_z = tf.cumsum(log_p_z, axis=-1)\n",
    "            log_q_z_given_x_obs = tf.cumsum(log_q_z_given_x_obs, axis=-1)\n",
    "        else:\n",
    "            log_p_x_obs_given_z = tf.reduce_sum(log_p_x_obs_given_z, axis=-1, keepdims=True)\n",
    "            log_p_z = tf.reduce_sum(log_p_z, axis=-1, keepdims=True)\n",
    "            log_q_z_given_x_obs = tf.reduce_sum(log_q_z_given_x_obs, axis=-1, keepdims=True)\n",
    "        # print('main-14',log_p_x_obs_given_z)  # [n_latents, batch, time_len]\n",
    "        # print('main-15',log_p_z)  # [n_latents, batch, time_len, z_dim]\n",
    "        # print('main-16',log_q_z_given_x_obs)  # [n_latents, batch, time_len, z_dim]\n",
    "\n",
    "        \n",
    "        # Generate: xₖⱼᵐ ~ p(xₖᵐ | zₖ)\n",
    "        x_tilde = self.generate(p_x_tilde, training=training)\n",
    "        # print('main-17',x_tilde)  # [n_samples, n_latents, batch, time_len, feature_dim]\n",
    "\n",
    "        # Dropout\n",
    "        drop_mask, log_m = self.generate_observe_dropout_mask(tf.shape(x_tilde), missing_mask, training=training)\n",
    "        # print('main-18',drop_mask)  # [None, None, None, time_len, feature_dim]\n",
    "        # print('main-19',log_m)  # [n_samples, n_latents, batch, time_len]\n",
    "        \n",
    "        # print('main-check',drop_mask)\n",
    "\n",
    "        # Impute\n",
    "        x_impute = self.impute(times, x_obs, x_tilde, drop_mask)\n",
    "        # print('main-20',x_impute)   # [batch, time_len, feature_dim]\n",
    "        \n",
    "#         # Reconstruction error = p(xᵒ | z)\n",
    "#         recon_error = -tf.reduce_mean(log_p_x_obs_given_z)\n",
    "#         # print('main-21',recon_error)\n",
    "        \n",
    "        return x_impute\n",
    "    \n",
    "    \n",
    "    def generate(self, p_x_tilde, training=True):\n",
    "        \n",
    "        n_samples = self.n_train_samples\n",
    "\n",
    "        # xₖⱼᵐ ~ p(xₖᵐ | zₖ)\n",
    "        x_tilde = p_x_tilde.sample(n_samples)\n",
    "\n",
    "        return x_tilde\n",
    "\n",
    "    def generate_observe_dropout_mask(self, shape, missing_mask, training=True):\n",
    "        missing_mask = tf.cast(missing_mask, tf.bool)\n",
    "        if tf.reduce_any(self.observe_dropout > 0.) and training:\n",
    "            p_m = tfd.Bernoulli(probs=(1. - self.observe_dropout))\n",
    "            drop_mask = p_m.sample(shape if self.observe_dropout.ndim == 0 else shape[:-1])\n",
    "\n",
    "            log_m = tf.reduce_sum(tf.where(\n",
    "                missing_mask,\n",
    "                p_m.log_prob(drop_mask),\n",
    "                0.,\n",
    "            ), axis=-1)\n",
    "\n",
    "            drop_mask = tf.cast(drop_mask, dtype=bool) & missing_mask\n",
    "\n",
    "        else:\n",
    "            drop_mask = tf.broadcast_to(missing_mask, shape)\n",
    "\n",
    "            log_m = tf.zeros(shape[:4])\n",
    "\n",
    "        if self.return_sequences:\n",
    "            log_m = tf.cumsum(log_m, axis=-1)\n",
    "        else:\n",
    "            log_m = tf.reduce_sum(log_m, axis=-1, keepdims=True)\n",
    "\n",
    "        return drop_mask, log_m\n",
    "\n",
    "    def impute(self, times, x_obs, x_tilde, missing_mask):\n",
    "        # print('imp-1',times)  # [batch, time_len]\n",
    "        # print('imp-2',x_obs)  # [batch, time_len, feature_dim]\n",
    "        # print('imp-3',x_tilde)  # [n_samples, n_latents, batch, time_len, feature_dim]\n",
    "        # print('imp-4',missing_mask)  # [batch, time_len, feature_dim]\n",
    "        # Interpolate obs and combine with generated missing\n",
    "        x_comb = tf.where(tf.cast(missing_mask,tf.bool), x_obs, x_tilde)\n",
    "        # print('imp-5',x_comb)  # [n_samples, n_latents, batch, time_len, feature_dim]\n",
    "\n",
    "        shape = tf.shape(x_comb)\n",
    "        n_tiles = self.n_train_samples * self.n_train_latents\n",
    "        \n",
    "        times = tf.expand_dims(tf.tile(times, multiples=[n_tiles, 1]), axis=-1)\n",
    "        # print('imp-6',times)  # [n_samples*n_latents*batch, time_len, 1]\n",
    "        x_comb = tf.reshape(x_comb, shape=[-1, Length, OrigDim])\n",
    "        # print('imp-7',x_comb)  # [n_samples*n_latents*batch, time_len, feature_dim]\n",
    "        missing_mask = tf.reshape(missing_mask, shape=[n_tiles * shape[2], shape[3], shape[4]])\n",
    "        # print('imp-8',missing_mask)  # [n_samples*n_latents*batch, time_len, feature_dim]\n",
    "        # print('imp1:',x_comb)\n",
    "        # print('imp2:',missing_mask)\n",
    "        # print('imp3:',times)\n",
    "\n",
    "        x_impute = self.interpolator(InterpolateInput(values=x_comb, mask=missing_mask, times=times))\n",
    "        # print('imp-9',x_impute)  # [n_samples*n_latents*batch, time_len, feature_dim]\n",
    "        x_impute = tf.reshape(x_impute, shape=[shape[0], shape[1], shape[2], shape[3], shape[4]])   \n",
    "        # print('imp-10',x_impute)  # [n_samples, n_latents, batch, time_len, feature_dim]\n",
    "        \n",
    "        x_impute_mean = tf.reduce_mean(x_impute, axis=[0,1])\n",
    "        # print('imp-11',x_impute_mean)\n",
    "\n",
    "        return x_impute_mean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d6bdd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "Length = Tr_X.shape[1]\n",
    "OrigDim = Tr_X.shape[-1]\n",
    "InpLayer = tf.keras.layers.Input((Length, OrigDim))\n",
    "InpMask  = tf.keras.layers.Input((Length, OrigDim))\n",
    "ImputeOut = SupNotMIWAE(n_train_latents=10, n_train_samples=1)([InpLayer, InpMask])\n",
    "\n",
    "learning_rate = 0.0005  # replace this with your desired learning rate\n",
    "decay = 1e-6\n",
    "adam = tf.keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=decay)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "SNM = Model([InpLayer,InpMask ], ImputeOut)\n",
    "SNM.compile(loss='mse', optimizer = adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84180ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# ModelCheckpoint 설정\n",
    "checkpoint_filepath = './results/SNM_epoch{epoch:d}_loss{loss:.7f}_valloss{val_loss:.7f}.hdf5'  # 저장할 파일 경로 및 이름\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',  # 검증 데이터의 loss를 기준으로 함\n",
    "    mode='min',  # loss의 최솟값을 모니터링\n",
    "    save_best_only=True  # 가장 낮은 loss 값을 가질 때만 저장\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c18973c2-a9e7-4520-821d-b2d299f0281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNM.load_weights('./results/SNM_epoch2975_loss0.0002969_valloss0.0003326.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12bbc0ad-4096-47d8-881c-4ed1a3853a8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['sup_not_miwae_8/prior/gru/gru_cell_16/kernel:0', 'sup_not_miwae_8/prior/gru/gru_cell_16/recurrent_kernel:0', 'sup_not_miwae_8/prior/gru/gru_cell_16/bias:0', 'sup_not_miwae_8/prior/mu/kernel:0', 'sup_not_miwae_8/prior/mu/bias:0', 'sup_not_miwae_8/prior/std/kernel:0', 'sup_not_miwae_8/prior/std/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['sup_not_miwae_8/prior/gru/gru_cell_16/kernel:0', 'sup_not_miwae_8/prior/gru/gru_cell_16/recurrent_kernel:0', 'sup_not_miwae_8/prior/gru/gru_cell_16/bias:0', 'sup_not_miwae_8/prior/mu/kernel:0', 'sup_not_miwae_8/prior/mu/bias:0', 'sup_not_miwae_8/prior/std/kernel:0', 'sup_not_miwae_8/prior/std/bias:0'] when minimizing the loss.\n",
      "189/189 [==============================] - 88s 448ms/step - loss: 5.5457e-04 - val_loss: 3.5835e-04\n",
      "Epoch 2/10000\n",
      "189/189 [==============================] - 83s 441ms/step - loss: 3.2028e-04 - val_loss: 3.4766e-04\n",
      "Epoch 3/10000\n",
      "189/189 [==============================] - 84s 443ms/step - loss: 3.1600e-04 - val_loss: 3.5524e-04\n",
      "Epoch 4/10000\n",
      "189/189 [==============================] - 84s 446ms/step - loss: 3.1319e-04 - val_loss: 3.3709e-04\n",
      "Epoch 5/10000\n",
      "189/189 [==============================] - 84s 443ms/step - loss: 3.1215e-04 - val_loss: 3.4948e-04\n",
      "Epoch 6/10000\n",
      "189/189 [==============================] - 83s 438ms/step - loss: 3.1486e-04 - val_loss: 3.4326e-04\n",
      "Epoch 7/10000\n",
      "189/189 [==============================] - 83s 439ms/step - loss: 3.1094e-04 - val_loss: 3.4374e-04\n",
      "Epoch 8/10000\n",
      "189/189 [==============================] - 83s 437ms/step - loss: 3.1599e-04 - val_loss: 3.5007e-04\n",
      "Epoch 9/10000\n",
      "189/189 [==============================] - 83s 441ms/step - loss: 3.1291e-04 - val_loss: 3.3986e-04\n",
      "Epoch 10/10000\n",
      "189/189 [==============================] - 83s 442ms/step - loss: 3.1285e-04 - val_loss: 3.4213e-04\n",
      "Epoch 11/10000\n",
      "189/189 [==============================] - 84s 444ms/step - loss: 3.1317e-04 - val_loss: 3.4731e-04\n",
      "Epoch 12/10000\n",
      "189/189 [==============================] - 83s 440ms/step - loss: 3.1888e-04 - val_loss: 3.3828e-04\n",
      "Epoch 13/10000\n",
      "189/189 [==============================] - 84s 444ms/step - loss: 3.1462e-04 - val_loss: 3.5574e-04\n",
      "Epoch 14/10000\n",
      "189/189 [==============================] - 84s 445ms/step - loss: 3.1537e-04 - val_loss: 3.5204e-04\n",
      "Epoch 15/10000\n",
      "189/189 [==============================] - 83s 440ms/step - loss: 3.1480e-04 - val_loss: 3.4848e-04\n",
      "Epoch 16/10000\n",
      "189/189 [==============================] - 83s 438ms/step - loss: 3.1332e-04 - val_loss: 3.5180e-04\n",
      "Epoch 17/10000\n",
      "189/189 [==============================] - 83s 437ms/step - loss: 3.1420e-04 - val_loss: 3.4565e-04\n",
      "Epoch 18/10000\n",
      "189/189 [==============================] - 83s 438ms/step - loss: 3.1664e-04 - val_loss: 3.4220e-04\n",
      "Epoch 19/10000\n",
      "189/189 [==============================] - 83s 437ms/step - loss: 3.1526e-04 - val_loss: 3.5213e-04\n",
      "Epoch 20/10000\n",
      "189/189 [==============================] - 83s 438ms/step - loss: 3.1302e-04 - val_loss: 3.3805e-04\n",
      "Epoch 21/10000\n",
      "189/189 [==============================] - 83s 437ms/step - loss: 3.1259e-04 - val_loss: 3.4289e-04\n",
      "Epoch 22/10000\n",
      "189/189 [==============================] - 83s 438ms/step - loss: 3.1076e-04 - val_loss: 3.4162e-04\n",
      "Epoch 23/10000\n",
      "189/189 [==============================] - 82s 436ms/step - loss: 3.0873e-04 - val_loss: 3.4561e-04\n",
      "Epoch 24/10000\n",
      "189/189 [==============================] - 83s 440ms/step - loss: 3.0871e-04 - val_loss: 3.4890e-04\n",
      "Epoch 25/10000\n",
      "189/189 [==============================] - 83s 440ms/step - loss: 3.1351e-04 - val_loss: 3.3912e-04\n",
      "Epoch 26/10000\n",
      "189/189 [==============================] - 84s 443ms/step - loss: 3.1352e-04 - val_loss: 3.4012e-04\n",
      "Epoch 27/10000\n",
      "189/189 [==============================] - 84s 444ms/step - loss: 3.1073e-04 - val_loss: 3.3957e-04\n",
      "Epoch 28/10000\n",
      "189/189 [==============================] - 84s 442ms/step - loss: 3.1267e-04 - val_loss: 3.5308e-04\n",
      "Epoch 29/10000\n",
      "189/189 [==============================] - 84s 443ms/step - loss: 3.1421e-04 - val_loss: 3.4228e-04\n",
      "Epoch 30/10000\n",
      "189/189 [==============================] - 84s 443ms/step - loss: 3.1012e-04 - val_loss: 3.4650e-04\n",
      "Epoch 31/10000\n",
      "189/189 [==============================] - 84s 442ms/step - loss: 3.1216e-04 - val_loss: 3.4211e-04\n",
      "Epoch 32/10000\n",
      "189/189 [==============================] - 84s 445ms/step - loss: 3.0989e-04 - val_loss: 3.4430e-04\n",
      "Epoch 33/10000\n",
      "189/189 [==============================] - 84s 442ms/step - loss: 3.0973e-04 - val_loss: 3.3713e-04\n",
      "Epoch 34/10000\n",
      "189/189 [==============================] - 83s 439ms/step - loss: 3.1031e-04 - val_loss: 3.4275e-04\n",
      "Epoch 35/10000\n",
      "189/189 [==============================] - 84s 442ms/step - loss: 3.0961e-04 - val_loss: 3.4260e-04\n",
      "Epoch 36/10000\n",
      "189/189 [==============================] - 84s 442ms/step - loss: 3.1142e-04 - val_loss: 3.4208e-04\n",
      "Epoch 37/10000\n",
      "189/189 [==============================] - 83s 442ms/step - loss: 3.1019e-04 - val_loss: 3.4244e-04\n",
      "Epoch 38/10000\n",
      "189/189 [==============================] - 84s 442ms/step - loss: 3.0915e-04 - val_loss: 3.4993e-04\n",
      "Epoch 39/10000\n",
      "189/189 [==============================] - 83s 438ms/step - loss: 3.1148e-04 - val_loss: 3.4485e-04\n",
      "Epoch 40/10000\n",
      "189/189 [==============================] - 83s 438ms/step - loss: 3.0919e-04 - val_loss: 3.3802e-04\n",
      "Epoch 41/10000\n",
      "189/189 [==============================] - 83s 438ms/step - loss: 3.1097e-04 - val_loss: 3.4428e-04\n",
      "Epoch 42/10000\n",
      "189/189 [==============================] - 83s 438ms/step - loss: 3.0850e-04 - val_loss: 3.5083e-04\n",
      "Epoch 43/10000\n",
      "189/189 [==============================] - 83s 437ms/step - loss: 3.1107e-04 - val_loss: 3.4073e-04\n",
      "Epoch 44/10000\n",
      "189/189 [==============================] - 83s 437ms/step - loss: 3.0949e-04 - val_loss: 3.3856e-04\n",
      "Epoch 45/10000\n",
      "189/189 [==============================] - 83s 438ms/step - loss: 3.0721e-04 - val_loss: 3.3511e-04\n",
      "Epoch 46/10000\n",
      "189/189 [==============================] - 84s 443ms/step - loss: 3.0798e-04 - val_loss: 3.3902e-04\n",
      "Epoch 47/10000\n",
      "189/189 [==============================] - 84s 446ms/step - loss: 3.0610e-04 - val_loss: 3.4126e-04\n",
      "Epoch 48/10000\n",
      "189/189 [==============================] - 84s 446ms/step - loss: 3.0845e-04 - val_loss: 3.4813e-04\n",
      "Epoch 49/10000\n",
      "189/189 [==============================] - 84s 444ms/step - loss: 3.0935e-04 - val_loss: 3.4520e-04\n",
      "Epoch 50/10000\n",
      "189/189 [==============================] - 83s 442ms/step - loss: 3.0998e-04 - val_loss: 3.3963e-04\n",
      "Epoch 51/10000\n",
      "189/189 [==============================] - 83s 440ms/step - loss: 3.0846e-04 - val_loss: 3.4357e-04\n",
      "Epoch 52/10000\n",
      "189/189 [==============================] - 83s 437ms/step - loss: 3.0804e-04 - val_loss: 3.3925e-04\n",
      "Epoch 53/10000\n",
      "189/189 [==============================] - 83s 438ms/step - loss: 3.0618e-04 - val_loss: 3.5463e-04\n",
      "Epoch 54/10000\n",
      "189/189 [==============================] - 84s 442ms/step - loss: 3.0977e-04 - val_loss: 3.4650e-04\n",
      "Epoch 55/10000\n",
      "189/189 [==============================] - 84s 442ms/step - loss: 3.0790e-04 - val_loss: 3.4907e-04\n",
      "Epoch 56/10000\n",
      " 40/189 [=====>........................] - ETA: 1:00 - loss: 3.0584e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mSNM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTr_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrMask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTr_Y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mVal_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mValMask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVal_Y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1100\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1095\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1096\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1097\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1098\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1099\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1100\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1102\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name) \u001b[38;5;28;01mas\u001b[39;00m tm:\n\u001b[1;32m--> 828\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    829\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m   new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:855\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    852\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    853\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 855\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    857\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    858\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2940\u001b[0m   (graph_function,\n\u001b[0;32m   2941\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1914\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1916\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1917\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1918\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1920\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m     args,\n\u001b[0;32m   1922\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1923\u001b[0m     executing_eagerly)\n\u001b[0;32m   1924\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 555\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    562\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    564\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    567\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    568\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SNM.fit([Tr_X, TrMask], tf.convert_to_tensor(Tr_Y), validation_data=([Val_X, ValMask], tf.convert_to_tensor(Val_Y)), batch_size=1500, verbose=1, shuffle=True, epochs=10000, callbacks=[model_checkpoint_callback] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2278f-2a2b-44c0-b65b-52bbb00b764a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69cbe7d-86b3-47a8-b831-12920061b77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9d6e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNM.load_weights('./results/model_rightedge_mask_no_vital_val_2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa846f21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['sup_not_miwae/prior/gru/gru_cell/kernel:0', 'sup_not_miwae/prior/gru/gru_cell/recurrent_kernel:0', 'sup_not_miwae/prior/gru/gru_cell/bias:0', 'sup_not_miwae/prior/mu/kernel:0', 'sup_not_miwae/prior/mu/bias:0', 'sup_not_miwae/prior/std/kernel:0', 'sup_not_miwae/prior/std/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['sup_not_miwae/prior/gru/gru_cell/kernel:0', 'sup_not_miwae/prior/gru/gru_cell/recurrent_kernel:0', 'sup_not_miwae/prior/gru/gru_cell/bias:0', 'sup_not_miwae/prior/mu/kernel:0', 'sup_not_miwae/prior/mu/bias:0', 'sup_not_miwae/prior/std/kernel:0', 'sup_not_miwae/prior/std/bias:0'] when minimizing the loss.\n",
      "95/95 [==============================] - 107s 1s/step - loss: 5.3819e-04 - val_loss: 5.3310e-04\n",
      "Epoch 2/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 5.3851e-04 - val_loss: 5.3305e-04\n",
      "Epoch 3/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3596e-04 - val_loss: 5.3271e-04\n",
      "Epoch 4/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3761e-04 - val_loss: 5.3272e-04\n",
      "Epoch 5/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3589e-04 - val_loss: 5.3263e-04\n",
      "Epoch 6/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3716e-04 - val_loss: 5.3273e-04\n",
      "Epoch 7/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3462e-04 - val_loss: 5.3272e-04\n",
      "Epoch 8/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3485e-04 - val_loss: 5.3216e-04\n",
      "Epoch 9/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3494e-04 - val_loss: 5.3201e-04\n",
      "Epoch 10/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3351e-04 - val_loss: 5.3197e-04\n",
      "Epoch 11/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3793e-04 - val_loss: 5.3158e-04\n",
      "Epoch 12/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3491e-04 - val_loss: 5.3153e-04\n",
      "Epoch 13/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3723e-04 - val_loss: 5.3144e-04\n",
      "Epoch 14/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3418e-04 - val_loss: 5.3138e-04\n",
      "Epoch 15/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 5.3468e-04 - val_loss: 5.3212e-04\n",
      "Epoch 16/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3562e-04 - val_loss: 5.3104e-04\n",
      "Epoch 17/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3300e-04 - val_loss: 5.3095e-04\n",
      "Epoch 18/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3533e-04 - val_loss: 5.3077e-04\n",
      "Epoch 19/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 5.3435e-04 - val_loss: 5.3054e-04\n",
      "Epoch 20/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 5.3393e-04 - val_loss: 5.3074e-04\n",
      "Epoch 21/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 5.3449e-04 - val_loss: 5.3025e-04\n",
      "Epoch 22/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 5.3294e-04 - val_loss: 5.2982e-04\n",
      "Epoch 23/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.3309e-04 - val_loss: 5.3035e-04\n",
      "Epoch 24/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 5.3561e-04 - val_loss: 5.2953e-04\n",
      "Epoch 25/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 5.3394e-04 - val_loss: 5.3024e-04\n",
      "Epoch 26/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.3463e-04 - val_loss: 5.2939e-04\n",
      "Epoch 27/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3330e-04 - val_loss: 5.2940e-04\n",
      "Epoch 28/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3306e-04 - val_loss: 5.2900e-04\n",
      "Epoch 29/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3318e-04 - val_loss: 5.2882e-04\n",
      "Epoch 30/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3132e-04 - val_loss: 5.2919e-04\n",
      "Epoch 31/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.3419e-04 - val_loss: 5.2868e-04\n",
      "Epoch 32/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.3273e-04 - val_loss: 5.2839e-04\n",
      "Epoch 33/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.3266e-04 - val_loss: 5.2865e-04\n",
      "Epoch 34/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3147e-04 - val_loss: 5.2827e-04\n",
      "Epoch 35/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 5.3216e-04 - val_loss: 5.2829e-04\n",
      "Epoch 36/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3158e-04 - val_loss: 5.2790e-04\n",
      "Epoch 37/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2996e-04 - val_loss: 5.2783e-04\n",
      "Epoch 38/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.2995e-04 - val_loss: 5.2788e-04\n",
      "Epoch 39/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.3180e-04 - val_loss: 5.2751e-04\n",
      "Epoch 40/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3002e-04 - val_loss: 5.2735e-04\n",
      "Epoch 41/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.3072e-04 - val_loss: 5.2733e-04\n",
      "Epoch 42/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.2994e-04 - val_loss: 5.2704e-04\n",
      "Epoch 43/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.2936e-04 - val_loss: 5.2675e-04\n",
      "Epoch 44/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.3102e-04 - val_loss: 5.2685e-04\n",
      "Epoch 45/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 5.3003e-04 - val_loss: 5.2680e-04\n",
      "Epoch 46/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.3097e-04 - val_loss: 5.2695e-04\n",
      "Epoch 47/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.3004e-04 - val_loss: 5.2637e-04\n",
      "Epoch 48/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.2657e-04 - val_loss: 5.2631e-04\n",
      "Epoch 49/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.2878e-04 - val_loss: 5.2665e-04\n",
      "Epoch 50/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2941e-04 - val_loss: 5.2605e-04\n",
      "Epoch 51/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2888e-04 - val_loss: 5.2590e-04\n",
      "Epoch 52/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 5.3004e-04 - val_loss: 5.2591e-04\n",
      "Epoch 53/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.2884e-04 - val_loss: 5.2595e-04\n",
      "Epoch 54/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.2653e-04 - val_loss: 5.2530e-04\n",
      "Epoch 55/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.2705e-04 - val_loss: 5.2496e-04\n",
      "Epoch 56/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 5.2779e-04 - val_loss: 5.2522e-04\n",
      "Epoch 57/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2857e-04 - val_loss: 5.2485e-04\n",
      "Epoch 58/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2687e-04 - val_loss: 5.2468e-04\n",
      "Epoch 59/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2826e-04 - val_loss: 5.2495e-04\n",
      "Epoch 60/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2744e-04 - val_loss: 5.2544e-04\n",
      "Epoch 61/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 5.2903e-04 - val_loss: 5.2428e-04\n",
      "Epoch 62/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.2685e-04 - val_loss: 5.2397e-04\n",
      "Epoch 63/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.2684e-04 - val_loss: 5.2407e-04\n",
      "Epoch 64/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.2544e-04 - val_loss: 5.2383e-04\n",
      "Epoch 65/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.2876e-04 - val_loss: 5.2379e-04\n",
      "Epoch 66/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 5.2521e-04 - val_loss: 5.2350e-04\n",
      "Epoch 67/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.2609e-04 - val_loss: 5.2344e-04\n",
      "Epoch 68/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.2647e-04 - val_loss: 5.2332e-04\n",
      "Epoch 69/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2610e-04 - val_loss: 5.2316e-04\n",
      "Epoch 70/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.2650e-04 - val_loss: 5.2344e-04\n",
      "Epoch 71/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2536e-04 - val_loss: 5.2293e-04\n",
      "Epoch 72/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.2540e-04 - val_loss: 5.2278e-04\n",
      "Epoch 73/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 5.2584e-04 - val_loss: 5.2271e-04\n",
      "Epoch 74/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2619e-04 - val_loss: 5.2307e-04\n",
      "Epoch 75/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.2530e-04 - val_loss: 5.2251e-04\n",
      "Epoch 76/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2623e-04 - val_loss: 5.2203e-04\n",
      "Epoch 77/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 5.2548e-04 - val_loss: 5.2198e-04\n",
      "Epoch 78/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2434e-04 - val_loss: 5.2447e-04\n",
      "Epoch 79/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.2410e-04 - val_loss: 5.2165e-04\n",
      "Epoch 80/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2432e-04 - val_loss: 5.2157e-04\n",
      "Epoch 81/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2456e-04 - val_loss: 5.2177e-04\n",
      "Epoch 82/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.2440e-04 - val_loss: 5.2129e-04\n",
      "Epoch 83/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 5.2383e-04 - val_loss: 5.2168e-04\n",
      "Epoch 84/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.2525e-04 - val_loss: 5.2117e-04\n",
      "Epoch 85/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2443e-04 - val_loss: 5.2080e-04\n",
      "Epoch 86/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.2316e-04 - val_loss: 5.2173e-04\n",
      "Epoch 87/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.2280e-04 - val_loss: 5.2083e-04\n",
      "Epoch 88/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.2431e-04 - val_loss: 5.2050e-04\n",
      "Epoch 89/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.2471e-04 - val_loss: 5.2040e-04\n",
      "Epoch 90/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.2163e-04 - val_loss: 5.2039e-04\n",
      "Epoch 91/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.2342e-04 - val_loss: 5.2029e-04\n",
      "Epoch 92/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.2473e-04 - val_loss: 5.2114e-04\n",
      "Epoch 93/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2523e-04 - val_loss: 5.1993e-04\n",
      "Epoch 94/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.2218e-04 - val_loss: 5.1982e-04\n",
      "Epoch 95/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.2264e-04 - val_loss: 5.2007e-04\n",
      "Epoch 96/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2166e-04 - val_loss: 5.1933e-04\n",
      "Epoch 97/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1996e-04 - val_loss: 5.1931e-04\n",
      "Epoch 98/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2222e-04 - val_loss: 5.1918e-04\n",
      "Epoch 99/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.2114e-04 - val_loss: 5.1909e-04\n",
      "Epoch 100/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2275e-04 - val_loss: 5.1978e-04\n",
      "Epoch 101/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.2096e-04 - val_loss: 5.1896e-04\n",
      "Epoch 102/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.2201e-04 - val_loss: 5.1881e-04\n",
      "Epoch 103/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1989e-04 - val_loss: 5.1856e-04\n",
      "Epoch 104/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.2115e-04 - val_loss: 5.1857e-04\n",
      "Epoch 105/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.2075e-04 - val_loss: 5.1835e-04\n",
      "Epoch 106/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.2324e-04 - val_loss: 5.1860e-04\n",
      "Epoch 107/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.2054e-04 - val_loss: 5.1825e-04\n",
      "Epoch 108/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.2061e-04 - val_loss: 5.1820e-04\n",
      "Epoch 109/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1981e-04 - val_loss: 5.1856e-04\n",
      "Epoch 110/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1954e-04 - val_loss: 5.1765e-04\n",
      "Epoch 111/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1971e-04 - val_loss: 5.1737e-04\n",
      "Epoch 112/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.2012e-04 - val_loss: 5.1749e-04\n",
      "Epoch 113/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.2047e-04 - val_loss: 5.1721e-04\n",
      "Epoch 114/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.2032e-04 - val_loss: 5.1734e-04\n",
      "Epoch 115/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1871e-04 - val_loss: 5.1688e-04\n",
      "Epoch 116/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1947e-04 - val_loss: 5.1808e-04\n",
      "Epoch 117/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1736e-04 - val_loss: 5.1687e-04\n",
      "Epoch 118/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1955e-04 - val_loss: 5.1693e-04\n",
      "Epoch 119/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1964e-04 - val_loss: 5.1669e-04\n",
      "Epoch 120/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.1926e-04 - val_loss: 5.1647e-04\n",
      "Epoch 121/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.1723e-04 - val_loss: 5.1630e-04\n",
      "Epoch 122/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.1751e-04 - val_loss: 5.1634e-04\n",
      "Epoch 123/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1853e-04 - val_loss: 5.1624e-04\n",
      "Epoch 124/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1817e-04 - val_loss: 5.1587e-04\n",
      "Epoch 125/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1823e-04 - val_loss: 5.1572e-04\n",
      "Epoch 126/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.1871e-04 - val_loss: 5.1553e-04\n",
      "Epoch 127/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.1990e-04 - val_loss: 5.1540e-04\n",
      "Epoch 128/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1732e-04 - val_loss: 5.1538e-04\n",
      "Epoch 129/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1605e-04 - val_loss: 5.1534e-04\n",
      "Epoch 130/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1745e-04 - val_loss: 5.1512e-04\n",
      "Epoch 131/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1659e-04 - val_loss: 5.1493e-04\n",
      "Epoch 132/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.1829e-04 - val_loss: 5.1476e-04\n",
      "Epoch 133/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.1902e-04 - val_loss: 5.1467e-04\n",
      "Epoch 134/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1592e-04 - val_loss: 5.1464e-04\n",
      "Epoch 135/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1744e-04 - val_loss: 5.1438e-04\n",
      "Epoch 136/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1730e-04 - val_loss: 5.1443e-04\n",
      "Epoch 137/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1626e-04 - val_loss: 5.1408e-04\n",
      "Epoch 138/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1688e-04 - val_loss: 5.1473e-04\n",
      "Epoch 139/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.1865e-04 - val_loss: 5.1406e-04\n",
      "Epoch 140/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.1723e-04 - val_loss: 5.1378e-04\n",
      "Epoch 141/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1952e-04 - val_loss: 5.1375e-04\n",
      "Epoch 142/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1655e-04 - val_loss: 5.1354e-04\n",
      "Epoch 143/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1344e-04 - val_loss: 5.1336e-04\n",
      "Epoch 144/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1627e-04 - val_loss: 5.1372e-04\n",
      "Epoch 145/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.1607e-04 - val_loss: 5.1334e-04\n",
      "Epoch 146/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.1598e-04 - val_loss: 5.1309e-04\n",
      "Epoch 147/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.1709e-04 - val_loss: 5.1383e-04\n",
      "Epoch 148/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1510e-04 - val_loss: 5.1361e-04\n",
      "Epoch 149/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.1553e-04 - val_loss: 5.1278e-04\n",
      "Epoch 150/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.1496e-04 - val_loss: 5.1293e-04\n",
      "Epoch 151/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1493e-04 - val_loss: 5.1261e-04\n",
      "Epoch 152/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.1628e-04 - val_loss: 5.1227e-04\n",
      "Epoch 153/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1497e-04 - val_loss: 5.1213e-04\n",
      "Epoch 154/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.1366e-04 - val_loss: 5.1217e-04\n",
      "Epoch 155/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1317e-04 - val_loss: 5.1195e-04\n",
      "Epoch 156/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1376e-04 - val_loss: 5.1215e-04\n",
      "Epoch 157/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1365e-04 - val_loss: 5.1173e-04\n",
      "Epoch 158/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1455e-04 - val_loss: 5.1161e-04\n",
      "Epoch 159/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1473e-04 - val_loss: 5.1168e-04\n",
      "Epoch 160/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1429e-04 - val_loss: 5.1144e-04\n",
      "Epoch 161/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1380e-04 - val_loss: 5.1129e-04\n",
      "Epoch 162/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1366e-04 - val_loss: 5.1118e-04\n",
      "Epoch 163/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1341e-04 - val_loss: 5.1217e-04\n",
      "Epoch 164/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.1243e-04 - val_loss: 5.1120e-04\n",
      "Epoch 165/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.1312e-04 - val_loss: 5.1082e-04\n",
      "Epoch 166/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1509e-04 - val_loss: 5.1072e-04\n",
      "Epoch 167/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.1342e-04 - val_loss: 5.1136e-04\n",
      "Epoch 168/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.1110e-04 - val_loss: 5.1078e-04\n",
      "Epoch 169/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1280e-04 - val_loss: 5.1120e-04\n",
      "Epoch 170/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1298e-04 - val_loss: 5.1017e-04\n",
      "Epoch 171/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1016e-04 - val_loss: 5.1004e-04\n",
      "Epoch 172/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1006e-04 - val_loss: 5.1031e-04\n",
      "Epoch 173/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.1213e-04 - val_loss: 5.0989e-04\n",
      "Epoch 174/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1171e-04 - val_loss: 5.0971e-04\n",
      "Epoch 175/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1099e-04 - val_loss: 5.0990e-04\n",
      "Epoch 176/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1311e-04 - val_loss: 5.0955e-04\n",
      "Epoch 177/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.1294e-04 - val_loss: 5.0946e-04\n",
      "Epoch 178/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1024e-04 - val_loss: 5.0981e-04\n",
      "Epoch 179/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 5.1382e-04 - val_loss: 5.0917e-04\n",
      "Epoch 180/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1111e-04 - val_loss: 5.0903e-04\n",
      "Epoch 181/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.1126e-04 - val_loss: 5.0957e-04\n",
      "Epoch 182/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1019e-04 - val_loss: 5.0930e-04\n",
      "Epoch 183/10000\n",
      "95/95 [==============================] - 95s 995ms/step - loss: 5.0988e-04 - val_loss: 5.0864e-04\n",
      "Epoch 184/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.1154e-04 - val_loss: 5.0898e-04\n",
      "Epoch 185/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1254e-04 - val_loss: 5.0908e-04\n",
      "Epoch 186/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0930e-04 - val_loss: 5.0837e-04\n",
      "Epoch 187/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.0889e-04 - val_loss: 5.0866e-04\n",
      "Epoch 188/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1009e-04 - val_loss: 5.0829e-04\n",
      "Epoch 189/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0856e-04 - val_loss: 5.0815e-04\n",
      "Epoch 190/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.1191e-04 - val_loss: 5.0787e-04\n",
      "Epoch 191/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0938e-04 - val_loss: 5.0768e-04\n",
      "Epoch 192/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0999e-04 - val_loss: 5.0790e-04\n",
      "Epoch 193/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0956e-04 - val_loss: 5.0759e-04\n",
      "Epoch 194/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1099e-04 - val_loss: 5.0750e-04\n",
      "Epoch 195/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.1042e-04 - val_loss: 5.0724e-04\n",
      "Epoch 196/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0941e-04 - val_loss: 5.0806e-04\n",
      "Epoch 197/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0980e-04 - val_loss: 5.0745e-04\n",
      "Epoch 198/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0813e-04 - val_loss: 5.0727e-04\n",
      "Epoch 199/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0870e-04 - val_loss: 5.0732e-04\n",
      "Epoch 200/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 5.0977e-04 - val_loss: 5.0731e-04\n",
      "Epoch 201/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0775e-04 - val_loss: 5.0691e-04\n",
      "Epoch 202/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0793e-04 - val_loss: 5.0647e-04\n",
      "Epoch 203/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0973e-04 - val_loss: 5.0704e-04\n",
      "Epoch 204/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0828e-04 - val_loss: 5.0652e-04\n",
      "Epoch 205/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0841e-04 - val_loss: 5.0643e-04\n",
      "Epoch 206/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.0983e-04 - val_loss: 5.0633e-04\n",
      "Epoch 207/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0739e-04 - val_loss: 5.0599e-04\n",
      "Epoch 208/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0638e-04 - val_loss: 5.0630e-04\n",
      "Epoch 209/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0785e-04 - val_loss: 5.0579e-04\n",
      "Epoch 210/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0710e-04 - val_loss: 5.0579e-04\n",
      "Epoch 211/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0647e-04 - val_loss: 5.0548e-04\n",
      "Epoch 212/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.0780e-04 - val_loss: 5.0577e-04\n",
      "Epoch 213/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0634e-04 - val_loss: 5.0561e-04\n",
      "Epoch 214/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0629e-04 - val_loss: 5.0508e-04\n",
      "Epoch 215/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0625e-04 - val_loss: 5.0520e-04\n",
      "Epoch 216/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0714e-04 - val_loss: 5.0500e-04\n",
      "Epoch 217/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0682e-04 - val_loss: 5.0527e-04\n",
      "Epoch 218/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0590e-04 - val_loss: 5.0486e-04\n",
      "Epoch 219/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0418e-04 - val_loss: 5.0471e-04\n",
      "Epoch 220/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0776e-04 - val_loss: 5.0469e-04\n",
      "Epoch 221/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0830e-04 - val_loss: 5.0487e-04\n",
      "Epoch 222/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0700e-04 - val_loss: 5.0436e-04\n",
      "Epoch 223/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0837e-04 - val_loss: 5.0449e-04\n",
      "Epoch 224/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.0347e-04 - val_loss: 5.0435e-04\n",
      "Epoch 225/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0480e-04 - val_loss: 5.0422e-04\n",
      "Epoch 226/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0665e-04 - val_loss: 5.0395e-04\n",
      "Epoch 227/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0405e-04 - val_loss: 5.0396e-04\n",
      "Epoch 228/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0462e-04 - val_loss: 5.0384e-04\n",
      "Epoch 229/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0516e-04 - val_loss: 5.0365e-04\n",
      "Epoch 230/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0661e-04 - val_loss: 5.0347e-04\n",
      "Epoch 231/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0465e-04 - val_loss: 5.0334e-04\n",
      "Epoch 232/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0549e-04 - val_loss: 5.0334e-04\n",
      "Epoch 233/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0590e-04 - val_loss: 5.0360e-04\n",
      "Epoch 234/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0419e-04 - val_loss: 5.0346e-04\n",
      "Epoch 235/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.0442e-04 - val_loss: 5.0288e-04\n",
      "Epoch 236/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0458e-04 - val_loss: 5.0274e-04\n",
      "Epoch 237/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0402e-04 - val_loss: 5.0339e-04\n",
      "Epoch 238/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0329e-04 - val_loss: 5.0314e-04\n",
      "Epoch 239/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0542e-04 - val_loss: 5.0283e-04\n",
      "Epoch 240/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0581e-04 - val_loss: 5.0261e-04\n",
      "Epoch 241/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0322e-04 - val_loss: 5.0267e-04\n",
      "Epoch 242/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0632e-04 - val_loss: 5.0236e-04\n",
      "Epoch 243/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0468e-04 - val_loss: 5.0278e-04\n",
      "Epoch 244/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0550e-04 - val_loss: 5.0193e-04\n",
      "Epoch 245/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0407e-04 - val_loss: 5.0286e-04\n",
      "Epoch 246/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0553e-04 - val_loss: 5.0176e-04\n",
      "Epoch 247/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0205e-04 - val_loss: 5.0173e-04\n",
      "Epoch 248/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0368e-04 - val_loss: 5.0181e-04\n",
      "Epoch 249/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0284e-04 - val_loss: 5.0183e-04\n",
      "Epoch 250/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0245e-04 - val_loss: 5.0152e-04\n",
      "Epoch 251/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0316e-04 - val_loss: 5.0148e-04\n",
      "Epoch 252/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0231e-04 - val_loss: 5.0118e-04\n",
      "Epoch 253/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0254e-04 - val_loss: 5.0096e-04\n",
      "Epoch 254/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0175e-04 - val_loss: 5.0085e-04\n",
      "Epoch 255/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0198e-04 - val_loss: 5.0128e-04\n",
      "Epoch 256/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0230e-04 - val_loss: 5.0103e-04\n",
      "Epoch 257/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0343e-04 - val_loss: 5.0131e-04\n",
      "Epoch 258/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0360e-04 - val_loss: 5.0094e-04\n",
      "Epoch 259/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.9986e-04 - val_loss: 5.0054e-04\n",
      "Epoch 260/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 5.0242e-04 - val_loss: 5.0023e-04\n",
      "Epoch 261/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0113e-04 - val_loss: 5.0036e-04\n",
      "Epoch 262/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0302e-04 - val_loss: 5.0024e-04\n",
      "Epoch 263/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0126e-04 - val_loss: 5.0020e-04\n",
      "Epoch 264/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9961e-04 - val_loss: 5.0020e-04\n",
      "Epoch 265/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0031e-04 - val_loss: 5.0000e-04\n",
      "Epoch 266/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0305e-04 - val_loss: 4.9974e-04\n",
      "Epoch 267/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0221e-04 - val_loss: 4.9963e-04\n",
      "Epoch 268/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0140e-04 - val_loss: 4.9964e-04\n",
      "Epoch 269/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0093e-04 - val_loss: 4.9995e-04\n",
      "Epoch 270/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 5.0015e-04 - val_loss: 4.9968e-04\n",
      "Epoch 271/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0352e-04 - val_loss: 4.9931e-04\n",
      "Epoch 272/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0097e-04 - val_loss: 4.9917e-04\n",
      "Epoch 273/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0055e-04 - val_loss: 4.9918e-04\n",
      "Epoch 274/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 5.0112e-04 - val_loss: 5.0086e-04\n",
      "Epoch 275/10000\n",
      "95/95 [==============================] - 95s 995ms/step - loss: 5.0013e-04 - val_loss: 4.9886e-04\n",
      "Epoch 276/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0038e-04 - val_loss: 4.9911e-04\n",
      "Epoch 277/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0037e-04 - val_loss: 4.9882e-04\n",
      "Epoch 278/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0269e-04 - val_loss: 4.9890e-04\n",
      "Epoch 279/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.9964e-04 - val_loss: 4.9859e-04\n",
      "Epoch 280/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0085e-04 - val_loss: 4.9837e-04\n",
      "Epoch 281/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9993e-04 - val_loss: 4.9894e-04\n",
      "Epoch 282/10000\n",
      "95/95 [==============================] - 95s 995ms/step - loss: 5.0008e-04 - val_loss: 4.9814e-04\n",
      "Epoch 283/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.9939e-04 - val_loss: 4.9865e-04\n",
      "Epoch 284/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9852e-04 - val_loss: 4.9814e-04\n",
      "Epoch 285/10000\n",
      "95/95 [==============================] - 94s 995ms/step - loss: 4.9924e-04 - val_loss: 4.9822e-04\n",
      "Epoch 286/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9897e-04 - val_loss: 4.9799e-04\n",
      "Epoch 287/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9977e-04 - val_loss: 4.9812e-04\n",
      "Epoch 288/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9938e-04 - val_loss: 4.9761e-04\n",
      "Epoch 289/10000\n",
      "95/95 [==============================] - 94s 995ms/step - loss: 4.9859e-04 - val_loss: 4.9756e-04\n",
      "Epoch 290/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9949e-04 - val_loss: 4.9893e-04\n",
      "Epoch 291/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 5.0163e-04 - val_loss: 4.9738e-04\n",
      "Epoch 292/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9931e-04 - val_loss: 4.9720e-04\n",
      "Epoch 293/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9717e-04 - val_loss: 4.9707e-04\n",
      "Epoch 294/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9820e-04 - val_loss: 4.9788e-04\n",
      "Epoch 295/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9891e-04 - val_loss: 4.9695e-04\n",
      "Epoch 296/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9689e-04 - val_loss: 4.9773e-04\n",
      "Epoch 297/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9839e-04 - val_loss: 4.9700e-04\n",
      "Epoch 298/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9848e-04 - val_loss: 4.9670e-04\n",
      "Epoch 299/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9585e-04 - val_loss: 4.9638e-04\n",
      "Epoch 300/10000\n",
      "95/95 [==============================] - 94s 995ms/step - loss: 4.9593e-04 - val_loss: 4.9710e-04\n",
      "Epoch 301/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9897e-04 - val_loss: 4.9636e-04\n",
      "Epoch 302/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9773e-04 - val_loss: 4.9622e-04\n",
      "Epoch 303/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9675e-04 - val_loss: 4.9639e-04\n",
      "Epoch 304/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9668e-04 - val_loss: 4.9623e-04\n",
      "Epoch 305/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.9765e-04 - val_loss: 4.9600e-04\n",
      "Epoch 306/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9838e-04 - val_loss: 4.9601e-04\n",
      "Epoch 307/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9771e-04 - val_loss: 4.9585e-04\n",
      "Epoch 308/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9651e-04 - val_loss: 4.9572e-04\n",
      "Epoch 309/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.9671e-04 - val_loss: 4.9571e-04\n",
      "Epoch 310/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9721e-04 - val_loss: 4.9677e-04\n",
      "Epoch 311/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9681e-04 - val_loss: 4.9546e-04\n",
      "Epoch 312/10000\n",
      "95/95 [==============================] - 95s 995ms/step - loss: 4.9463e-04 - val_loss: 4.9520e-04\n",
      "Epoch 313/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9597e-04 - val_loss: 4.9513e-04\n",
      "Epoch 314/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.9792e-04 - val_loss: 4.9583e-04\n",
      "Epoch 315/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9757e-04 - val_loss: 4.9524e-04\n",
      "Epoch 316/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9662e-04 - val_loss: 4.9509e-04\n",
      "Epoch 317/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9593e-04 - val_loss: 4.9525e-04\n",
      "Epoch 318/10000\n",
      "95/95 [==============================] - 94s 995ms/step - loss: 4.9706e-04 - val_loss: 4.9501e-04\n",
      "Epoch 319/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.9515e-04 - val_loss: 4.9465e-04\n",
      "Epoch 320/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9340e-04 - val_loss: 4.9469e-04\n",
      "Epoch 321/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9691e-04 - val_loss: 4.9439e-04\n",
      "Epoch 322/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9608e-04 - val_loss: 4.9440e-04\n",
      "Epoch 323/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.9654e-04 - val_loss: 4.9498e-04\n",
      "Epoch 324/10000\n",
      "95/95 [==============================] - 95s 995ms/step - loss: 4.9562e-04 - val_loss: 4.9455e-04\n",
      "Epoch 325/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.9619e-04 - val_loss: 4.9462e-04\n",
      "Epoch 326/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.9654e-04 - val_loss: 4.9408e-04\n",
      "Epoch 327/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9606e-04 - val_loss: 4.9415e-04\n",
      "Epoch 328/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9390e-04 - val_loss: 4.9377e-04\n",
      "Epoch 329/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.9576e-04 - val_loss: 4.9422e-04\n",
      "Epoch 330/10000\n",
      "95/95 [==============================] - 94s 995ms/step - loss: 4.9531e-04 - val_loss: 4.9353e-04\n",
      "Epoch 331/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.9475e-04 - val_loss: 4.9344e-04\n",
      "Epoch 332/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9559e-04 - val_loss: 4.9346e-04\n",
      "Epoch 333/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9438e-04 - val_loss: 4.9353e-04\n",
      "Epoch 334/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.9508e-04 - val_loss: 4.9333e-04\n",
      "Epoch 335/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9376e-04 - val_loss: 4.9327e-04\n",
      "Epoch 336/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9510e-04 - val_loss: 4.9306e-04\n",
      "Epoch 337/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9267e-04 - val_loss: 4.9311e-04\n",
      "Epoch 338/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9458e-04 - val_loss: 4.9280e-04\n",
      "Epoch 339/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9610e-04 - val_loss: 4.9311e-04\n",
      "Epoch 340/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.9587e-04 - val_loss: 4.9290e-04\n",
      "Epoch 341/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9485e-04 - val_loss: 4.9295e-04\n",
      "Epoch 342/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.9370e-04 - val_loss: 4.9279e-04\n",
      "Epoch 343/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9408e-04 - val_loss: 4.9252e-04\n",
      "Epoch 344/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9319e-04 - val_loss: 4.9251e-04\n",
      "Epoch 345/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9286e-04 - val_loss: 4.9223e-04\n",
      "Epoch 346/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9269e-04 - val_loss: 4.9238e-04\n",
      "Epoch 347/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.9335e-04 - val_loss: 4.9206e-04\n",
      "Epoch 348/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9324e-04 - val_loss: 4.9194e-04\n",
      "Epoch 349/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9171e-04 - val_loss: 4.9191e-04\n",
      "Epoch 350/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9301e-04 - val_loss: 4.9183e-04\n",
      "Epoch 351/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.9355e-04 - val_loss: 4.9167e-04\n",
      "Epoch 352/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9318e-04 - val_loss: 4.9225e-04\n",
      "Epoch 353/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9103e-04 - val_loss: 4.9186e-04\n",
      "Epoch 354/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9254e-04 - val_loss: 4.9156e-04\n",
      "Epoch 355/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9205e-04 - val_loss: 4.9137e-04\n",
      "Epoch 356/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.9000e-04 - val_loss: 4.9122e-04\n",
      "Epoch 357/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9351e-04 - val_loss: 4.9103e-04\n",
      "Epoch 358/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.9090e-04 - val_loss: 4.9139e-04\n",
      "Epoch 359/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9112e-04 - val_loss: 4.9115e-04\n",
      "Epoch 360/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.9293e-04 - val_loss: 4.9089e-04\n",
      "Epoch 361/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9267e-04 - val_loss: 4.9097e-04\n",
      "Epoch 362/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9310e-04 - val_loss: 4.9163e-04\n",
      "Epoch 363/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9334e-04 - val_loss: 4.9085e-04\n",
      "Epoch 364/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.9205e-04 - val_loss: 4.9069e-04\n",
      "Epoch 365/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.9047e-04 - val_loss: 4.9046e-04\n",
      "Epoch 366/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9063e-04 - val_loss: 4.9035e-04\n",
      "Epoch 367/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.9011e-04 - val_loss: 4.9035e-04\n",
      "Epoch 368/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9174e-04 - val_loss: 4.9058e-04\n",
      "Epoch 369/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8871e-04 - val_loss: 4.9015e-04\n",
      "Epoch 370/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9110e-04 - val_loss: 4.9013e-04\n",
      "Epoch 371/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.9150e-04 - val_loss: 4.9002e-04\n",
      "Epoch 372/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9168e-04 - val_loss: 4.9005e-04\n",
      "Epoch 373/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.9175e-04 - val_loss: 4.9012e-04\n",
      "Epoch 374/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.8906e-04 - val_loss: 4.8963e-04\n",
      "Epoch 375/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8897e-04 - val_loss: 4.8965e-04\n",
      "Epoch 376/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9003e-04 - val_loss: 4.9036e-04\n",
      "Epoch 377/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.9001e-04 - val_loss: 4.9100e-04\n",
      "Epoch 378/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9099e-04 - val_loss: 4.8944e-04\n",
      "Epoch 379/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9067e-04 - val_loss: 4.8930e-04\n",
      "Epoch 380/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8949e-04 - val_loss: 4.8936e-04\n",
      "Epoch 381/10000\n",
      "95/95 [==============================] - 94s 994ms/step - loss: 4.8997e-04 - val_loss: 4.8981e-04\n",
      "Epoch 382/10000\n",
      "95/95 [==============================] - 94s 994ms/step - loss: 4.9111e-04 - val_loss: 4.8897e-04\n",
      "Epoch 383/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9079e-04 - val_loss: 4.8916e-04\n",
      "Epoch 384/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8962e-04 - val_loss: 4.8908e-04\n",
      "Epoch 385/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.9114e-04 - val_loss: 4.8925e-04\n",
      "Epoch 386/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.9006e-04 - val_loss: 4.8893e-04\n",
      "Epoch 387/10000\n",
      "95/95 [==============================] - 98s 1s/step - loss: 4.9003e-04 - val_loss: 4.8881e-04\n",
      "Epoch 388/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.9018e-04 - val_loss: 4.8842e-04\n",
      "Epoch 389/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8921e-04 - val_loss: 4.8859e-04\n",
      "Epoch 390/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9188e-04 - val_loss: 4.8848e-04\n",
      "Epoch 391/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.8912e-04 - val_loss: 4.8831e-04\n",
      "Epoch 392/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8936e-04 - val_loss: 4.8822e-04\n",
      "Epoch 393/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.8910e-04 - val_loss: 4.8831e-04\n",
      "Epoch 394/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8852e-04 - val_loss: 4.8803e-04\n",
      "Epoch 395/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8952e-04 - val_loss: 4.8800e-04\n",
      "Epoch 396/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8843e-04 - val_loss: 4.8792e-04\n",
      "Epoch 397/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8788e-04 - val_loss: 4.8766e-04\n",
      "Epoch 398/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8852e-04 - val_loss: 4.8791e-04\n",
      "Epoch 399/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8825e-04 - val_loss: 4.8769e-04\n",
      "Epoch 400/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.9025e-04 - val_loss: 4.8742e-04\n",
      "Epoch 401/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8776e-04 - val_loss: 4.8742e-04\n",
      "Epoch 402/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8938e-04 - val_loss: 4.8771e-04\n",
      "Epoch 403/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8765e-04 - val_loss: 4.8718e-04\n",
      "Epoch 404/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8751e-04 - val_loss: 4.8707e-04\n",
      "Epoch 405/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8861e-04 - val_loss: 4.8743e-04\n",
      "Epoch 406/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.8753e-04 - val_loss: 4.8706e-04\n",
      "Epoch 407/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8953e-04 - val_loss: 4.8705e-04\n",
      "Epoch 408/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8805e-04 - val_loss: 4.8743e-04\n",
      "Epoch 409/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8645e-04 - val_loss: 4.8698e-04\n",
      "Epoch 410/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8692e-04 - val_loss: 4.8683e-04\n",
      "Epoch 411/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8792e-04 - val_loss: 4.8682e-04\n",
      "Epoch 412/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8891e-04 - val_loss: 4.8683e-04\n",
      "Epoch 413/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8804e-04 - val_loss: 4.8642e-04\n",
      "Epoch 414/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8778e-04 - val_loss: 4.8642e-04\n",
      "Epoch 415/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8828e-04 - val_loss: 4.8621e-04\n",
      "Epoch 416/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.8782e-04 - val_loss: 4.8625e-04\n",
      "Epoch 417/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.8828e-04 - val_loss: 4.8677e-04\n",
      "Epoch 418/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8792e-04 - val_loss: 4.8601e-04\n",
      "Epoch 419/10000\n",
      "95/95 [==============================] - 94s 995ms/step - loss: 4.8645e-04 - val_loss: 4.8601e-04\n",
      "Epoch 420/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.8650e-04 - val_loss: 4.8616e-04\n",
      "Epoch 421/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8437e-04 - val_loss: 4.8570e-04\n",
      "Epoch 422/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8794e-04 - val_loss: 4.8592e-04\n",
      "Epoch 423/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8521e-04 - val_loss: 4.8634e-04\n",
      "Epoch 424/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8764e-04 - val_loss: 4.8579e-04\n",
      "Epoch 425/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.8509e-04 - val_loss: 4.8609e-04\n",
      "Epoch 426/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8598e-04 - val_loss: 4.8552e-04\n",
      "Epoch 427/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8564e-04 - val_loss: 4.8543e-04\n",
      "Epoch 428/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8619e-04 - val_loss: 4.8569e-04\n",
      "Epoch 429/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.8664e-04 - val_loss: 4.8583e-04\n",
      "Epoch 430/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8495e-04 - val_loss: 4.8632e-04\n",
      "Epoch 431/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8656e-04 - val_loss: 4.8540e-04\n",
      "Epoch 432/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8652e-04 - val_loss: 4.8503e-04\n",
      "Epoch 433/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8602e-04 - val_loss: 4.8508e-04\n",
      "Epoch 434/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8567e-04 - val_loss: 4.8468e-04\n",
      "Epoch 435/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.8407e-04 - val_loss: 4.8472e-04\n",
      "Epoch 436/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8520e-04 - val_loss: 4.8466e-04\n",
      "Epoch 437/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8498e-04 - val_loss: 4.8471e-04\n",
      "Epoch 438/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8593e-04 - val_loss: 4.8450e-04\n",
      "Epoch 439/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8418e-04 - val_loss: 4.8462e-04\n",
      "Epoch 440/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8444e-04 - val_loss: 4.8462e-04\n",
      "Epoch 441/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8497e-04 - val_loss: 4.8418e-04\n",
      "Epoch 442/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.8370e-04 - val_loss: 4.8422e-04\n",
      "Epoch 443/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.8627e-04 - val_loss: 4.8410e-04\n",
      "Epoch 444/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8331e-04 - val_loss: 4.8462e-04\n",
      "Epoch 445/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.8727e-04 - val_loss: 4.8372e-04\n",
      "Epoch 446/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8429e-04 - val_loss: 4.8415e-04\n",
      "Epoch 447/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8334e-04 - val_loss: 4.8368e-04\n",
      "Epoch 448/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8489e-04 - val_loss: 4.8403e-04\n",
      "Epoch 449/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8506e-04 - val_loss: 4.8349e-04\n",
      "Epoch 450/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.8188e-04 - val_loss: 4.8362e-04\n",
      "Epoch 451/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8558e-04 - val_loss: 4.8340e-04\n",
      "Epoch 452/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8311e-04 - val_loss: 4.8336e-04\n",
      "Epoch 453/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8222e-04 - val_loss: 4.8313e-04\n",
      "Epoch 454/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8318e-04 - val_loss: 4.8307e-04\n",
      "Epoch 455/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8386e-04 - val_loss: 4.8300e-04\n",
      "Epoch 456/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.8316e-04 - val_loss: 4.8303e-04\n",
      "Epoch 457/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8354e-04 - val_loss: 4.8397e-04\n",
      "Epoch 458/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8642e-04 - val_loss: 4.8299e-04\n",
      "Epoch 459/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8454e-04 - val_loss: 4.8297e-04\n",
      "Epoch 460/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8464e-04 - val_loss: 4.8277e-04\n",
      "Epoch 461/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8323e-04 - val_loss: 4.8270e-04\n",
      "Epoch 462/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.8351e-04 - val_loss: 4.8255e-04\n",
      "Epoch 463/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.8231e-04 - val_loss: 4.8245e-04\n",
      "Epoch 464/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.8399e-04 - val_loss: 4.8237e-04\n",
      "Epoch 465/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8254e-04 - val_loss: 4.8266e-04\n",
      "Epoch 466/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8384e-04 - val_loss: 4.8253e-04\n",
      "Epoch 467/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8246e-04 - val_loss: 4.8203e-04\n",
      "Epoch 468/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8224e-04 - val_loss: 4.8226e-04\n",
      "Epoch 469/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8382e-04 - val_loss: 4.8210e-04\n",
      "Epoch 470/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8491e-04 - val_loss: 4.8189e-04\n",
      "Epoch 471/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8407e-04 - val_loss: 4.8174e-04\n",
      "Epoch 472/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8380e-04 - val_loss: 4.8231e-04\n",
      "Epoch 473/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8061e-04 - val_loss: 4.8188e-04\n",
      "Epoch 474/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.8207e-04 - val_loss: 4.8184e-04\n",
      "Epoch 475/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8307e-04 - val_loss: 4.8197e-04\n",
      "Epoch 476/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8163e-04 - val_loss: 4.8132e-04\n",
      "Epoch 477/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8101e-04 - val_loss: 4.8137e-04\n",
      "Epoch 478/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8382e-04 - val_loss: 4.8130e-04\n",
      "Epoch 479/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8233e-04 - val_loss: 4.8122e-04\n",
      "Epoch 480/10000\n",
      "95/95 [==============================] - 95s 995ms/step - loss: 4.8040e-04 - val_loss: 4.8110e-04\n",
      "Epoch 481/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.8321e-04 - val_loss: 4.8114e-04\n",
      "Epoch 482/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.8331e-04 - val_loss: 4.8110e-04\n",
      "Epoch 483/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8327e-04 - val_loss: 4.8132e-04\n",
      "Epoch 484/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8303e-04 - val_loss: 4.8109e-04\n",
      "Epoch 485/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8165e-04 - val_loss: 4.8081e-04\n",
      "Epoch 486/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.8235e-04 - val_loss: 4.8084e-04\n",
      "Epoch 487/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8184e-04 - val_loss: 4.8075e-04\n",
      "Epoch 488/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8180e-04 - val_loss: 4.8097e-04\n",
      "Epoch 489/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8113e-04 - val_loss: 4.8051e-04\n",
      "Epoch 490/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8144e-04 - val_loss: 4.8044e-04\n",
      "Epoch 491/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.8278e-04 - val_loss: 4.8027e-04\n",
      "Epoch 492/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8011e-04 - val_loss: 4.8028e-04\n",
      "Epoch 493/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8235e-04 - val_loss: 4.8182e-04\n",
      "Epoch 494/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8279e-04 - val_loss: 4.8008e-04\n",
      "Epoch 495/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8112e-04 - val_loss: 4.8009e-04\n",
      "Epoch 496/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8156e-04 - val_loss: 4.7992e-04\n",
      "Epoch 497/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8017e-04 - val_loss: 4.7984e-04\n",
      "Epoch 498/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8076e-04 - val_loss: 4.7987e-04\n",
      "Epoch 499/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8194e-04 - val_loss: 4.7969e-04\n",
      "Epoch 500/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.7950e-04 - val_loss: 4.8013e-04\n",
      "Epoch 501/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8068e-04 - val_loss: 4.7970e-04\n",
      "Epoch 502/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7979e-04 - val_loss: 4.8032e-04\n",
      "Epoch 503/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7950e-04 - val_loss: 4.7958e-04\n",
      "Epoch 504/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.7891e-04 - val_loss: 4.7986e-04\n",
      "Epoch 505/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8044e-04 - val_loss: 4.7932e-04\n",
      "Epoch 506/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8051e-04 - val_loss: 4.7973e-04\n",
      "Epoch 507/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7945e-04 - val_loss: 4.7922e-04\n",
      "Epoch 508/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8089e-04 - val_loss: 4.7930e-04\n",
      "Epoch 509/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.8104e-04 - val_loss: 4.7916e-04\n",
      "Epoch 510/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8112e-04 - val_loss: 4.7894e-04\n",
      "Epoch 511/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7942e-04 - val_loss: 4.7882e-04\n",
      "Epoch 512/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.8078e-04 - val_loss: 4.7868e-04\n",
      "Epoch 513/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7888e-04 - val_loss: 4.7874e-04\n",
      "Epoch 514/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7907e-04 - val_loss: 4.7857e-04\n",
      "Epoch 515/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.8072e-04 - val_loss: 4.7903e-04\n",
      "Epoch 516/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7850e-04 - val_loss: 4.7846e-04\n",
      "Epoch 517/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7794e-04 - val_loss: 4.7895e-04\n",
      "Epoch 518/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7697e-04 - val_loss: 4.7830e-04\n",
      "Epoch 519/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.7751e-04 - val_loss: 4.7822e-04\n",
      "Epoch 520/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7902e-04 - val_loss: 4.7877e-04\n",
      "Epoch 521/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7960e-04 - val_loss: 4.8016e-04\n",
      "Epoch 522/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.7762e-04 - val_loss: 4.7906e-04\n",
      "Epoch 523/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7979e-04 - val_loss: 4.7802e-04\n",
      "Epoch 524/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.7842e-04 - val_loss: 4.7787e-04\n",
      "Epoch 525/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7818e-04 - val_loss: 4.7779e-04\n",
      "Epoch 526/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.7802e-04 - val_loss: 4.7775e-04\n",
      "Epoch 527/10000\n",
      "95/95 [==============================] - 94s 995ms/step - loss: 4.7883e-04 - val_loss: 4.7780e-04\n",
      "Epoch 528/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.7672e-04 - val_loss: 4.7819e-04\n",
      "Epoch 529/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7821e-04 - val_loss: 4.7746e-04\n",
      "Epoch 530/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.7904e-04 - val_loss: 4.7757e-04\n",
      "Epoch 531/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.7745e-04 - val_loss: 4.7751e-04\n",
      "Epoch 532/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.7660e-04 - val_loss: 4.7765e-04\n",
      "Epoch 533/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7945e-04 - val_loss: 4.7726e-04\n",
      "Epoch 534/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7710e-04 - val_loss: 4.7806e-04\n",
      "Epoch 535/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7805e-04 - val_loss: 4.7832e-04\n",
      "Epoch 536/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7715e-04 - val_loss: 4.7710e-04\n",
      "Epoch 537/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7552e-04 - val_loss: 4.7741e-04\n",
      "Epoch 538/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7758e-04 - val_loss: 4.7702e-04\n",
      "Epoch 539/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7695e-04 - val_loss: 4.7673e-04\n",
      "Epoch 540/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7590e-04 - val_loss: 4.7684e-04\n",
      "Epoch 541/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.7728e-04 - val_loss: 4.7688e-04\n",
      "Epoch 542/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7830e-04 - val_loss: 4.7706e-04\n",
      "Epoch 543/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.7953e-04 - val_loss: 4.7662e-04\n",
      "Epoch 544/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.7813e-04 - val_loss: 4.7643e-04\n",
      "Epoch 545/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7612e-04 - val_loss: 4.7648e-04\n",
      "Epoch 546/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7683e-04 - val_loss: 4.7662e-04\n",
      "Epoch 547/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.7551e-04 - val_loss: 4.7637e-04\n",
      "Epoch 548/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7606e-04 - val_loss: 4.7629e-04\n",
      "Epoch 549/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7509e-04 - val_loss: 4.7615e-04\n",
      "Epoch 550/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7542e-04 - val_loss: 4.7632e-04\n",
      "Epoch 551/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7657e-04 - val_loss: 4.7613e-04\n",
      "Epoch 552/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7559e-04 - val_loss: 4.7595e-04\n",
      "Epoch 553/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7514e-04 - val_loss: 4.7627e-04\n",
      "Epoch 554/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.7537e-04 - val_loss: 4.7596e-04\n",
      "Epoch 555/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.7561e-04 - val_loss: 4.7621e-04\n",
      "Epoch 556/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7669e-04 - val_loss: 4.7568e-04\n",
      "Epoch 557/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.7440e-04 - val_loss: 4.7561e-04\n",
      "Epoch 558/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.7657e-04 - val_loss: 4.7616e-04\n",
      "Epoch 559/10000\n",
      "95/95 [==============================] - 95s 995ms/step - loss: 4.7306e-04 - val_loss: 4.7544e-04\n",
      "Epoch 560/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7491e-04 - val_loss: 4.7550e-04\n",
      "Epoch 561/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7681e-04 - val_loss: 4.7546e-04\n",
      "Epoch 562/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7357e-04 - val_loss: 4.7548e-04\n",
      "Epoch 563/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7613e-04 - val_loss: 4.7552e-04\n",
      "Epoch 564/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7708e-04 - val_loss: 4.7520e-04\n",
      "Epoch 565/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7507e-04 - val_loss: 4.7507e-04\n",
      "Epoch 566/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7316e-04 - val_loss: 4.7571e-04\n",
      "Epoch 567/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7388e-04 - val_loss: 4.7517e-04\n",
      "Epoch 568/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7564e-04 - val_loss: 4.7488e-04\n",
      "Epoch 569/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.7523e-04 - val_loss: 4.7486e-04\n",
      "Epoch 570/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7520e-04 - val_loss: 4.7480e-04\n",
      "Epoch 571/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7465e-04 - val_loss: 4.7470e-04\n",
      "Epoch 572/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7632e-04 - val_loss: 4.7477e-04\n",
      "Epoch 573/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7679e-04 - val_loss: 4.7545e-04\n",
      "Epoch 574/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.7518e-04 - val_loss: 4.7443e-04\n",
      "Epoch 575/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7510e-04 - val_loss: 4.7496e-04\n",
      "Epoch 576/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7345e-04 - val_loss: 4.7552e-04\n",
      "Epoch 577/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7303e-04 - val_loss: 4.7556e-04\n",
      "Epoch 578/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.7421e-04 - val_loss: 4.7427e-04\n",
      "Epoch 579/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7585e-04 - val_loss: 4.7453e-04\n",
      "Epoch 580/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7502e-04 - val_loss: 4.7466e-04\n",
      "Epoch 581/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7441e-04 - val_loss: 4.7407e-04\n",
      "Epoch 582/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7331e-04 - val_loss: 4.7447e-04\n",
      "Epoch 583/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7350e-04 - val_loss: 4.7438e-04\n",
      "Epoch 584/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.7434e-04 - val_loss: 4.7390e-04\n",
      "Epoch 585/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7204e-04 - val_loss: 4.7379e-04\n",
      "Epoch 586/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.7402e-04 - val_loss: 4.7362e-04\n",
      "Epoch 587/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7405e-04 - val_loss: 4.7365e-04\n",
      "Epoch 588/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7374e-04 - val_loss: 4.7431e-04\n",
      "Epoch 589/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7514e-04 - val_loss: 4.7382e-04\n",
      "Epoch 590/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7410e-04 - val_loss: 4.7358e-04\n",
      "Epoch 591/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7536e-04 - val_loss: 4.7334e-04\n",
      "Epoch 592/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7429e-04 - val_loss: 4.7345e-04\n",
      "Epoch 593/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7350e-04 - val_loss: 4.7374e-04\n",
      "Epoch 594/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7319e-04 - val_loss: 4.7317e-04\n",
      "Epoch 595/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.7177e-04 - val_loss: 4.7379e-04\n",
      "Epoch 596/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7436e-04 - val_loss: 4.7336e-04\n",
      "Epoch 597/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7364e-04 - val_loss: 4.7307e-04\n",
      "Epoch 598/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7294e-04 - val_loss: 4.7293e-04\n",
      "Epoch 599/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.7360e-04 - val_loss: 4.7312e-04\n",
      "Epoch 600/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7240e-04 - val_loss: 4.7312e-04\n",
      "Epoch 601/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7352e-04 - val_loss: 4.7300e-04\n",
      "Epoch 602/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.7384e-04 - val_loss: 4.7286e-04\n",
      "Epoch 603/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.7250e-04 - val_loss: 4.7332e-04\n",
      "Epoch 604/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7301e-04 - val_loss: 4.7355e-04\n",
      "Epoch 605/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7283e-04 - val_loss: 4.7316e-04\n",
      "Epoch 606/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7128e-04 - val_loss: 4.7257e-04\n",
      "Epoch 607/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.7113e-04 - val_loss: 4.7244e-04\n",
      "Epoch 608/10000\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 4.7286e-04 - val_loss: 4.7222e-04\n",
      "Epoch 609/10000\n",
      "95/95 [==============================] - 95s 997ms/step - loss: 4.7292e-04 - val_loss: 4.7234e-04\n",
      "Epoch 610/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7395e-04 - val_loss: 4.7211e-04\n",
      "Epoch 611/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7434e-04 - val_loss: 4.7203e-04\n",
      "Epoch 612/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.7185e-04 - val_loss: 4.7191e-04\n",
      "Epoch 613/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7132e-04 - val_loss: 4.7289e-04\n",
      "Epoch 614/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7140e-04 - val_loss: 4.7209e-04\n",
      "Epoch 615/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.7183e-04 - val_loss: 4.7204e-04\n",
      "Epoch 616/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7212e-04 - val_loss: 4.7262e-04\n",
      "Epoch 617/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7283e-04 - val_loss: 4.7184e-04\n",
      "Epoch 618/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.7198e-04 - val_loss: 4.7219e-04\n",
      "Epoch 619/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7407e-04 - val_loss: 4.7158e-04\n",
      "Epoch 620/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7313e-04 - val_loss: 4.7138e-04\n",
      "Epoch 621/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7227e-04 - val_loss: 4.7163e-04\n",
      "Epoch 622/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.7314e-04 - val_loss: 4.7142e-04\n",
      "Epoch 623/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7071e-04 - val_loss: 4.7129e-04\n",
      "Epoch 624/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7140e-04 - val_loss: 4.7187e-04\n",
      "Epoch 625/10000\n",
      "95/95 [==============================] - 99s 1s/step - loss: 4.7223e-04 - val_loss: 4.7114e-04\n",
      "Epoch 626/10000\n",
      "95/95 [==============================] - 98s 1s/step - loss: 4.7217e-04 - val_loss: 4.7117e-04\n",
      "Epoch 627/10000\n",
      "95/95 [==============================] - 99s 1s/step - loss: 4.7095e-04 - val_loss: 4.7120e-04\n",
      "Epoch 628/10000\n",
      "95/95 [==============================] - 98s 1s/step - loss: 4.7073e-04 - val_loss: 4.7100e-04\n",
      "Epoch 629/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7122e-04 - val_loss: 4.7124e-04\n",
      "Epoch 630/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7058e-04 - val_loss: 4.7105e-04\n",
      "Epoch 631/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.6930e-04 - val_loss: 4.7083e-04\n",
      "Epoch 632/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7209e-04 - val_loss: 4.7082e-04\n",
      "Epoch 633/10000\n",
      "95/95 [==============================] - 98s 1s/step - loss: 4.7011e-04 - val_loss: 4.7116e-04\n",
      "Epoch 634/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7022e-04 - val_loss: 4.7079e-04\n",
      "Epoch 635/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7075e-04 - val_loss: 4.7072e-04\n",
      "Epoch 636/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.6980e-04 - val_loss: 4.7092e-04\n",
      "Epoch 637/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7291e-04 - val_loss: 4.7035e-04\n",
      "Epoch 638/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.7095e-04 - val_loss: 4.7065e-04\n",
      "Epoch 639/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.6928e-04 - val_loss: 4.7109e-04\n",
      "Epoch 640/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.7006e-04 - val_loss: 4.7091e-04\n",
      "Epoch 641/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.7104e-04 - val_loss: 4.7022e-04\n",
      "Epoch 642/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.7089e-04 - val_loss: 4.7041e-04\n",
      "Epoch 643/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.7072e-04 - val_loss: 4.7003e-04\n",
      "Epoch 644/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.6990e-04 - val_loss: 4.7010e-04\n",
      "Epoch 645/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.7105e-04 - val_loss: 4.6995e-04\n",
      "Epoch 646/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6789e-04 - val_loss: 4.7014e-04\n",
      "Epoch 647/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.7162e-04 - val_loss: 4.6984e-04\n",
      "Epoch 648/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6840e-04 - val_loss: 4.7070e-04\n",
      "Epoch 649/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.7019e-04 - val_loss: 4.6964e-04\n",
      "Epoch 650/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6892e-04 - val_loss: 4.6974e-04\n",
      "Epoch 651/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.7096e-04 - val_loss: 4.6959e-04\n",
      "Epoch 652/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.7137e-04 - val_loss: 4.6969e-04\n",
      "Epoch 653/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6876e-04 - val_loss: 4.6982e-04\n",
      "Epoch 654/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6888e-04 - val_loss: 4.6960e-04\n",
      "Epoch 655/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6816e-04 - val_loss: 4.7057e-04\n",
      "Epoch 656/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.7046e-04 - val_loss: 4.6993e-04\n",
      "Epoch 657/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.7168e-04 - val_loss: 4.6924e-04\n",
      "Epoch 658/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6849e-04 - val_loss: 4.6936e-04\n",
      "Epoch 659/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6983e-04 - val_loss: 4.6935e-04\n",
      "Epoch 660/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6849e-04 - val_loss: 4.6922e-04\n",
      "Epoch 661/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6879e-04 - val_loss: 4.6891e-04\n",
      "Epoch 662/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6662e-04 - val_loss: 4.6945e-04\n",
      "Epoch 663/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6793e-04 - val_loss: 4.6884e-04\n",
      "Epoch 664/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.7012e-04 - val_loss: 4.6904e-04\n",
      "Epoch 665/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6889e-04 - val_loss: 4.6952e-04\n",
      "Epoch 666/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6985e-04 - val_loss: 4.6886e-04\n",
      "Epoch 667/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6870e-04 - val_loss: 4.6893e-04\n",
      "Epoch 668/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6826e-04 - val_loss: 4.6862e-04\n",
      "Epoch 669/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6931e-04 - val_loss: 4.6860e-04\n",
      "Epoch 670/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.7046e-04 - val_loss: 4.6849e-04\n",
      "Epoch 671/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.7033e-04 - val_loss: 4.6854e-04\n",
      "Epoch 672/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6856e-04 - val_loss: 4.6869e-04\n",
      "Epoch 673/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6683e-04 - val_loss: 4.6812e-04\n",
      "Epoch 674/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6705e-04 - val_loss: 4.6817e-04\n",
      "Epoch 675/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6948e-04 - val_loss: 4.6805e-04\n",
      "Epoch 676/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6729e-04 - val_loss: 4.6805e-04\n",
      "Epoch 677/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6763e-04 - val_loss: 4.6796e-04\n",
      "Epoch 678/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6766e-04 - val_loss: 4.6815e-04\n",
      "Epoch 679/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6882e-04 - val_loss: 4.6813e-04\n",
      "Epoch 680/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6689e-04 - val_loss: 4.6779e-04\n",
      "Epoch 681/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6717e-04 - val_loss: 4.6781e-04\n",
      "Epoch 682/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6844e-04 - val_loss: 4.6795e-04\n",
      "Epoch 683/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6860e-04 - val_loss: 4.6804e-04\n",
      "Epoch 684/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6687e-04 - val_loss: 4.6763e-04\n",
      "Epoch 685/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6699e-04 - val_loss: 4.6777e-04\n",
      "Epoch 686/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6925e-04 - val_loss: 4.6775e-04\n",
      "Epoch 687/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6698e-04 - val_loss: 4.6735e-04\n",
      "Epoch 688/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6709e-04 - val_loss: 4.6741e-04\n",
      "Epoch 689/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6587e-04 - val_loss: 4.6783e-04\n",
      "Epoch 690/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6692e-04 - val_loss: 4.6715e-04\n",
      "Epoch 691/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6875e-04 - val_loss: 4.6835e-04\n",
      "Epoch 692/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6665e-04 - val_loss: 4.6720e-04\n",
      "Epoch 693/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6651e-04 - val_loss: 4.6708e-04\n",
      "Epoch 694/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6723e-04 - val_loss: 4.6765e-04\n",
      "Epoch 695/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6686e-04 - val_loss: 4.6726e-04\n",
      "Epoch 696/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6628e-04 - val_loss: 4.6717e-04\n",
      "Epoch 697/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6625e-04 - val_loss: 4.6730e-04\n",
      "Epoch 698/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6822e-04 - val_loss: 4.6742e-04\n",
      "Epoch 699/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6617e-04 - val_loss: 4.6751e-04\n",
      "Epoch 700/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6742e-04 - val_loss: 4.6752e-04\n",
      "Epoch 701/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6511e-04 - val_loss: 4.6653e-04\n",
      "Epoch 702/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6606e-04 - val_loss: 4.6657e-04\n",
      "Epoch 703/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6540e-04 - val_loss: 4.6705e-04\n",
      "Epoch 704/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6500e-04 - val_loss: 4.6661e-04\n",
      "Epoch 705/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6555e-04 - val_loss: 4.6695e-04\n",
      "Epoch 706/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6685e-04 - val_loss: 4.6651e-04\n",
      "Epoch 707/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6559e-04 - val_loss: 4.6628e-04\n",
      "Epoch 708/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6484e-04 - val_loss: 4.6663e-04\n",
      "Epoch 709/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6676e-04 - val_loss: 4.6633e-04\n",
      "Epoch 710/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6583e-04 - val_loss: 4.6695e-04\n",
      "Epoch 711/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6498e-04 - val_loss: 4.6615e-04\n",
      "Epoch 712/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6572e-04 - val_loss: 4.6613e-04\n",
      "Epoch 713/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6734e-04 - val_loss: 4.6595e-04\n",
      "Epoch 714/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6616e-04 - val_loss: 4.6823e-04\n",
      "Epoch 715/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6357e-04 - val_loss: 4.6591e-04\n",
      "Epoch 716/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6619e-04 - val_loss: 4.6570e-04\n",
      "Epoch 717/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6508e-04 - val_loss: 4.6582e-04\n",
      "Epoch 718/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6580e-04 - val_loss: 4.6556e-04\n",
      "Epoch 719/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6583e-04 - val_loss: 4.6575e-04\n",
      "Epoch 720/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6626e-04 - val_loss: 4.6571e-04\n",
      "Epoch 721/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6435e-04 - val_loss: 4.6541e-04\n",
      "Epoch 722/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6555e-04 - val_loss: 4.6543e-04\n",
      "Epoch 723/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6503e-04 - val_loss: 4.6575e-04\n",
      "Epoch 724/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6617e-04 - val_loss: 4.6532e-04\n",
      "Epoch 725/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6636e-04 - val_loss: 4.6537e-04\n",
      "Epoch 726/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6703e-04 - val_loss: 4.6515e-04\n",
      "Epoch 727/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6531e-04 - val_loss: 4.6540e-04\n",
      "Epoch 728/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6645e-04 - val_loss: 4.6523e-04\n",
      "Epoch 729/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6604e-04 - val_loss: 4.6568e-04\n",
      "Epoch 730/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6435e-04 - val_loss: 4.6553e-04\n",
      "Epoch 731/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6511e-04 - val_loss: 4.6558e-04\n",
      "Epoch 732/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6453e-04 - val_loss: 4.6527e-04\n",
      "Epoch 733/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6355e-04 - val_loss: 4.6487e-04\n",
      "Epoch 734/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6286e-04 - val_loss: 4.6502e-04\n",
      "Epoch 735/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6472e-04 - val_loss: 4.6486e-04\n",
      "Epoch 736/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6450e-04 - val_loss: 4.6487e-04\n",
      "Epoch 737/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6336e-04 - val_loss: 4.6563e-04\n",
      "Epoch 738/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6589e-04 - val_loss: 4.6488e-04\n",
      "Epoch 739/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6546e-04 - val_loss: 4.6549e-04\n",
      "Epoch 740/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6493e-04 - val_loss: 4.6476e-04\n",
      "Epoch 741/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6516e-04 - val_loss: 4.6433e-04\n",
      "Epoch 742/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6337e-04 - val_loss: 4.6491e-04\n",
      "Epoch 743/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6486e-04 - val_loss: 4.6431e-04\n",
      "Epoch 744/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6319e-04 - val_loss: 4.6427e-04\n",
      "Epoch 745/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6531e-04 - val_loss: 4.6449e-04\n",
      "Epoch 746/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6374e-04 - val_loss: 4.6497e-04\n",
      "Epoch 747/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6356e-04 - val_loss: 4.6442e-04\n",
      "Epoch 748/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6250e-04 - val_loss: 4.6404e-04\n",
      "Epoch 749/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6392e-04 - val_loss: 4.6420e-04\n",
      "Epoch 750/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6385e-04 - val_loss: 4.6389e-04\n",
      "Epoch 751/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6412e-04 - val_loss: 4.6445e-04\n",
      "Epoch 752/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6393e-04 - val_loss: 4.6409e-04\n",
      "Epoch 753/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6402e-04 - val_loss: 4.6371e-04\n",
      "Epoch 754/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6338e-04 - val_loss: 4.6386e-04\n",
      "Epoch 755/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6348e-04 - val_loss: 4.6369e-04\n",
      "Epoch 756/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6302e-04 - val_loss: 4.6378e-04\n",
      "Epoch 757/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6160e-04 - val_loss: 4.6361e-04\n",
      "Epoch 758/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6283e-04 - val_loss: 4.6355e-04\n",
      "Epoch 759/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6274e-04 - val_loss: 4.6497e-04\n",
      "Epoch 760/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6358e-04 - val_loss: 4.6396e-04\n",
      "Epoch 761/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6095e-04 - val_loss: 4.6342e-04\n",
      "Epoch 762/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6227e-04 - val_loss: 4.6323e-04\n",
      "Epoch 763/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6204e-04 - val_loss: 4.6354e-04\n",
      "Epoch 764/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6205e-04 - val_loss: 4.6322e-04\n",
      "Epoch 765/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6402e-04 - val_loss: 4.6325e-04\n",
      "Epoch 766/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6294e-04 - val_loss: 4.6308e-04\n",
      "Epoch 767/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6041e-04 - val_loss: 4.6318e-04\n",
      "Epoch 768/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6302e-04 - val_loss: 4.6314e-04\n",
      "Epoch 769/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6170e-04 - val_loss: 4.6299e-04\n",
      "Epoch 770/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6086e-04 - val_loss: 4.6304e-04\n",
      "Epoch 771/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6162e-04 - val_loss: 4.6298e-04\n",
      "Epoch 772/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6093e-04 - val_loss: 4.6290e-04\n",
      "Epoch 773/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6107e-04 - val_loss: 4.6282e-04\n",
      "Epoch 774/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6249e-04 - val_loss: 4.6284e-04\n",
      "Epoch 775/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6183e-04 - val_loss: 4.6303e-04\n",
      "Epoch 776/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6327e-04 - val_loss: 4.6264e-04\n",
      "Epoch 777/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6121e-04 - val_loss: 4.6253e-04\n",
      "Epoch 778/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6259e-04 - val_loss: 4.6257e-04\n",
      "Epoch 779/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6240e-04 - val_loss: 4.6257e-04\n",
      "Epoch 780/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6196e-04 - val_loss: 4.6251e-04\n",
      "Epoch 781/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6233e-04 - val_loss: 4.6223e-04\n",
      "Epoch 782/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6124e-04 - val_loss: 4.6259e-04\n",
      "Epoch 783/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6185e-04 - val_loss: 4.6231e-04\n",
      "Epoch 784/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6214e-04 - val_loss: 4.6206e-04\n",
      "Epoch 785/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6312e-04 - val_loss: 4.6227e-04\n",
      "Epoch 786/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6133e-04 - val_loss: 4.6268e-04\n",
      "Epoch 787/10000\n",
      "95/95 [==============================] - 102s 1s/step - loss: 4.6183e-04 - val_loss: 4.6188e-04\n",
      "Epoch 788/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5939e-04 - val_loss: 4.6203e-04\n",
      "Epoch 789/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6107e-04 - val_loss: 4.6193e-04\n",
      "Epoch 790/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6194e-04 - val_loss: 4.6181e-04\n",
      "Epoch 791/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6045e-04 - val_loss: 4.6181e-04\n",
      "Epoch 792/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6088e-04 - val_loss: 4.6175e-04\n",
      "Epoch 793/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6199e-04 - val_loss: 4.6230e-04\n",
      "Epoch 794/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6101e-04 - val_loss: 4.6156e-04\n",
      "Epoch 795/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5905e-04 - val_loss: 4.6153e-04\n",
      "Epoch 796/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6065e-04 - val_loss: 4.6142e-04\n",
      "Epoch 797/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6116e-04 - val_loss: 4.6135e-04\n",
      "Epoch 798/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5887e-04 - val_loss: 4.6152e-04\n",
      "Epoch 799/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6072e-04 - val_loss: 4.6180e-04\n",
      "Epoch 800/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5995e-04 - val_loss: 4.6243e-04\n",
      "Epoch 801/10000\n",
      "95/95 [==============================] - 98s 1s/step - loss: 4.5968e-04 - val_loss: 4.6140e-04\n",
      "Epoch 802/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5994e-04 - val_loss: 4.6132e-04\n",
      "Epoch 803/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.5935e-04 - val_loss: 4.6168e-04\n",
      "Epoch 804/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.6027e-04 - val_loss: 4.6107e-04\n",
      "Epoch 805/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6020e-04 - val_loss: 4.6122e-04\n",
      "Epoch 806/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6010e-04 - val_loss: 4.6108e-04\n",
      "Epoch 807/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5907e-04 - val_loss: 4.6133e-04\n",
      "Epoch 808/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6199e-04 - val_loss: 4.6108e-04\n",
      "Epoch 809/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6026e-04 - val_loss: 4.6127e-04\n",
      "Epoch 810/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5861e-04 - val_loss: 4.6092e-04\n",
      "Epoch 811/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6128e-04 - val_loss: 4.6102e-04\n",
      "Epoch 812/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.6050e-04 - val_loss: 4.6075e-04\n",
      "Epoch 813/10000\n",
      "95/95 [==============================] - 117s 1s/step - loss: 4.5933e-04 - val_loss: 4.6136e-04\n",
      "Epoch 814/10000\n",
      "95/95 [==============================] - 98s 1s/step - loss: 4.6051e-04 - val_loss: 4.6066e-04\n",
      "Epoch 815/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6039e-04 - val_loss: 4.6060e-04\n",
      "Epoch 816/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6149e-04 - val_loss: 4.6068e-04\n",
      "Epoch 817/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6071e-04 - val_loss: 4.6047e-04\n",
      "Epoch 818/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6016e-04 - val_loss: 4.6056e-04\n",
      "Epoch 819/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5906e-04 - val_loss: 4.6065e-04\n",
      "Epoch 820/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6001e-04 - val_loss: 4.6055e-04\n",
      "Epoch 821/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6191e-04 - val_loss: 4.6008e-04\n",
      "Epoch 822/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5874e-04 - val_loss: 4.6076e-04\n",
      "Epoch 823/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5783e-04 - val_loss: 4.6038e-04\n",
      "Epoch 824/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5914e-04 - val_loss: 4.6009e-04\n",
      "Epoch 825/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6119e-04 - val_loss: 4.6070e-04\n",
      "Epoch 826/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.6017e-04 - val_loss: 4.6020e-04\n",
      "Epoch 827/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5962e-04 - val_loss: 4.6115e-04\n",
      "Epoch 828/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5928e-04 - val_loss: 4.5994e-04\n",
      "Epoch 829/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6004e-04 - val_loss: 4.5996e-04\n",
      "Epoch 830/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5933e-04 - val_loss: 4.6002e-04\n",
      "Epoch 831/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.6021e-04 - val_loss: 4.5974e-04\n",
      "Epoch 832/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5870e-04 - val_loss: 4.6031e-04\n",
      "Epoch 833/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5976e-04 - val_loss: 4.6000e-04\n",
      "Epoch 834/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5977e-04 - val_loss: 4.5969e-04\n",
      "Epoch 835/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5888e-04 - val_loss: 4.5996e-04\n",
      "Epoch 836/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5720e-04 - val_loss: 4.5978e-04\n",
      "Epoch 837/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5915e-04 - val_loss: 4.5971e-04\n",
      "Epoch 838/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5666e-04 - val_loss: 4.5942e-04\n",
      "Epoch 839/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5788e-04 - val_loss: 4.5939e-04\n",
      "Epoch 840/10000\n",
      "95/95 [==============================] - 99s 1s/step - loss: 4.5779e-04 - val_loss: 4.5973e-04\n",
      "Epoch 841/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5895e-04 - val_loss: 4.5934e-04\n",
      "Epoch 842/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.5718e-04 - val_loss: 4.5940e-04\n",
      "Epoch 843/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5766e-04 - val_loss: 4.6026e-04\n",
      "Epoch 844/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5912e-04 - val_loss: 4.5908e-04\n",
      "Epoch 845/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5839e-04 - val_loss: 4.5910e-04\n",
      "Epoch 846/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5793e-04 - val_loss: 4.5924e-04\n",
      "Epoch 847/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5801e-04 - val_loss: 4.5900e-04\n",
      "Epoch 848/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5662e-04 - val_loss: 4.5907e-04\n",
      "Epoch 849/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5822e-04 - val_loss: 4.5890e-04\n",
      "Epoch 850/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5671e-04 - val_loss: 4.5892e-04\n",
      "Epoch 851/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5992e-04 - val_loss: 4.5973e-04\n",
      "Epoch 852/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5879e-04 - val_loss: 4.5905e-04\n",
      "Epoch 853/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5839e-04 - val_loss: 4.5872e-04\n",
      "Epoch 854/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5769e-04 - val_loss: 4.5863e-04\n",
      "Epoch 855/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5834e-04 - val_loss: 4.5872e-04\n",
      "Epoch 856/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5793e-04 - val_loss: 4.5880e-04\n",
      "Epoch 857/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5561e-04 - val_loss: 4.5865e-04\n",
      "Epoch 858/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5721e-04 - val_loss: 4.5949e-04\n",
      "Epoch 859/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5539e-04 - val_loss: 4.5892e-04\n",
      "Epoch 860/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5839e-04 - val_loss: 4.5848e-04\n",
      "Epoch 861/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5767e-04 - val_loss: 4.5878e-04\n",
      "Epoch 862/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5814e-04 - val_loss: 4.5843e-04\n",
      "Epoch 863/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5768e-04 - val_loss: 4.5880e-04\n",
      "Epoch 864/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5649e-04 - val_loss: 4.5856e-04\n",
      "Epoch 865/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5799e-04 - val_loss: 4.5860e-04\n",
      "Epoch 866/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5793e-04 - val_loss: 4.5824e-04\n",
      "Epoch 867/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5667e-04 - val_loss: 4.5811e-04\n",
      "Epoch 868/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5694e-04 - val_loss: 4.5804e-04\n",
      "Epoch 869/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5740e-04 - val_loss: 4.5874e-04\n",
      "Epoch 870/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5710e-04 - val_loss: 4.5799e-04\n",
      "Epoch 871/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5624e-04 - val_loss: 4.5813e-04\n",
      "Epoch 872/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5823e-04 - val_loss: 4.5781e-04\n",
      "Epoch 873/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5567e-04 - val_loss: 4.5796e-04\n",
      "Epoch 874/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5682e-04 - val_loss: 4.5780e-04\n",
      "Epoch 875/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5684e-04 - val_loss: 4.5790e-04\n",
      "Epoch 876/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.5777e-04 - val_loss: 4.5800e-04\n",
      "Epoch 877/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5829e-04 - val_loss: 4.5823e-04\n",
      "Epoch 878/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5784e-04 - val_loss: 4.5794e-04\n",
      "Epoch 879/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5752e-04 - val_loss: 4.5806e-04\n",
      "Epoch 880/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5711e-04 - val_loss: 4.5741e-04\n",
      "Epoch 881/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5790e-04 - val_loss: 4.5750e-04\n",
      "Epoch 882/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5571e-04 - val_loss: 4.5739e-04\n",
      "Epoch 883/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5607e-04 - val_loss: 4.5804e-04\n",
      "Epoch 884/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5695e-04 - val_loss: 4.5715e-04\n",
      "Epoch 885/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5566e-04 - val_loss: 4.5746e-04\n",
      "Epoch 886/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5515e-04 - val_loss: 4.5722e-04\n",
      "Epoch 887/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5607e-04 - val_loss: 4.5722e-04\n",
      "Epoch 888/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5671e-04 - val_loss: 4.5715e-04\n",
      "Epoch 889/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5731e-04 - val_loss: 4.5841e-04\n",
      "Epoch 890/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5633e-04 - val_loss: 4.5682e-04\n",
      "Epoch 891/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5543e-04 - val_loss: 4.5701e-04\n",
      "Epoch 892/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.5559e-04 - val_loss: 4.5865e-04\n",
      "Epoch 893/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5530e-04 - val_loss: 4.5694e-04\n",
      "Epoch 894/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5556e-04 - val_loss: 4.5681e-04\n",
      "Epoch 895/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5684e-04 - val_loss: 4.5691e-04\n",
      "Epoch 896/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5640e-04 - val_loss: 4.5688e-04\n",
      "Epoch 897/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5760e-04 - val_loss: 4.5658e-04\n",
      "Epoch 898/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5476e-04 - val_loss: 4.5730e-04\n",
      "Epoch 899/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5484e-04 - val_loss: 4.5657e-04\n",
      "Epoch 900/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5616e-04 - val_loss: 4.5658e-04\n",
      "Epoch 901/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.5550e-04 - val_loss: 4.5661e-04\n",
      "Epoch 902/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5370e-04 - val_loss: 4.5634e-04\n",
      "Epoch 903/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5562e-04 - val_loss: 4.5639e-04\n",
      "Epoch 904/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5791e-04 - val_loss: 4.5656e-04\n",
      "Epoch 905/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5672e-04 - val_loss: 4.5720e-04\n",
      "Epoch 906/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5520e-04 - val_loss: 4.5619e-04\n",
      "Epoch 907/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5574e-04 - val_loss: 4.5612e-04\n",
      "Epoch 908/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5544e-04 - val_loss: 4.5630e-04\n",
      "Epoch 909/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5485e-04 - val_loss: 4.5700e-04\n",
      "Epoch 910/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5602e-04 - val_loss: 4.5604e-04\n",
      "Epoch 911/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5450e-04 - val_loss: 4.5628e-04\n",
      "Epoch 912/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5591e-04 - val_loss: 4.5593e-04\n",
      "Epoch 913/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5566e-04 - val_loss: 4.5684e-04\n",
      "Epoch 914/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5344e-04 - val_loss: 4.5608e-04\n",
      "Epoch 915/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5550e-04 - val_loss: 4.5613e-04\n",
      "Epoch 916/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5426e-04 - val_loss: 4.5578e-04\n",
      "Epoch 917/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5602e-04 - val_loss: 4.5571e-04\n",
      "Epoch 918/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5351e-04 - val_loss: 4.5582e-04\n",
      "Epoch 919/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5492e-04 - val_loss: 4.5655e-04\n",
      "Epoch 920/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5485e-04 - val_loss: 4.5565e-04\n",
      "Epoch 921/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5554e-04 - val_loss: 4.5562e-04\n",
      "Epoch 922/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5498e-04 - val_loss: 4.5590e-04\n",
      "Epoch 923/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.5457e-04 - val_loss: 4.5566e-04\n",
      "Epoch 924/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5563e-04 - val_loss: 4.5553e-04\n",
      "Epoch 925/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5244e-04 - val_loss: 4.5572e-04\n",
      "Epoch 926/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5517e-04 - val_loss: 4.5568e-04\n",
      "Epoch 927/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5452e-04 - val_loss: 4.5551e-04\n",
      "Epoch 928/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5541e-04 - val_loss: 4.5538e-04\n",
      "Epoch 929/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5478e-04 - val_loss: 4.5525e-04\n",
      "Epoch 930/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5371e-04 - val_loss: 4.5591e-04\n",
      "Epoch 931/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5531e-04 - val_loss: 4.5591e-04\n",
      "Epoch 932/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5529e-04 - val_loss: 4.5509e-04\n",
      "Epoch 933/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5492e-04 - val_loss: 4.5572e-04\n",
      "Epoch 934/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5383e-04 - val_loss: 4.5515e-04\n",
      "Epoch 935/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5565e-04 - val_loss: 4.5503e-04\n",
      "Epoch 936/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5405e-04 - val_loss: 4.5509e-04\n",
      "Epoch 937/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5268e-04 - val_loss: 4.5514e-04\n",
      "Epoch 938/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.5262e-04 - val_loss: 4.5519e-04\n",
      "Epoch 939/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.5421e-04 - val_loss: 4.5513e-04\n",
      "Epoch 940/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.5273e-04 - val_loss: 4.5540e-04\n",
      "Epoch 941/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5471e-04 - val_loss: 4.5558e-04\n",
      "Epoch 942/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5528e-04 - val_loss: 4.5455e-04\n",
      "Epoch 943/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5295e-04 - val_loss: 4.5485e-04\n",
      "Epoch 944/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5502e-04 - val_loss: 4.5541e-04\n",
      "Epoch 945/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5295e-04 - val_loss: 4.5463e-04\n",
      "Epoch 946/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5218e-04 - val_loss: 4.5488e-04\n",
      "Epoch 947/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5368e-04 - val_loss: 4.5450e-04\n",
      "Epoch 948/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5147e-04 - val_loss: 4.5441e-04\n",
      "Epoch 949/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5456e-04 - val_loss: 4.5528e-04\n",
      "Epoch 950/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5384e-04 - val_loss: 4.5435e-04\n",
      "Epoch 951/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5293e-04 - val_loss: 4.5444e-04\n",
      "Epoch 952/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5566e-04 - val_loss: 4.5446e-04\n",
      "Epoch 953/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5439e-04 - val_loss: 4.5507e-04\n",
      "Epoch 954/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5366e-04 - val_loss: 4.5427e-04\n",
      "Epoch 955/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5429e-04 - val_loss: 4.5403e-04\n",
      "Epoch 956/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5280e-04 - val_loss: 4.5442e-04\n",
      "Epoch 957/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5073e-04 - val_loss: 4.5436e-04\n",
      "Epoch 958/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5301e-04 - val_loss: 4.5565e-04\n",
      "Epoch 959/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5294e-04 - val_loss: 4.5495e-04\n",
      "Epoch 960/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5311e-04 - val_loss: 4.5380e-04\n",
      "Epoch 961/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5028e-04 - val_loss: 4.5433e-04\n",
      "Epoch 962/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5151e-04 - val_loss: 4.5379e-04\n",
      "Epoch 963/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5358e-04 - val_loss: 4.5399e-04\n",
      "Epoch 964/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5381e-04 - val_loss: 4.5438e-04\n",
      "Epoch 965/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5284e-04 - val_loss: 4.5368e-04\n",
      "Epoch 966/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5353e-04 - val_loss: 4.5481e-04\n",
      "Epoch 967/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5153e-04 - val_loss: 4.5432e-04\n",
      "Epoch 968/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5311e-04 - val_loss: 4.5384e-04\n",
      "Epoch 969/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5022e-04 - val_loss: 4.5371e-04\n",
      "Epoch 970/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5178e-04 - val_loss: 4.5374e-04\n",
      "Epoch 971/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5302e-04 - val_loss: 4.5348e-04\n",
      "Epoch 972/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5297e-04 - val_loss: 4.5341e-04\n",
      "Epoch 973/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5035e-04 - val_loss: 4.5351e-04\n",
      "Epoch 974/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5332e-04 - val_loss: 4.5351e-04\n",
      "Epoch 975/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5218e-04 - val_loss: 4.5375e-04\n",
      "Epoch 976/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5113e-04 - val_loss: 4.5437e-04\n",
      "Epoch 977/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5266e-04 - val_loss: 4.5345e-04\n",
      "Epoch 978/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.5298e-04 - val_loss: 4.5357e-04\n",
      "Epoch 979/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5282e-04 - val_loss: 4.5329e-04\n",
      "Epoch 980/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5317e-04 - val_loss: 4.5344e-04\n",
      "Epoch 981/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5152e-04 - val_loss: 4.5317e-04\n",
      "Epoch 982/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5163e-04 - val_loss: 4.5334e-04\n",
      "Epoch 983/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5158e-04 - val_loss: 4.5350e-04\n",
      "Epoch 984/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.5229e-04 - val_loss: 4.5287e-04\n",
      "Epoch 985/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5228e-04 - val_loss: 4.5318e-04\n",
      "Epoch 986/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5210e-04 - val_loss: 4.5280e-04\n",
      "Epoch 987/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5014e-04 - val_loss: 4.5279e-04\n",
      "Epoch 988/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5198e-04 - val_loss: 4.5295e-04\n",
      "Epoch 989/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5410e-04 - val_loss: 4.5325e-04\n",
      "Epoch 990/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5055e-04 - val_loss: 4.5316e-04\n",
      "Epoch 991/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5019e-04 - val_loss: 4.5262e-04\n",
      "Epoch 992/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5085e-04 - val_loss: 4.5301e-04\n",
      "Epoch 993/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5385e-04 - val_loss: 4.5253e-04\n",
      "Epoch 994/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5029e-04 - val_loss: 4.5240e-04\n",
      "Epoch 995/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5375e-04 - val_loss: 4.5248e-04\n",
      "Epoch 996/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5143e-04 - val_loss: 4.5244e-04\n",
      "Epoch 997/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5169e-04 - val_loss: 4.5243e-04\n",
      "Epoch 998/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5154e-04 - val_loss: 4.5259e-04\n",
      "Epoch 999/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5076e-04 - val_loss: 4.5231e-04\n",
      "Epoch 1000/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5059e-04 - val_loss: 4.5249e-04\n",
      "Epoch 1001/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5118e-04 - val_loss: 4.5215e-04\n",
      "Epoch 1002/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5150e-04 - val_loss: 4.5231e-04\n",
      "Epoch 1003/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5105e-04 - val_loss: 4.5204e-04\n",
      "Epoch 1004/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4902e-04 - val_loss: 4.5210e-04\n",
      "Epoch 1005/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4982e-04 - val_loss: 4.5205e-04\n",
      "Epoch 1006/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5270e-04 - val_loss: 4.5232e-04\n",
      "Epoch 1007/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5167e-04 - val_loss: 4.5197e-04\n",
      "Epoch 1008/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5102e-04 - val_loss: 4.5191e-04\n",
      "Epoch 1009/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.5162e-04 - val_loss: 4.5191e-04\n",
      "Epoch 1010/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5346e-04 - val_loss: 4.5191e-04\n",
      "Epoch 1011/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5200e-04 - val_loss: 4.5184e-04\n",
      "Epoch 1012/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5085e-04 - val_loss: 4.5174e-04\n",
      "Epoch 1013/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5113e-04 - val_loss: 4.5179e-04\n",
      "Epoch 1014/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5213e-04 - val_loss: 4.5176e-04\n",
      "Epoch 1015/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5223e-04 - val_loss: 4.5152e-04\n",
      "Epoch 1016/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5102e-04 - val_loss: 4.5197e-04\n",
      "Epoch 1017/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5089e-04 - val_loss: 4.5159e-04\n",
      "Epoch 1018/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5082e-04 - val_loss: 4.5174e-04\n",
      "Epoch 1019/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5023e-04 - val_loss: 4.5202e-04\n",
      "Epoch 1020/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5303e-04 - val_loss: 4.5149e-04\n",
      "Epoch 1021/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4811e-04 - val_loss: 4.5131e-04\n",
      "Epoch 1022/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4971e-04 - val_loss: 4.5142e-04\n",
      "Epoch 1023/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4946e-04 - val_loss: 4.5192e-04\n",
      "Epoch 1024/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5159e-04 - val_loss: 4.5153e-04\n",
      "Epoch 1025/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4894e-04 - val_loss: 4.5172e-04\n",
      "Epoch 1026/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4959e-04 - val_loss: 4.5115e-04\n",
      "Epoch 1027/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4988e-04 - val_loss: 4.5107e-04\n",
      "Epoch 1028/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5125e-04 - val_loss: 4.5132e-04\n",
      "Epoch 1029/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5219e-04 - val_loss: 4.5110e-04\n",
      "Epoch 1030/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4895e-04 - val_loss: 4.5111e-04\n",
      "Epoch 1031/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4893e-04 - val_loss: 4.5107e-04\n",
      "Epoch 1032/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5057e-04 - val_loss: 4.5227e-04\n",
      "Epoch 1033/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5009e-04 - val_loss: 4.5117e-04\n",
      "Epoch 1034/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4921e-04 - val_loss: 4.5075e-04\n",
      "Epoch 1035/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5013e-04 - val_loss: 4.5094e-04\n",
      "Epoch 1036/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4842e-04 - val_loss: 4.5148e-04\n",
      "Epoch 1037/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4955e-04 - val_loss: 4.5234e-04\n",
      "Epoch 1038/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5121e-04 - val_loss: 4.5084e-04\n",
      "Epoch 1039/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4823e-04 - val_loss: 4.5065e-04\n",
      "Epoch 1040/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5080e-04 - val_loss: 4.5085e-04\n",
      "Epoch 1041/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4901e-04 - val_loss: 4.5094e-04\n",
      "Epoch 1042/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5049e-04 - val_loss: 4.5108e-04\n",
      "Epoch 1043/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4911e-04 - val_loss: 4.5039e-04\n",
      "Epoch 1044/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4921e-04 - val_loss: 4.5074e-04\n",
      "Epoch 1045/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4827e-04 - val_loss: 4.5033e-04\n",
      "Epoch 1046/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5024e-04 - val_loss: 4.5091e-04\n",
      "Epoch 1047/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4884e-04 - val_loss: 4.5033e-04\n",
      "Epoch 1048/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4770e-04 - val_loss: 4.5095e-04\n",
      "Epoch 1049/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5010e-04 - val_loss: 4.5037e-04\n",
      "Epoch 1050/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4873e-04 - val_loss: 4.5049e-04\n",
      "Epoch 1051/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4828e-04 - val_loss: 4.5059e-04\n",
      "Epoch 1052/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4953e-04 - val_loss: 4.5111e-04\n",
      "Epoch 1053/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4889e-04 - val_loss: 4.5027e-04\n",
      "Epoch 1054/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4823e-04 - val_loss: 4.5004e-04\n",
      "Epoch 1055/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4848e-04 - val_loss: 4.5023e-04\n",
      "Epoch 1056/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4948e-04 - val_loss: 4.5037e-04\n",
      "Epoch 1057/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4906e-04 - val_loss: 4.4993e-04\n",
      "Epoch 1058/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4832e-04 - val_loss: 4.5002e-04\n",
      "Epoch 1059/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4813e-04 - val_loss: 4.5060e-04\n",
      "Epoch 1060/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5039e-04 - val_loss: 4.4998e-04\n",
      "Epoch 1061/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4791e-04 - val_loss: 4.4977e-04\n",
      "Epoch 1062/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4795e-04 - val_loss: 4.5002e-04\n",
      "Epoch 1063/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4989e-04 - val_loss: 4.5002e-04\n",
      "Epoch 1064/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5020e-04 - val_loss: 4.5006e-04\n",
      "Epoch 1065/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5060e-04 - val_loss: 4.5055e-04\n",
      "Epoch 1066/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4825e-04 - val_loss: 4.4995e-04\n",
      "Epoch 1067/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4796e-04 - val_loss: 4.4969e-04\n",
      "Epoch 1068/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4736e-04 - val_loss: 4.4957e-04\n",
      "Epoch 1069/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4858e-04 - val_loss: 4.4968e-04\n",
      "Epoch 1070/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4886e-04 - val_loss: 4.4953e-04\n",
      "Epoch 1071/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4796e-04 - val_loss: 4.4956e-04\n",
      "Epoch 1072/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4791e-04 - val_loss: 4.4950e-04\n",
      "Epoch 1073/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4731e-04 - val_loss: 4.4946e-04\n",
      "Epoch 1074/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4685e-04 - val_loss: 4.4958e-04\n",
      "Epoch 1075/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4727e-04 - val_loss: 4.4918e-04\n",
      "Epoch 1076/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4857e-04 - val_loss: 4.4931e-04\n",
      "Epoch 1077/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4811e-04 - val_loss: 4.4965e-04\n",
      "Epoch 1078/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4650e-04 - val_loss: 4.4945e-04\n",
      "Epoch 1079/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4814e-04 - val_loss: 4.4967e-04\n",
      "Epoch 1080/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4808e-04 - val_loss: 4.4911e-04\n",
      "Epoch 1081/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4755e-04 - val_loss: 4.5007e-04\n",
      "Epoch 1082/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4808e-04 - val_loss: 4.4919e-04\n",
      "Epoch 1083/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4775e-04 - val_loss: 4.4907e-04\n",
      "Epoch 1084/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4850e-04 - val_loss: 4.4897e-04\n",
      "Epoch 1085/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4662e-04 - val_loss: 4.4905e-04\n",
      "Epoch 1086/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4652e-04 - val_loss: 4.4932e-04\n",
      "Epoch 1087/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4386e-04 - val_loss: 4.4890e-04\n",
      "Epoch 1088/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4679e-04 - val_loss: 4.4898e-04\n",
      "Epoch 1089/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4860e-04 - val_loss: 4.4871e-04\n",
      "Epoch 1090/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4784e-04 - val_loss: 4.4890e-04\n",
      "Epoch 1091/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4677e-04 - val_loss: 4.4877e-04\n",
      "Epoch 1092/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4513e-04 - val_loss: 4.4881e-04\n",
      "Epoch 1093/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4591e-04 - val_loss: 4.4901e-04\n",
      "Epoch 1094/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4867e-04 - val_loss: 4.4855e-04\n",
      "Epoch 1095/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4915e-04 - val_loss: 4.4952e-04\n",
      "Epoch 1096/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4644e-04 - val_loss: 4.4860e-04\n",
      "Epoch 1097/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4765e-04 - val_loss: 4.4844e-04\n",
      "Epoch 1098/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4822e-04 - val_loss: 4.4855e-04\n",
      "Epoch 1099/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4611e-04 - val_loss: 4.4955e-04\n",
      "Epoch 1100/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4764e-04 - val_loss: 4.4833e-04\n",
      "Epoch 1101/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4682e-04 - val_loss: 4.4885e-04\n",
      "Epoch 1102/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4602e-04 - val_loss: 4.4836e-04\n",
      "Epoch 1103/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4784e-04 - val_loss: 4.4815e-04\n",
      "Epoch 1104/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4883e-04 - val_loss: 4.4836e-04\n",
      "Epoch 1105/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4508e-04 - val_loss: 4.4851e-04\n",
      "Epoch 1106/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4716e-04 - val_loss: 4.4825e-04\n",
      "Epoch 1107/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4811e-04 - val_loss: 4.4823e-04\n",
      "Epoch 1108/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4618e-04 - val_loss: 4.4850e-04\n",
      "Epoch 1109/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4782e-04 - val_loss: 4.4828e-04\n",
      "Epoch 1110/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4682e-04 - val_loss: 4.4807e-04\n",
      "Epoch 1111/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4679e-04 - val_loss: 4.4803e-04\n",
      "Epoch 1112/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4629e-04 - val_loss: 4.4797e-04\n",
      "Epoch 1113/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4496e-04 - val_loss: 4.4789e-04\n",
      "Epoch 1114/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4689e-04 - val_loss: 4.4780e-04\n",
      "Epoch 1115/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4611e-04 - val_loss: 4.4767e-04\n",
      "Epoch 1116/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.5017e-04 - val_loss: 4.4816e-04\n",
      "Epoch 1117/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4626e-04 - val_loss: 4.4822e-04\n",
      "Epoch 1118/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4511e-04 - val_loss: 4.4764e-04\n",
      "Epoch 1119/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4487e-04 - val_loss: 4.4760e-04\n",
      "Epoch 1120/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4650e-04 - val_loss: 4.4796e-04\n",
      "Epoch 1121/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4503e-04 - val_loss: 4.4758e-04\n",
      "Epoch 1122/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4524e-04 - val_loss: 4.4769e-04\n",
      "Epoch 1123/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4582e-04 - val_loss: 4.4732e-04\n",
      "Epoch 1124/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4477e-04 - val_loss: 4.4772e-04\n",
      "Epoch 1125/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4611e-04 - val_loss: 4.4741e-04\n",
      "Epoch 1126/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4631e-04 - val_loss: 4.4796e-04\n",
      "Epoch 1127/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4403e-04 - val_loss: 4.4726e-04\n",
      "Epoch 1128/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4528e-04 - val_loss: 4.4782e-04\n",
      "Epoch 1129/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4656e-04 - val_loss: 4.4718e-04\n",
      "Epoch 1130/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4678e-04 - val_loss: 4.4729e-04\n",
      "Epoch 1131/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4534e-04 - val_loss: 4.4789e-04\n",
      "Epoch 1132/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4516e-04 - val_loss: 4.4729e-04\n",
      "Epoch 1133/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4642e-04 - val_loss: 4.4702e-04\n",
      "Epoch 1134/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4509e-04 - val_loss: 4.4765e-04\n",
      "Epoch 1135/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4593e-04 - val_loss: 4.4725e-04\n",
      "Epoch 1136/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4487e-04 - val_loss: 4.4695e-04\n",
      "Epoch 1137/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4402e-04 - val_loss: 4.4875e-04\n",
      "Epoch 1138/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4360e-04 - val_loss: 4.4699e-04\n",
      "Epoch 1139/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4619e-04 - val_loss: 4.4692e-04\n",
      "Epoch 1140/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4511e-04 - val_loss: 4.4683e-04\n",
      "Epoch 1141/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.4367e-04 - val_loss: 4.4699e-04\n",
      "Epoch 1142/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.4350e-04 - val_loss: 4.4692e-04\n",
      "Epoch 1143/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4613e-04 - val_loss: 4.4742e-04\n",
      "Epoch 1144/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4478e-04 - val_loss: 4.4677e-04\n",
      "Epoch 1145/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4709e-04 - val_loss: 4.4679e-04\n",
      "Epoch 1146/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4572e-04 - val_loss: 4.4677e-04\n",
      "Epoch 1147/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4395e-04 - val_loss: 4.4653e-04\n",
      "Epoch 1148/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4479e-04 - val_loss: 4.4679e-04\n",
      "Epoch 1149/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4624e-04 - val_loss: 4.4702e-04\n",
      "Epoch 1150/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4458e-04 - val_loss: 4.4687e-04\n",
      "Epoch 1151/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4539e-04 - val_loss: 4.4650e-04\n",
      "Epoch 1152/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4309e-04 - val_loss: 4.4636e-04\n",
      "Epoch 1153/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4338e-04 - val_loss: 4.4635e-04\n",
      "Epoch 1154/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4453e-04 - val_loss: 4.4637e-04\n",
      "Epoch 1155/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4442e-04 - val_loss: 4.4724e-04\n",
      "Epoch 1156/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4524e-04 - val_loss: 4.4611e-04\n",
      "Epoch 1157/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4564e-04 - val_loss: 4.4637e-04\n",
      "Epoch 1158/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4533e-04 - val_loss: 4.4660e-04\n",
      "Epoch 1159/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4469e-04 - val_loss: 4.4667e-04\n",
      "Epoch 1160/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4393e-04 - val_loss: 4.4628e-04\n",
      "Epoch 1161/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4355e-04 - val_loss: 4.4618e-04\n",
      "Epoch 1162/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4378e-04 - val_loss: 4.4755e-04\n",
      "Epoch 1163/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4542e-04 - val_loss: 4.4604e-04\n",
      "Epoch 1164/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4338e-04 - val_loss: 4.4646e-04\n",
      "Epoch 1165/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4215e-04 - val_loss: 4.4778e-04\n",
      "Epoch 1166/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4278e-04 - val_loss: 4.4666e-04\n",
      "Epoch 1167/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4371e-04 - val_loss: 4.4588e-04\n",
      "Epoch 1168/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4481e-04 - val_loss: 4.4579e-04\n",
      "Epoch 1169/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4572e-04 - val_loss: 4.4600e-04\n",
      "Epoch 1170/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4428e-04 - val_loss: 4.4565e-04\n",
      "Epoch 1171/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4470e-04 - val_loss: 4.4576e-04\n",
      "Epoch 1172/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4516e-04 - val_loss: 4.4572e-04\n",
      "Epoch 1173/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4415e-04 - val_loss: 4.4584e-04\n",
      "Epoch 1174/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4390e-04 - val_loss: 4.4618e-04\n",
      "Epoch 1175/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.4400e-04 - val_loss: 4.4574e-04\n",
      "Epoch 1176/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4207e-04 - val_loss: 4.4569e-04\n",
      "Epoch 1177/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4374e-04 - val_loss: 4.4560e-04\n",
      "Epoch 1178/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4366e-04 - val_loss: 4.4624e-04\n",
      "Epoch 1179/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4679e-04 - val_loss: 4.4540e-04\n",
      "Epoch 1180/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4395e-04 - val_loss: 4.4560e-04\n",
      "Epoch 1181/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4392e-04 - val_loss: 4.4548e-04\n",
      "Epoch 1182/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4280e-04 - val_loss: 4.4565e-04\n",
      "Epoch 1183/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4266e-04 - val_loss: 4.4537e-04\n",
      "Epoch 1184/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4263e-04 - val_loss: 4.4516e-04\n",
      "Epoch 1185/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4454e-04 - val_loss: 4.4523e-04\n",
      "Epoch 1186/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4375e-04 - val_loss: 4.4547e-04\n",
      "Epoch 1187/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4233e-04 - val_loss: 4.4560e-04\n",
      "Epoch 1188/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4516e-04 - val_loss: 4.4606e-04\n",
      "Epoch 1189/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4383e-04 - val_loss: 4.4534e-04\n",
      "Epoch 1190/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4261e-04 - val_loss: 4.4546e-04\n",
      "Epoch 1191/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4320e-04 - val_loss: 4.4527e-04\n",
      "Epoch 1192/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4204e-04 - val_loss: 4.4499e-04\n",
      "Epoch 1193/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.4252e-04 - val_loss: 4.4510e-04\n",
      "Epoch 1194/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4235e-04 - val_loss: 4.4526e-04\n",
      "Epoch 1195/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4311e-04 - val_loss: 4.4519e-04\n",
      "Epoch 1196/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4203e-04 - val_loss: 4.4519e-04\n",
      "Epoch 1197/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4344e-04 - val_loss: 4.4484e-04\n",
      "Epoch 1198/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4518e-04 - val_loss: 4.4569e-04\n",
      "Epoch 1199/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4259e-04 - val_loss: 4.4491e-04\n",
      "Epoch 1200/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4248e-04 - val_loss: 4.4486e-04\n",
      "Epoch 1201/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4456e-04 - val_loss: 4.4473e-04\n",
      "Epoch 1202/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4160e-04 - val_loss: 4.4476e-04\n",
      "Epoch 1203/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4359e-04 - val_loss: 4.4470e-04\n",
      "Epoch 1204/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4165e-04 - val_loss: 4.4501e-04\n",
      "Epoch 1205/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4226e-04 - val_loss: 4.4508e-04\n",
      "Epoch 1206/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4122e-04 - val_loss: 4.4467e-04\n",
      "Epoch 1207/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4146e-04 - val_loss: 4.4461e-04\n",
      "Epoch 1208/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4322e-04 - val_loss: 4.4434e-04\n",
      "Epoch 1209/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4055e-04 - val_loss: 4.4464e-04\n",
      "Epoch 1210/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4159e-04 - val_loss: 4.4444e-04\n",
      "Epoch 1211/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4298e-04 - val_loss: 4.4467e-04\n",
      "Epoch 1212/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4257e-04 - val_loss: 4.4438e-04\n",
      "Epoch 1213/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4225e-04 - val_loss: 4.4454e-04\n",
      "Epoch 1214/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4099e-04 - val_loss: 4.4457e-04\n",
      "Epoch 1215/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4145e-04 - val_loss: 4.4464e-04\n",
      "Epoch 1216/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4173e-04 - val_loss: 4.4468e-04\n",
      "Epoch 1217/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4529e-04 - val_loss: 4.4424e-04\n",
      "Epoch 1218/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4137e-04 - val_loss: 4.4428e-04\n",
      "Epoch 1219/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4456e-04 - val_loss: 4.4412e-04\n",
      "Epoch 1220/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4444e-04 - val_loss: 4.4405e-04\n",
      "Epoch 1221/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4053e-04 - val_loss: 4.4448e-04\n",
      "Epoch 1222/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4200e-04 - val_loss: 4.4412e-04\n",
      "Epoch 1223/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4275e-04 - val_loss: 4.4422e-04\n",
      "Epoch 1224/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4085e-04 - val_loss: 4.4395e-04\n",
      "Epoch 1225/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4178e-04 - val_loss: 4.4431e-04\n",
      "Epoch 1226/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4224e-04 - val_loss: 4.4404e-04\n",
      "Epoch 1227/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4331e-04 - val_loss: 4.4391e-04\n",
      "Epoch 1228/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4059e-04 - val_loss: 4.4390e-04\n",
      "Epoch 1229/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4338e-04 - val_loss: 4.4384e-04\n",
      "Epoch 1230/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4241e-04 - val_loss: 4.4367e-04\n",
      "Epoch 1231/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4214e-04 - val_loss: 4.4433e-04\n",
      "Epoch 1232/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4063e-04 - val_loss: 4.4397e-04\n",
      "Epoch 1233/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4281e-04 - val_loss: 4.4387e-04\n",
      "Epoch 1234/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4094e-04 - val_loss: 4.4357e-04\n",
      "Epoch 1235/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4252e-04 - val_loss: 4.4379e-04\n",
      "Epoch 1236/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4045e-04 - val_loss: 4.4403e-04\n",
      "Epoch 1237/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4221e-04 - val_loss: 4.4423e-04\n",
      "Epoch 1238/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4339e-04 - val_loss: 4.4476e-04\n",
      "Epoch 1239/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4016e-04 - val_loss: 4.4354e-04\n",
      "Epoch 1240/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4241e-04 - val_loss: 4.4379e-04\n",
      "Epoch 1241/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4290e-04 - val_loss: 4.4327e-04\n",
      "Epoch 1242/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4122e-04 - val_loss: 4.4343e-04\n",
      "Epoch 1243/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4235e-04 - val_loss: 4.4328e-04\n",
      "Epoch 1244/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4217e-04 - val_loss: 4.4337e-04\n",
      "Epoch 1245/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4087e-04 - val_loss: 4.4329e-04\n",
      "Epoch 1246/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4231e-04 - val_loss: 4.4323e-04\n",
      "Epoch 1247/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4008e-04 - val_loss: 4.4336e-04\n",
      "Epoch 1248/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4106e-04 - val_loss: 4.4325e-04\n",
      "Epoch 1249/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4237e-04 - val_loss: 4.4308e-04\n",
      "Epoch 1250/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4271e-04 - val_loss: 4.4315e-04\n",
      "Epoch 1251/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4183e-04 - val_loss: 4.4377e-04\n",
      "Epoch 1252/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3978e-04 - val_loss: 4.4322e-04\n",
      "Epoch 1253/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4220e-04 - val_loss: 4.4301e-04\n",
      "Epoch 1254/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4156e-04 - val_loss: 4.4352e-04\n",
      "Epoch 1255/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4345e-04 - val_loss: 4.4307e-04\n",
      "Epoch 1256/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4171e-04 - val_loss: 4.4322e-04\n",
      "Epoch 1257/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3799e-04 - val_loss: 4.4273e-04\n",
      "Epoch 1258/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4188e-04 - val_loss: 4.4431e-04\n",
      "Epoch 1259/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4033e-04 - val_loss: 4.4290e-04\n",
      "Epoch 1260/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4260e-04 - val_loss: 4.4332e-04\n",
      "Epoch 1261/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3953e-04 - val_loss: 4.4318e-04\n",
      "Epoch 1262/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4293e-04 - val_loss: 4.4293e-04\n",
      "Epoch 1263/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4230e-04 - val_loss: 4.4311e-04\n",
      "Epoch 1264/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4399e-04 - val_loss: 4.4274e-04\n",
      "Epoch 1265/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3973e-04 - val_loss: 4.4417e-04\n",
      "Epoch 1266/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3981e-04 - val_loss: 4.4295e-04\n",
      "Epoch 1267/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4175e-04 - val_loss: 4.4249e-04\n",
      "Epoch 1268/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3961e-04 - val_loss: 4.4280e-04\n",
      "Epoch 1269/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.4185e-04 - val_loss: 4.4253e-04\n",
      "Epoch 1270/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3991e-04 - val_loss: 4.4248e-04\n",
      "Epoch 1271/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3861e-04 - val_loss: 4.4306e-04\n",
      "Epoch 1272/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4186e-04 - val_loss: 4.4243e-04\n",
      "Epoch 1273/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4072e-04 - val_loss: 4.4228e-04\n",
      "Epoch 1274/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3992e-04 - val_loss: 4.4291e-04\n",
      "Epoch 1275/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3960e-04 - val_loss: 4.4283e-04\n",
      "Epoch 1276/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3978e-04 - val_loss: 4.4218e-04\n",
      "Epoch 1277/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4064e-04 - val_loss: 4.4239e-04\n",
      "Epoch 1278/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4019e-04 - val_loss: 4.4229e-04\n",
      "Epoch 1279/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4207e-04 - val_loss: 4.4202e-04\n",
      "Epoch 1280/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3845e-04 - val_loss: 4.4209e-04\n",
      "Epoch 1281/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4004e-04 - val_loss: 4.4238e-04\n",
      "Epoch 1282/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3974e-04 - val_loss: 4.4210e-04\n",
      "Epoch 1283/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4028e-04 - val_loss: 4.4203e-04\n",
      "Epoch 1284/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4083e-04 - val_loss: 4.4213e-04\n",
      "Epoch 1285/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4128e-04 - val_loss: 4.4236e-04\n",
      "Epoch 1286/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3921e-04 - val_loss: 4.4206e-04\n",
      "Epoch 1287/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4052e-04 - val_loss: 4.4182e-04\n",
      "Epoch 1288/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3916e-04 - val_loss: 4.4205e-04\n",
      "Epoch 1289/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3818e-04 - val_loss: 4.4203e-04\n",
      "Epoch 1290/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4037e-04 - val_loss: 4.4284e-04\n",
      "Epoch 1291/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4091e-04 - val_loss: 4.4238e-04\n",
      "Epoch 1292/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4100e-04 - val_loss: 4.4188e-04\n",
      "Epoch 1293/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3864e-04 - val_loss: 4.4183e-04\n",
      "Epoch 1294/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.4051e-04 - val_loss: 4.4182e-04\n",
      "Epoch 1295/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3875e-04 - val_loss: 4.4162e-04\n",
      "Epoch 1296/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4088e-04 - val_loss: 4.4204e-04\n",
      "Epoch 1297/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3955e-04 - val_loss: 4.4223e-04\n",
      "Epoch 1298/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3891e-04 - val_loss: 4.4136e-04\n",
      "Epoch 1299/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4069e-04 - val_loss: 4.4167e-04\n",
      "Epoch 1300/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4030e-04 - val_loss: 4.4175e-04\n",
      "Epoch 1301/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.3925e-04 - val_loss: 4.4266e-04\n",
      "Epoch 1302/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3984e-04 - val_loss: 4.4133e-04\n",
      "Epoch 1303/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3952e-04 - val_loss: 4.4126e-04\n",
      "Epoch 1304/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4000e-04 - val_loss: 4.4149e-04\n",
      "Epoch 1305/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4001e-04 - val_loss: 4.4215e-04\n",
      "Epoch 1306/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.4055e-04 - val_loss: 4.4136e-04\n",
      "Epoch 1307/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3704e-04 - val_loss: 4.4127e-04\n",
      "Epoch 1308/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4218e-04 - val_loss: 4.4113e-04\n",
      "Epoch 1309/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3713e-04 - val_loss: 4.4130e-04\n",
      "Epoch 1310/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4034e-04 - val_loss: 4.4100e-04\n",
      "Epoch 1311/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3850e-04 - val_loss: 4.4126e-04\n",
      "Epoch 1312/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3839e-04 - val_loss: 4.4097e-04\n",
      "Epoch 1313/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3809e-04 - val_loss: 4.4194e-04\n",
      "Epoch 1314/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3900e-04 - val_loss: 4.4169e-04\n",
      "Epoch 1315/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3987e-04 - val_loss: 4.4125e-04\n",
      "Epoch 1316/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3891e-04 - val_loss: 4.4126e-04\n",
      "Epoch 1317/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3875e-04 - val_loss: 4.4142e-04\n",
      "Epoch 1318/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3844e-04 - val_loss: 4.4083e-04\n",
      "Epoch 1319/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3798e-04 - val_loss: 4.4083e-04\n",
      "Epoch 1320/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3808e-04 - val_loss: 4.4128e-04\n",
      "Epoch 1321/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3620e-04 - val_loss: 4.4095e-04\n",
      "Epoch 1322/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3985e-04 - val_loss: 4.4064e-04\n",
      "Epoch 1323/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3915e-04 - val_loss: 4.4070e-04\n",
      "Epoch 1324/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3962e-04 - val_loss: 4.4165e-04\n",
      "Epoch 1325/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3847e-04 - val_loss: 4.4061e-04\n",
      "Epoch 1326/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3846e-04 - val_loss: 4.4063e-04\n",
      "Epoch 1327/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3882e-04 - val_loss: 4.4052e-04\n",
      "Epoch 1328/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3894e-04 - val_loss: 4.4057e-04\n",
      "Epoch 1329/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3778e-04 - val_loss: 4.4122e-04\n",
      "Epoch 1330/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4079e-04 - val_loss: 4.4052e-04\n",
      "Epoch 1331/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3983e-04 - val_loss: 4.4102e-04\n",
      "Epoch 1332/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3687e-04 - val_loss: 4.4089e-04\n",
      "Epoch 1333/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3843e-04 - val_loss: 4.4081e-04\n",
      "Epoch 1334/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3775e-04 - val_loss: 4.4047e-04\n",
      "Epoch 1335/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3805e-04 - val_loss: 4.4046e-04\n",
      "Epoch 1336/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3758e-04 - val_loss: 4.4080e-04\n",
      "Epoch 1337/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3867e-04 - val_loss: 4.4031e-04\n",
      "Epoch 1338/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3655e-04 - val_loss: 4.4025e-04\n",
      "Epoch 1339/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3813e-04 - val_loss: 4.4025e-04\n",
      "Epoch 1340/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3846e-04 - val_loss: 4.4014e-04\n",
      "Epoch 1341/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3998e-04 - val_loss: 4.4106e-04\n",
      "Epoch 1342/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3768e-04 - val_loss: 4.4054e-04\n",
      "Epoch 1343/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3940e-04 - val_loss: 4.4046e-04\n",
      "Epoch 1344/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3715e-04 - val_loss: 4.4031e-04\n",
      "Epoch 1345/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3860e-04 - val_loss: 4.4002e-04\n",
      "Epoch 1346/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3911e-04 - val_loss: 4.4011e-04\n",
      "Epoch 1347/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3987e-04 - val_loss: 4.4044e-04\n",
      "Epoch 1348/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3852e-04 - val_loss: 4.3992e-04\n",
      "Epoch 1349/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3791e-04 - val_loss: 4.4015e-04\n",
      "Epoch 1350/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.4045e-04 - val_loss: 4.4130e-04\n",
      "Epoch 1351/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3760e-04 - val_loss: 4.3991e-04\n",
      "Epoch 1352/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3608e-04 - val_loss: 4.3978e-04\n",
      "Epoch 1353/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3796e-04 - val_loss: 4.3998e-04\n",
      "Epoch 1354/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3653e-04 - val_loss: 4.3974e-04\n",
      "Epoch 1355/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3937e-04 - val_loss: 4.3999e-04\n",
      "Epoch 1356/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3742e-04 - val_loss: 4.4041e-04\n",
      "Epoch 1357/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3871e-04 - val_loss: 4.3967e-04\n",
      "Epoch 1358/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3797e-04 - val_loss: 4.3982e-04\n",
      "Epoch 1359/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3622e-04 - val_loss: 4.4015e-04\n",
      "Epoch 1360/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3794e-04 - val_loss: 4.3946e-04\n",
      "Epoch 1361/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3597e-04 - val_loss: 4.3970e-04\n",
      "Epoch 1362/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3810e-04 - val_loss: 4.3957e-04\n",
      "Epoch 1363/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3743e-04 - val_loss: 4.3979e-04\n",
      "Epoch 1364/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3669e-04 - val_loss: 4.3971e-04\n",
      "Epoch 1365/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3655e-04 - val_loss: 4.3948e-04\n",
      "Epoch 1366/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3553e-04 - val_loss: 4.3951e-04\n",
      "Epoch 1367/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3862e-04 - val_loss: 4.3952e-04\n",
      "Epoch 1368/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3621e-04 - val_loss: 4.3925e-04\n",
      "Epoch 1369/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3553e-04 - val_loss: 4.3962e-04\n",
      "Epoch 1370/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3593e-04 - val_loss: 4.3920e-04\n",
      "Epoch 1371/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3759e-04 - val_loss: 4.3945e-04\n",
      "Epoch 1372/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3778e-04 - val_loss: 4.3921e-04\n",
      "Epoch 1373/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3723e-04 - val_loss: 4.3922e-04\n",
      "Epoch 1374/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3495e-04 - val_loss: 4.3928e-04\n",
      "Epoch 1375/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3722e-04 - val_loss: 4.3925e-04\n",
      "Epoch 1376/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3816e-04 - val_loss: 4.3915e-04\n",
      "Epoch 1377/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3558e-04 - val_loss: 4.3903e-04\n",
      "Epoch 1378/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3573e-04 - val_loss: 4.3909e-04\n",
      "Epoch 1379/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3517e-04 - val_loss: 4.3911e-04\n",
      "Epoch 1380/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3802e-04 - val_loss: 4.3966e-04\n",
      "Epoch 1381/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3544e-04 - val_loss: 4.3889e-04\n",
      "Epoch 1382/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3603e-04 - val_loss: 4.3876e-04\n",
      "Epoch 1383/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3662e-04 - val_loss: 4.3930e-04\n",
      "Epoch 1384/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3687e-04 - val_loss: 4.3876e-04\n",
      "Epoch 1385/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3444e-04 - val_loss: 4.3885e-04\n",
      "Epoch 1386/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3823e-04 - val_loss: 4.3869e-04\n",
      "Epoch 1387/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3580e-04 - val_loss: 4.3870e-04\n",
      "Epoch 1388/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3622e-04 - val_loss: 4.3888e-04\n",
      "Epoch 1389/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3690e-04 - val_loss: 4.3870e-04\n",
      "Epoch 1390/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3693e-04 - val_loss: 4.3866e-04\n",
      "Epoch 1391/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3470e-04 - val_loss: 4.3908e-04\n",
      "Epoch 1392/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3614e-04 - val_loss: 4.3882e-04\n",
      "Epoch 1393/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3593e-04 - val_loss: 4.3894e-04\n",
      "Epoch 1394/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3578e-04 - val_loss: 4.3854e-04\n",
      "Epoch 1395/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3660e-04 - val_loss: 4.3883e-04\n",
      "Epoch 1396/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3548e-04 - val_loss: 4.3868e-04\n",
      "Epoch 1397/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3609e-04 - val_loss: 4.3861e-04\n",
      "Epoch 1398/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3473e-04 - val_loss: 4.3876e-04\n",
      "Epoch 1399/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3586e-04 - val_loss: 4.3914e-04\n",
      "Epoch 1400/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3483e-04 - val_loss: 4.3884e-04\n",
      "Epoch 1401/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3717e-04 - val_loss: 4.3846e-04\n",
      "Epoch 1402/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3667e-04 - val_loss: 4.3872e-04\n",
      "Epoch 1403/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3661e-04 - val_loss: 4.3875e-04\n",
      "Epoch 1404/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3715e-04 - val_loss: 4.3832e-04\n",
      "Epoch 1405/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3496e-04 - val_loss: 4.3834e-04\n",
      "Epoch 1406/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3453e-04 - val_loss: 4.3826e-04\n",
      "Epoch 1407/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3689e-04 - val_loss: 4.3843e-04\n",
      "Epoch 1408/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3766e-04 - val_loss: 4.3817e-04\n",
      "Epoch 1409/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3492e-04 - val_loss: 4.3844e-04\n",
      "Epoch 1410/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3649e-04 - val_loss: 4.3814e-04\n",
      "Epoch 1411/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3584e-04 - val_loss: 4.3837e-04\n",
      "Epoch 1412/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.3659e-04 - val_loss: 4.3798e-04\n",
      "Epoch 1413/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3731e-04 - val_loss: 4.3843e-04\n",
      "Epoch 1414/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3686e-04 - val_loss: 4.3895e-04\n",
      "Epoch 1415/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3599e-04 - val_loss: 4.3806e-04\n",
      "Epoch 1416/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3696e-04 - val_loss: 4.3869e-04\n",
      "Epoch 1417/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3589e-04 - val_loss: 4.3796e-04\n",
      "Epoch 1418/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3588e-04 - val_loss: 4.3810e-04\n",
      "Epoch 1419/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3552e-04 - val_loss: 4.3785e-04\n",
      "Epoch 1420/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3462e-04 - val_loss: 4.3786e-04\n",
      "Epoch 1421/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3694e-04 - val_loss: 4.3757e-04\n",
      "Epoch 1422/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3535e-04 - val_loss: 4.3813e-04\n",
      "Epoch 1423/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3583e-04 - val_loss: 4.3780e-04\n",
      "Epoch 1424/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3564e-04 - val_loss: 4.3776e-04\n",
      "Epoch 1425/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3708e-04 - val_loss: 4.3801e-04\n",
      "Epoch 1426/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3560e-04 - val_loss: 4.3763e-04\n",
      "Epoch 1427/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3530e-04 - val_loss: 4.3752e-04\n",
      "Epoch 1428/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3480e-04 - val_loss: 4.3758e-04\n",
      "Epoch 1429/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3541e-04 - val_loss: 4.3770e-04\n",
      "Epoch 1430/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3495e-04 - val_loss: 4.3777e-04\n",
      "Epoch 1431/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3524e-04 - val_loss: 4.3746e-04\n",
      "Epoch 1432/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3255e-04 - val_loss: 4.3856e-04\n",
      "Epoch 1433/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3563e-04 - val_loss: 4.3794e-04\n",
      "Epoch 1434/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3429e-04 - val_loss: 4.3770e-04\n",
      "Epoch 1435/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3605e-04 - val_loss: 4.3764e-04\n",
      "Epoch 1436/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3600e-04 - val_loss: 4.3737e-04\n",
      "Epoch 1437/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3393e-04 - val_loss: 4.3750e-04\n",
      "Epoch 1438/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3535e-04 - val_loss: 4.3751e-04\n",
      "Epoch 1439/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3576e-04 - val_loss: 4.3725e-04\n",
      "Epoch 1440/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3547e-04 - val_loss: 4.3704e-04\n",
      "Epoch 1441/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3576e-04 - val_loss: 4.3752e-04\n",
      "Epoch 1442/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3437e-04 - val_loss: 4.3775e-04\n",
      "Epoch 1443/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3277e-04 - val_loss: 4.3724e-04\n",
      "Epoch 1444/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3559e-04 - val_loss: 4.3739e-04\n",
      "Epoch 1445/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3444e-04 - val_loss: 4.3716e-04\n",
      "Epoch 1446/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3568e-04 - val_loss: 4.3692e-04\n",
      "Epoch 1447/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3382e-04 - val_loss: 4.3698e-04\n",
      "Epoch 1448/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3505e-04 - val_loss: 4.3814e-04\n",
      "Epoch 1449/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3399e-04 - val_loss: 4.3707e-04\n",
      "Epoch 1450/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3588e-04 - val_loss: 4.3693e-04\n",
      "Epoch 1451/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3576e-04 - val_loss: 4.3720e-04\n",
      "Epoch 1452/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3468e-04 - val_loss: 4.3683e-04\n",
      "Epoch 1453/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3583e-04 - val_loss: 4.3670e-04\n",
      "Epoch 1454/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3526e-04 - val_loss: 4.3694e-04\n",
      "Epoch 1455/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3433e-04 - val_loss: 4.3693e-04\n",
      "Epoch 1456/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3374e-04 - val_loss: 4.3704e-04\n",
      "Epoch 1457/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3385e-04 - val_loss: 4.3674e-04\n",
      "Epoch 1458/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3500e-04 - val_loss: 4.3668e-04\n",
      "Epoch 1459/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3407e-04 - val_loss: 4.3704e-04\n",
      "Epoch 1460/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3510e-04 - val_loss: 4.3668e-04\n",
      "Epoch 1461/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3460e-04 - val_loss: 4.3650e-04\n",
      "Epoch 1462/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3217e-04 - val_loss: 4.3716e-04\n",
      "Epoch 1463/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3555e-04 - val_loss: 4.3684e-04\n",
      "Epoch 1464/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.3449e-04 - val_loss: 4.3653e-04\n",
      "Epoch 1465/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3389e-04 - val_loss: 4.3749e-04\n",
      "Epoch 1466/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3446e-04 - val_loss: 4.3642e-04\n",
      "Epoch 1467/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3476e-04 - val_loss: 4.3654e-04\n",
      "Epoch 1468/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3540e-04 - val_loss: 4.3630e-04\n",
      "Epoch 1469/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3501e-04 - val_loss: 4.3648e-04\n",
      "Epoch 1470/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3290e-04 - val_loss: 4.3659e-04\n",
      "Epoch 1471/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3380e-04 - val_loss: 4.3622e-04\n",
      "Epoch 1472/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3531e-04 - val_loss: 4.3716e-04\n",
      "Epoch 1473/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3462e-04 - val_loss: 4.3633e-04\n",
      "Epoch 1474/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3550e-04 - val_loss: 4.3628e-04\n",
      "Epoch 1475/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3472e-04 - val_loss: 4.3613e-04\n",
      "Epoch 1476/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3201e-04 - val_loss: 4.3623e-04\n",
      "Epoch 1477/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3264e-04 - val_loss: 4.3631e-04\n",
      "Epoch 1478/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3427e-04 - val_loss: 4.3644e-04\n",
      "Epoch 1479/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3445e-04 - val_loss: 4.3670e-04\n",
      "Epoch 1480/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3451e-04 - val_loss: 4.3620e-04\n",
      "Epoch 1481/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3410e-04 - val_loss: 4.3613e-04\n",
      "Epoch 1482/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3493e-04 - val_loss: 4.3615e-04\n",
      "Epoch 1483/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3458e-04 - val_loss: 4.3632e-04\n",
      "Epoch 1484/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3341e-04 - val_loss: 4.3605e-04\n",
      "Epoch 1485/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3426e-04 - val_loss: 4.3662e-04\n",
      "Epoch 1486/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3321e-04 - val_loss: 4.3605e-04\n",
      "Epoch 1487/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3436e-04 - val_loss: 4.3598e-04\n",
      "Epoch 1488/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3250e-04 - val_loss: 4.3595e-04\n",
      "Epoch 1489/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3415e-04 - val_loss: 4.3585e-04\n",
      "Epoch 1490/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3531e-04 - val_loss: 4.3562e-04\n",
      "Epoch 1491/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3372e-04 - val_loss: 4.3605e-04\n",
      "Epoch 1492/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3389e-04 - val_loss: 4.3571e-04\n",
      "Epoch 1493/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3203e-04 - val_loss: 4.3582e-04\n",
      "Epoch 1494/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3269e-04 - val_loss: 4.3592e-04\n",
      "Epoch 1495/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3374e-04 - val_loss: 4.3563e-04\n",
      "Epoch 1496/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3139e-04 - val_loss: 4.3548e-04\n",
      "Epoch 1497/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.3377e-04 - val_loss: 4.3557e-04\n",
      "Epoch 1498/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3363e-04 - val_loss: 4.3537e-04\n",
      "Epoch 1499/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3152e-04 - val_loss: 4.3558e-04\n",
      "Epoch 1500/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3289e-04 - val_loss: 4.3629e-04\n",
      "Epoch 1501/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3228e-04 - val_loss: 4.3575e-04\n",
      "Epoch 1502/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3425e-04 - val_loss: 4.3547e-04\n",
      "Epoch 1503/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.3303e-04 - val_loss: 4.3560e-04\n",
      "Epoch 1504/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.2968e-04 - val_loss: 4.3550e-04\n",
      "Epoch 1505/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3400e-04 - val_loss: 4.3539e-04\n",
      "Epoch 1506/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3339e-04 - val_loss: 4.3522e-04\n",
      "Epoch 1507/10000\n",
      "95/95 [==============================] - 95s 1000ms/step - loss: 4.3371e-04 - val_loss: 4.3617e-04\n",
      "Epoch 1508/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3386e-04 - val_loss: 4.3551e-04\n",
      "Epoch 1509/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3123e-04 - val_loss: 4.3560e-04\n",
      "Epoch 1510/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3121e-04 - val_loss: 4.3531e-04\n",
      "Epoch 1511/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3062e-04 - val_loss: 4.3521e-04\n",
      "Epoch 1512/10000\n",
      "95/95 [==============================] - 95s 998ms/step - loss: 4.3342e-04 - val_loss: 4.3542e-04\n",
      "Epoch 1513/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3303e-04 - val_loss: 4.3713e-04\n",
      "Epoch 1514/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3305e-04 - val_loss: 4.3516e-04\n",
      "Epoch 1515/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3188e-04 - val_loss: 4.3544e-04\n",
      "Epoch 1516/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3185e-04 - val_loss: 4.3501e-04\n",
      "Epoch 1517/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.3264e-04 - val_loss: 4.3488e-04\n",
      "Epoch 1518/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3200e-04 - val_loss: 4.3658e-04\n",
      "Epoch 1519/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.3431e-04 - val_loss: 4.3508e-04\n",
      "Epoch 1520/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3127e-04 - val_loss: 4.3576e-04\n",
      "Epoch 1521/10000\n",
      "95/95 [==============================] - 97s 1s/step - loss: 4.3157e-04 - val_loss: 4.3524e-04\n",
      "Epoch 1522/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.3131e-04 - val_loss: 4.3485e-04\n",
      "Epoch 1523/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.3441e-04 - val_loss: 4.3555e-04\n",
      "Epoch 1524/10000\n",
      "95/95 [==============================] - 115s 1s/step - loss: 4.3334e-04 - val_loss: 4.3527e-04\n",
      "Epoch 1525/10000\n",
      "95/95 [==============================] - 124s 1s/step - loss: 4.3420e-04 - val_loss: 4.3482e-04\n",
      "Epoch 1526/10000\n",
      "95/95 [==============================] - 129s 1s/step - loss: 4.3227e-04 - val_loss: 4.3461e-04\n",
      "Epoch 1527/10000\n",
      "95/95 [==============================] - 129s 1s/step - loss: 4.3164e-04 - val_loss: 4.3472e-04\n",
      "Epoch 1528/10000\n",
      "95/95 [==============================] - 129s 1s/step - loss: 4.3186e-04 - val_loss: 4.3480e-04\n",
      "Epoch 1529/10000\n",
      "95/95 [==============================] - 130s 1s/step - loss: 4.3306e-04 - val_loss: 4.3479e-04\n",
      "Epoch 1530/10000\n",
      "95/95 [==============================] - 129s 1s/step - loss: 4.3296e-04 - val_loss: 4.3524e-04\n",
      "Epoch 1531/10000\n",
      "95/95 [==============================] - 128s 1s/step - loss: 4.3268e-04 - val_loss: 4.3456e-04\n",
      "Epoch 1532/10000\n",
      "95/95 [==============================] - 109s 1s/step - loss: 4.3192e-04 - val_loss: 4.3508e-04\n",
      "Epoch 1533/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3216e-04 - val_loss: 4.3549e-04\n",
      "Epoch 1534/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3325e-04 - val_loss: 4.3492e-04\n",
      "Epoch 1535/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3249e-04 - val_loss: 4.3450e-04\n",
      "Epoch 1536/10000\n",
      "95/95 [==============================] - 124s 1s/step - loss: 4.3114e-04 - val_loss: 4.3461e-04\n",
      "Epoch 1537/10000\n",
      "95/95 [==============================] - 129s 1s/step - loss: 4.3160e-04 - val_loss: 4.3439e-04\n",
      "Epoch 1538/10000\n",
      "95/95 [==============================] - 128s 1s/step - loss: 4.3261e-04 - val_loss: 4.3449e-04\n",
      "Epoch 1539/10000\n",
      "95/95 [==============================] - 126s 1s/step - loss: 4.3092e-04 - val_loss: 4.3489e-04\n",
      "Epoch 1540/10000\n",
      "95/95 [==============================] - 127s 1s/step - loss: 4.3246e-04 - val_loss: 4.3436e-04\n",
      "Epoch 1541/10000\n",
      "95/95 [==============================] - 126s 1s/step - loss: 4.3244e-04 - val_loss: 4.3438e-04\n",
      "Epoch 1542/10000\n",
      "95/95 [==============================] - 127s 1s/step - loss: 4.3147e-04 - val_loss: 4.3524e-04\n",
      "Epoch 1543/10000\n",
      "95/95 [==============================] - 128s 1s/step - loss: 4.3051e-04 - val_loss: 4.3448e-04\n",
      "Epoch 1544/10000\n",
      "95/95 [==============================] - 127s 1s/step - loss: 4.3299e-04 - val_loss: 4.3516e-04\n",
      "Epoch 1545/10000\n",
      "95/95 [==============================] - 129s 1s/step - loss: 4.3185e-04 - val_loss: 4.3434e-04\n",
      "Epoch 1546/10000\n",
      "95/95 [==============================] - 128s 1s/step - loss: 4.3061e-04 - val_loss: 4.3408e-04\n",
      "Epoch 1547/10000\n",
      "95/95 [==============================] - 128s 1s/step - loss: 4.3029e-04 - val_loss: 4.3415e-04\n",
      "Epoch 1548/10000\n",
      "95/95 [==============================] - 127s 1s/step - loss: 4.3281e-04 - val_loss: 4.3406e-04\n",
      "Epoch 1549/10000\n",
      "95/95 [==============================] - 125s 1s/step - loss: 4.3182e-04 - val_loss: 4.3408e-04\n",
      "Epoch 1550/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3084e-04 - val_loss: 4.3471e-04\n",
      "Epoch 1551/10000\n",
      "95/95 [==============================] - 95s 999ms/step - loss: 4.3099e-04 - val_loss: 4.3417e-04\n",
      "Epoch 1552/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.3126e-04 - val_loss: 4.3509e-04\n",
      "Epoch 1553/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.3202e-04 - val_loss: 4.3397e-04\n",
      "Epoch 1554/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.3003e-04 - val_loss: 4.3389e-04\n",
      "Epoch 1555/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3144e-04 - val_loss: 4.3410e-04\n",
      "Epoch 1556/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3153e-04 - val_loss: 4.3387e-04\n",
      "Epoch 1557/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3213e-04 - val_loss: 4.3397e-04\n",
      "Epoch 1558/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3114e-04 - val_loss: 4.3422e-04\n",
      "Epoch 1559/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3062e-04 - val_loss: 4.3393e-04\n",
      "Epoch 1560/10000\n",
      "95/95 [==============================] - 95s 1s/step - loss: 4.3204e-04 - val_loss: 4.3434e-04\n",
      "Epoch 1561/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.3089e-04 - val_loss: 4.3388e-04\n",
      "Epoch 1562/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.3213e-04 - val_loss: 4.3381e-04\n",
      "Epoch 1563/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.3211e-04 - val_loss: 4.3380e-04\n",
      "Epoch 1564/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.3178e-04 - val_loss: 4.3372e-04\n",
      "Epoch 1565/10000\n",
      "95/95 [==============================] - 96s 1s/step - loss: 4.3035e-04 - val_loss: 4.3409e-04\n",
      "Epoch 1566/10000\n",
      "28/95 [=======>......................] - ETA: 1:05 - loss: 4.3365e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mSNM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTr_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrMask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTr_Y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mVal_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mValMask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVal_Y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1100\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1095\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1096\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1097\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1098\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1099\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1100\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1102\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name) \u001b[38;5;28;01mas\u001b[39;00m tm:\n\u001b[1;32m--> 828\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    829\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m   new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:855\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    852\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    853\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 855\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    857\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    858\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2940\u001b[0m   (graph_function,\n\u001b[0;32m   2941\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1914\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1916\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1917\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1918\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1920\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m     args,\n\u001b[0;32m   1922\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1923\u001b[0m     executing_eagerly)\n\u001b[0;32m   1924\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 555\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    562\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    564\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    567\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    568\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SNM.fit([Tr_X, TrMask], tf.convert_to_tensor(Tr_Y), validation_data=([Val_X, ValMask], tf.convert_to_tensor(Val_Y)), batch_size=3000, verbose=1, shuffle=True, epochs=10000, callbacks=[model_checkpoint_callback] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaba795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNM.load_weights('./results/model_rightedge_mask_no_vital_val.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31eb1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SNM.fit([Tr_X, TrMask], tf.convert_to_tensor(Tr_Y), validation_data=([Val_X, ValMask], tf.convert_to_tensor(Val_Y)), batch_size=3000, verbose=1, shuffle=True, epochs=10000, callbacks=[model_checkpoint_callback] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71bf61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727a6c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e9d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ee1cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SNM.fit([Tr_X, TrMask], tf.convert_to_tensor(Tr_Y), validation_data=([Val_X, ValMask], tf.convert_to_tensor(Val_Y)), batch_size=3000, verbose=1, shuffle=True, epochs=10000, callbacks=[model_checkpoint_callback] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad8a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ab82c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
