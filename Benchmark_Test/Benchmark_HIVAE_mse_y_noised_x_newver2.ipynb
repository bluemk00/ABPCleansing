{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38dfd7ac",
   "metadata": {},
   "source": [
    "#### 원저자의 cost 함수를 수정함 maximize p(x|z) 대신 minimize MSE 사용: lib.models_varia\n",
    "#### 원저자의 cost 함수는 lib.models\n",
    "\n",
    "#####  missing values 로 mask 되어있는 hat_Values만 선택되어 mse 및 map 가 적용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65af1ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\ai\\lib\\site-packages\\scipy\\__init__.py:173: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.19.2)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "WARNING:tensorflow:From C:\\Users\\USER\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\keras\\mixed_precision\\loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from absl import app\n",
    "from absl import flags\n",
    "sys.path.append(\"..\")\n",
    "from lib.models_varia_mse_y import *\n",
    "\n",
    "# Enable mixed precision for performance\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "\n",
    "## GPU selection\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "outdir = './Results/HIVAE_newver2/'\n",
    "# DATA = np.load(\"../../2.ProcessedData/mimic_30s_sample.npy\")\n",
    "checkpoint_prefix = os.path.join(outdir, \"ckpt\")\n",
    "\n",
    "data_type = 'hmnist'\n",
    "testing = False\n",
    "num_steps = 0 # 'Number of training steps: If non-zero it overwrites num_epochs'\n",
    "num_epochs = 10000\n",
    "batch_size = 1500\n",
    "print_interval = 0\n",
    "TrRate = 0.8\n",
    "LatDim = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd9c0d6",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "360bd20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(283380, 3000) (70844, 3000)\n"
     ]
    }
   ],
   "source": [
    "Data = np.load(\"D:/Dropbox/AICleansing_ver2/2.ProcessedData/train_mimic_orgscale.npy\")\n",
    "# ValData = np.load(\"D:/Dropbox/AICleansing_ver2/2.ProcessedData/valid_vitaldb_orgscale.npy\")\n",
    "data_len = len(Data)\n",
    "np.random.shuffle(Data)\n",
    "TrData = Data[data_len//5:]\n",
    "ValData = Data[:data_len//5]\n",
    "print(TrData.shape, ValData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c9a21e",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74e7a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ver2\n",
    "\n",
    "TrDataFrame = tf.signal.frame(TrData.astype('float32'), 50, 50).numpy()\n",
    "ValDataFrame = tf.signal.frame(ValData.astype('float32'), 50, 50).numpy()\n",
    "\n",
    "# np.random.shuffle(TrDataFrame)\n",
    "# np.random.shuffle(ValDataFrame)\n",
    "\n",
    "m_train_miss = np.random.choice([1,0], size=TrDataFrame.shape, p=[0.1,0.9])\n",
    "m_val_miss = np.random.choice([1,0], size=ValDataFrame.shape, p=[0.1,0.9])\n",
    "\n",
    "Tr_X = TrDataFrame.copy()\n",
    "Tr_Y = (TrDataFrame - 20.0) / (220.0 - 20.0)\n",
    "# 평균 80, 표준 편차 25인 정규 분포에서 값을 뽑아서 채울 배열 생성\n",
    "random_values = np.random.normal(loc=80, scale=25, size=TrDataFrame.shape)\n",
    "# m_train_miss가 1인 위치에 뽑은 값을 할당\n",
    "Tr_X[m_train_miss == 1] = random_values[m_train_miss == 1]\n",
    "Tr_X = (Tr_X - 20.0) / (220.0 - 20.0)\n",
    "\n",
    "m_train_miss[:,-10:,:] = 1\n",
    "Tr_X[:,-10:,:] = Tr_X[:,-10:,:] + np.random.normal(loc=0.0, scale=0.05, size=Tr_X[:,-10:,:].shape)\n",
    "\n",
    "Tr_X = np.clip(Tr_X, 0.0, 1.0)\n",
    "\n",
    "Val_X = ValDataFrame.copy()\n",
    "Val_Y = (ValDataFrame - 20.0) / (220.0 - 20.0)\n",
    "# 평균 80, 표준 편차 25인 정규 분포에서 값을 뽑아서 채울 배열 생성\n",
    "random_values = np.random.normal(loc=80, scale=25, size=ValDataFrame.shape)\n",
    "# m_train_miss가 1인 위치에 뽑은 값을 할당\n",
    "Val_X[m_val_miss == 1] = random_values[m_val_miss == 1]\n",
    "Val_X = (Val_X - 20.0) / (220.0 - 20.0)\n",
    "\n",
    "m_val_miss[:,-10:,:] = 1\n",
    "Val_X[:,-10:,:] = Val_X[:,-10:,:] + np.random.normal(loc=0.0, scale=0.05, size=Val_X[:,-10:,:].shape)\n",
    "\n",
    "Val_X = np.clip(Val_X, 0.0, 1.0)\n",
    "\n",
    "data_dim = TrDataFrame.shape[-1]\n",
    "time_length = TrDataFrame.shape[1]\n",
    "tr_sig_nb = len(TrDataFrame)\n",
    "\n",
    "del random_values\n",
    "del TrDataFrame\n",
    "del ValDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c38de903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12d2646f820>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Tr_X[0].ravel())\n",
    "plt.plot(Tr_Y[0].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f45e91b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "tf_x_train_miss = (\n",
    "    tf.data.Dataset.from_tensor_slices((Tr_X, Tr_Y, m_train_miss))\n",
    "    .shuffle(tr_sig_nb)\n",
    "    .batch(batch_size)\n",
    "    .repeat()\n",
    "    .prefetch(AUTOTUNE)  # Add prefetching.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa190d1c",
   "metadata": {},
   "source": [
    "### Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f2f6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = JointEncoderGRU\n",
    "decoder = GaussianDecoder\n",
    "\n",
    "model = HI_VAE(latent_dim=LatDim, data_dim=data_dim, time_length=time_length,\n",
    "               encoder_sizes=[100, 80, 60], encoder=encoder,\n",
    "               decoder_sizes=[60,80,100], decoder=decoder,\n",
    "               M=1, K=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eb9002c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (1, 60, 100)              45600     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (1, 60, 80)               8080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, 60, 60)               4860      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, 60, 20)               1220      \n",
      "=================================================================\n",
      "Total params: 59,760\n",
      "Trainable params: 59,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Encoder:  None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (1, 60, 60)               660       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (1, 60, 80)               4880      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (1, 60, 100)              8100      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, 60, 50)               22800     \n",
      "=================================================================\n",
      "Total params: 36,440\n",
      "Trainable params: 36,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Decoder:  None\n"
     ]
    }
   ],
   "source": [
    "_ = tf.compat.v1.train.get_or_create_global_step()\n",
    "trainable_vars = model.get_trainable_vars()\n",
    "# optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3)\n",
    "\n",
    "print(\"Encoder: \", model.encoder.net.summary())\n",
    "print(\"Decoder: \", model.decoder.net.summary())\n",
    "\n",
    "# For global step\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "gradient_clip = 1e4\n",
    "\n",
    "# Initialize optimizer and learning rate scheduler\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Checkpoints\n",
    "checkpoint = {\n",
    "    \"optimizer\": optimizer,\n",
    "    \"encoder\": model.encoder.net,\n",
    "    \"decoder\": model.decoder.net,\n",
    "    \"global_step\": global_step\n",
    "}\n",
    "\n",
    "if model.preprocessor is not None:\n",
    "    print(\"Preprocessor: \", model.preprocessor.net.summary())\n",
    "    checkpoint[\"preprocessor\"] = model.preprocessor.net\n",
    "\n",
    "saver = tf.train.Checkpoint(**checkpoint)\n",
    "\n",
    "# TensorBoard\n",
    "summary_writer = tf.summary.create_file_writer(outdir+'log/', flush_millis=10000)\n",
    "\n",
    "# Compute steps and intervals\n",
    "if num_steps == 0:\n",
    "    num_steps = num_epochs * tr_sig_nb // batch_size\n",
    "else:\n",
    "    num_steps = num_steps\n",
    "\n",
    "if print_interval == 0:\n",
    "    print_interval = num_steps // num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28796ca8",
   "metadata": {},
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0444a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.encoder.net.load_weights(outdir+'encoder2.hdf5')\n",
    "# model.decoder.net.load_weights(outdir+'decoder2.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc60ace",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431724e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:334: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it with `scale_diag` directly instead.\n",
      "WARNING:tensorflow:From C:\\Users\\USER\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\ops\\linalg\\linear_operator_diag.py:167: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n",
      "WARNING:tensorflow:From C:\\Users\\USER\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:334: calling TransformedDistribution.__init__ (from tensorflow_probability.python.distributions.transformed_distribution) with batch_shape is deprecated and will be removed after 2020-06-01.\n",
      "Instructions for updating:\n",
      "`batch_shape` and `event_shape` args are deprecated. Please use `tfd.Sample`, `tfd.Independent`, and broadcasted parameters of the base distribution instead. For example, replace `tfd.TransformedDistribution(tfd.Normal(0., 1.), tfb.Exp(), batch_shape=[2, 3], event_shape=[4])` with `tfd.TransformedDistrbution(tfd.Sample(tfd.Normal(tf.zeros([2, 3]), 1.),sample_shape=[4]), tfb.Exp())` or `tfd.TransformedDistribution(tfd.Independent(tfd.Normal(tf.zeros([2, 3, 4]), 1.), reinterpreted_batch_ndims=1), tfb.Exp())`.\n",
      "WARNING:tensorflow:From C:\\Users\\USER\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:334: calling TransformedDistribution.__init__ (from tensorflow_probability.python.distributions.transformed_distribution) with event_shape is deprecated and will be removed after 2020-06-01.\n",
      "Instructions for updating:\n",
      "`batch_shape` and `event_shape` args are deprecated. Please use `tfd.Sample`, `tfd.Independent`, and broadcasted parameters of the base distribution instead. For example, replace `tfd.TransformedDistribution(tfd.Normal(0., 1.), tfb.Exp(), batch_shape=[2, 3], event_shape=[4])` with `tfd.TransformedDistrbution(tfd.Sample(tfd.Normal(tf.zeros([2, 3]), 1.),sample_shape=[4]), tfb.Exp())` or `tfd.TransformedDistribution(tfd.Independent(tfd.Normal(tf.zeros([2, 3, 4]), 1.), reinterpreted_batch_ndims=1), tfb.Exp())`.\n",
      "================================================\n",
      "Learning rate: 0.000999989453703165 | Global gradient norm: 26.87\n",
      "Step 0) Time = 7.394078\n",
      "Train loss = 49.26612 | mse = 0.10386 | KL = 49.16226\n",
      "Validation loss = 49.26278 | mse = 0.10460 | KL = 49.15818\n",
      "================================================\n",
      "Learning rate: 0.0009980106260627508 | Global gradient norm: 29.10\n",
      "Step 188) Time = 9.317541\n",
      "Train loss = 0.05742 | mse = 0.01181 | KL = 0.04561\n",
      "Validation loss = 0.05595 | mse = 0.01133 | KL = 0.04462\n",
      "================================================\n",
      "Learning rate: 0.0009960357565432787 | Global gradient norm: 30.64\n",
      "Step 376) Time = 15.369115\n",
      "Train loss = 0.02596 | mse = 0.01052 | KL = 0.01544\n",
      "Validation loss = 0.02543 | mse = 0.01049 | KL = 0.01495\n",
      "================================================\n",
      "Learning rate: 0.0009940648451447487 | Global gradient norm: 32.06\n",
      "Step 564) Time = 18.636458\n",
      "Train loss = 0.01849 | mse = 0.00965 | KL = 0.00883\n",
      "Validation loss = 0.01859 | mse = 0.00969 | KL = 0.00890\n",
      "================================================\n",
      "Learning rate: 0.000992097775451839 | Global gradient norm: 33.16\n",
      "Step 752) Time = 18.889302\n",
      "Train loss = 0.01512 | mse = 0.00896 | KL = 0.00615\n",
      "Validation loss = 0.01515 | mse = 0.00897 | KL = 0.00618\n",
      "================================================\n",
      "Learning rate: 0.0009901345474645495 | Global gradient norm: 33.88\n",
      "Step 940) Time = 19.631493\n",
      "Train loss = 0.01545 | mse = 0.00954 | KL = 0.00591\n",
      "Validation loss = 0.01595 | mse = 0.00980 | KL = 0.00615\n",
      "================================================\n",
      "Learning rate: 0.0009881752775982022 | Global gradient norm: 34.54\n",
      "Step 1128) Time = 21.382390\n",
      "Train loss = 0.01311 | mse = 0.00834 | KL = 0.00477\n",
      "Validation loss = 0.01339 | mse = 0.00853 | KL = 0.00486\n",
      "================================================\n",
      "Learning rate: 0.000986219965852797 | Global gradient norm: 35.18\n",
      "Step 1316) Time = 20.834410\n",
      "Train loss = 0.02459 | mse = 0.00810 | KL = 0.01650\n",
      "Validation loss = 0.02459 | mse = 0.00826 | KL = 0.01633\n",
      "================================================\n",
      "Learning rate: 0.0009842683793976903 | Global gradient norm: 35.84\n",
      "Step 1504) Time = 23.845019\n",
      "Train loss = 0.01185 | mse = 0.00773 | KL = 0.00412\n",
      "Validation loss = 0.01187 | mse = 0.00774 | KL = 0.00413\n",
      "================================================\n",
      "Learning rate: 0.0009823206346482038 | Global gradient norm: 36.52\n",
      "Step 1692) Time = 28.337762\n",
      "Train loss = 0.01054 | mse = 0.00765 | KL = 0.00289\n",
      "Validation loss = 0.01110 | mse = 0.00806 | KL = 0.00303\n",
      "================================================\n",
      "Learning rate: 0.0009803767316043377 | Global gradient norm: 37.22\n",
      "Step 1880) Time = 27.843183\n",
      "Train loss = 0.01004 | mse = 0.00733 | KL = 0.00271\n",
      "Validation loss = 0.01029 | mse = 0.00755 | KL = 0.00274\n",
      "================================================\n",
      "Learning rate: 0.0009784367866814137 | Global gradient norm: 37.90\n",
      "Step 2068) Time = 32.209213\n",
      "Train loss = 0.01051 | mse = 0.00788 | KL = 0.00263\n",
      "Validation loss = 0.01047 | mse = 0.00790 | KL = 0.00257\n",
      "================================================\n",
      "Learning rate: 0.0009765006834641099 | Global gradient norm: 38.54\n",
      "Step 2256) Time = 37.081086\n",
      "Train loss = 0.00993 | mse = 0.00746 | KL = 0.00247\n",
      "Validation loss = 0.00987 | mse = 0.00744 | KL = 0.00243\n",
      "================================================\n",
      "Learning rate: 0.0009745683637447655 | Global gradient norm: 39.23\n",
      "Step 2444) Time = 34.209618\n",
      "Train loss = 0.01039 | mse = 0.00740 | KL = 0.00299\n",
      "Validation loss = 0.01041 | mse = 0.00752 | KL = 0.00289\n",
      "================================================\n",
      "Learning rate: 0.0009726398275233805 | Global gradient norm: 39.91\n",
      "Step 2632) Time = 39.586428\n",
      "Train loss = 0.01047 | mse = 0.00790 | KL = 0.00256\n",
      "Validation loss = 0.01059 | mse = 0.00801 | KL = 0.00258\n",
      "================================================\n",
      "Learning rate: 0.0009707151330076158 | Global gradient norm: 40.54\n",
      "Step 2820) Time = 38.433810\n",
      "Train loss = 0.00916 | mse = 0.00728 | KL = 0.00187\n",
      "Validation loss = 0.00916 | mse = 0.00726 | KL = 0.00189\n",
      "================================================\n",
      "Learning rate: 0.0009687942801974714 | Global gradient norm: 41.13\n",
      "Step 3008) Time = 37.486006\n",
      "Train loss = 0.01138 | mse = 0.00748 | KL = 0.00390\n",
      "Validation loss = 0.01147 | mse = 0.00749 | KL = 0.00398\n",
      "================================================\n",
      "Learning rate: 0.0009668772690929472 | Global gradient norm: 41.71\n",
      "Step 3196) Time = 45.653031\n",
      "Train loss = 0.00881 | mse = 0.00702 | KL = 0.00180\n",
      "Validation loss = 0.00906 | mse = 0.00730 | KL = 0.00176\n",
      "================================================\n",
      "Learning rate: 0.0009649640414863825 | Global gradient norm: 42.28\n",
      "Step 3384) Time = 45.983030\n",
      "Train loss = 0.00887 | mse = 0.00716 | KL = 0.00172\n",
      "Validation loss = 0.00889 | mse = 0.00713 | KL = 0.00176\n",
      "================================================\n",
      "Learning rate: 0.0009630544809624553 | Global gradient norm: 42.86\n",
      "Step 3572) Time = 46.247680\n",
      "Train loss = 0.00903 | mse = 0.00697 | KL = 0.00206\n",
      "Validation loss = 0.00893 | mse = 0.00692 | KL = 0.00201\n",
      "================================================\n",
      "Learning rate: 0.0009611487621441483 | Global gradient norm: 43.37\n",
      "Step 3760) Time = 54.859117\n",
      "Train loss = 0.00873 | mse = 0.00700 | KL = 0.00173\n",
      "Validation loss = 0.00855 | mse = 0.00684 | KL = 0.00171\n",
      "================================================\n",
      "Learning rate: 0.0009592468850314617 | Global gradient norm: 43.84\n",
      "Step 3948) Time = 54.794727\n",
      "Train loss = 0.01107 | mse = 0.00802 | KL = 0.00305\n",
      "Validation loss = 0.01060 | mse = 0.00762 | KL = 0.00298\n",
      "================================================\n",
      "Learning rate: 0.0009573486750014126 | Global gradient norm: 44.33\n",
      "Step 4136) Time = 53.838058\n",
      "Train loss = 0.00798 | mse = 0.00673 | KL = 0.00125\n",
      "Validation loss = 0.00791 | mse = 0.00665 | KL = 0.00125\n",
      "================================================\n",
      "Learning rate: 0.0009554542484693229 | Global gradient norm: 44.79\n",
      "Step 4324) Time = 53.082270\n",
      "Train loss = 0.00854 | mse = 0.00687 | KL = 0.00167\n",
      "Validation loss = 0.00846 | mse = 0.00673 | KL = 0.00173\n",
      "================================================\n",
      "Learning rate: 0.0009535635472275317 | Global gradient norm: 45.16\n",
      "Step 4512) Time = 52.493589\n",
      "Train loss = 0.01061 | mse = 0.00719 | KL = 0.00342\n",
      "Validation loss = 0.01047 | mse = 0.00713 | KL = 0.00334\n",
      "================================================\n",
      "Learning rate: 0.0009516765712760389 | Global gradient norm: 45.66\n",
      "Step 4700) Time = 61.829126\n",
      "Train loss = 0.00908 | mse = 0.00739 | KL = 0.00168\n",
      "Validation loss = 0.00904 | mse = 0.00739 | KL = 0.00166\n",
      "================================================\n",
      "Learning rate: 0.0009497934370301664 | Global gradient norm: 46.07\n",
      "Step 4888) Time = 60.887105\n",
      "Train loss = 0.00936 | mse = 0.00714 | KL = 0.00223\n",
      "Validation loss = 0.00950 | mse = 0.00728 | KL = 0.00222\n",
      "================================================\n",
      "Learning rate: 0.0009479139698669314 | Global gradient norm: 46.49\n",
      "Step 5076) Time = 64.464170\n",
      "Train loss = 0.00773 | mse = 0.00669 | KL = 0.00104\n",
      "Validation loss = 0.00766 | mse = 0.00663 | KL = 0.00103\n",
      "================================================\n",
      "Learning rate: 0.000946038227993995 | Global gradient norm: 46.88\n",
      "Step 5264) Time = 65.313105\n",
      "Train loss = 0.00900 | mse = 0.00725 | KL = 0.00175\n",
      "Validation loss = 0.00901 | mse = 0.00718 | KL = 0.00183\n",
      "================================================\n",
      "Learning rate: 0.000944166153203696 | Global gradient norm: 47.31\n",
      "Step 5452) Time = 64.279098\n",
      "Train loss = 0.00929 | mse = 0.00803 | KL = 0.00126\n",
      "Validation loss = 0.00925 | mse = 0.00798 | KL = 0.00127\n",
      "================================================\n",
      "Learning rate: 0.0009422979201190174 | Global gradient norm: 47.70\n",
      "Step 5640) Time = 66.843045\n",
      "Train loss = 0.00975 | mse = 0.00698 | KL = 0.00277\n",
      "Validation loss = 0.00973 | mse = 0.00694 | KL = 0.00278\n",
      "================================================\n",
      "Learning rate: 0.0009404332377016544 | Global gradient norm: 48.19\n",
      "Step 5828) Time = 67.241088\n",
      "Train loss = 0.01407 | mse = 0.00810 | KL = 0.00596\n",
      "Validation loss = 0.01408 | mse = 0.00807 | KL = 0.00601\n",
      "================================================\n",
      "Learning rate: 0.00093857228057459 | Global gradient norm: 48.68\n",
      "Step 6016) Time = 64.526043\n",
      "Train loss = 0.00901 | mse = 0.00737 | KL = 0.00164\n",
      "Validation loss = 0.00894 | mse = 0.00730 | KL = 0.00164\n",
      "================================================\n",
      "Learning rate: 0.000936714990530163 | Global gradient norm: 49.19\n",
      "Step 6204) Time = 74.400051\n",
      "Train loss = 0.00837 | mse = 0.00718 | KL = 0.00119\n",
      "Validation loss = 0.00850 | mse = 0.00732 | KL = 0.00119\n",
      "================================================\n",
      "Learning rate: 0.0009348614257760346 | Global gradient norm: 49.70\n",
      "Step 6392) Time = 76.388052\n",
      "Train loss = 0.00778 | mse = 0.00626 | KL = 0.00152\n",
      "Validation loss = 0.00785 | mse = 0.00632 | KL = 0.00153\n",
      "================================================\n",
      "Learning rate: 0.0009330115281045437 | Global gradient norm: 50.21\n",
      "Step 6580) Time = 76.117165\n",
      "Train loss = 0.00760 | mse = 0.00640 | KL = 0.00120\n",
      "Validation loss = 0.00735 | mse = 0.00620 | KL = 0.00115\n",
      "================================================\n",
      "Learning rate: 0.0009311652393080294 | Global gradient norm: 50.68\n",
      "Step 6768) Time = 83.922056\n",
      "Train loss = 0.00962 | mse = 0.00735 | KL = 0.00227\n",
      "Validation loss = 0.00949 | mse = 0.00724 | KL = 0.00224\n",
      "================================================\n",
      "Learning rate: 0.0009293226175941527 | Global gradient norm: 50.87\n",
      "Step 6956) Time = 83.520169\n",
      "Train loss = 0.00765 | mse = 0.00554 | KL = 0.00211\n",
      "Validation loss = 0.00743 | mse = 0.00534 | KL = 0.00209\n",
      "================================================\n",
      "Learning rate: 0.0009274836629629135 | Global gradient norm: 51.03\n",
      "Step 7144) Time = 84.373104\n",
      "Train loss = 0.00799 | mse = 0.00600 | KL = 0.00198\n",
      "Validation loss = 0.00790 | mse = 0.00592 | KL = 0.00197\n",
      "================================================\n",
      "Learning rate: 0.0009256483754143119 | Global gradient norm: 51.16\n",
      "Step 7332) Time = 79.965055\n",
      "Train loss = 0.00815 | mse = 0.00649 | KL = 0.00166\n",
      "Validation loss = 0.00817 | mse = 0.00653 | KL = 0.00164\n",
      "================================================\n",
      "Learning rate: 0.000923816638533026 | Global gradient norm: 51.24\n",
      "Step 7520) Time = 78.382165\n",
      "Train loss = 0.00596 | mse = 0.00464 | KL = 0.00131\n",
      "Validation loss = 0.00607 | mse = 0.00471 | KL = 0.00136\n",
      "================================================\n",
      "Learning rate: 0.0009219885687343776 | Global gradient norm: 51.34\n",
      "Step 7708) Time = 82.926056\n",
      "Train loss = 0.01071 | mse = 0.00667 | KL = 0.00404\n",
      "Validation loss = 0.01071 | mse = 0.00666 | KL = 0.00405\n",
      "================================================\n",
      "Learning rate: 0.0009201641660183668 | Global gradient norm: 51.43\n",
      "Step 7896) Time = 82.296056\n",
      "Train loss = 0.00661 | mse = 0.00508 | KL = 0.00154\n",
      "Validation loss = 0.00672 | mse = 0.00510 | KL = 0.00162\n",
      "================================================\n",
      "Learning rate: 0.0009183433139696717 | Global gradient norm: 51.61\n",
      "Step 8084) Time = 82.659168\n",
      "Train loss = 0.00514 | mse = 0.00428 | KL = 0.00086\n",
      "Validation loss = 0.00501 | mse = 0.00417 | KL = 0.00085\n",
      "================================================\n",
      "Learning rate: 0.0009165261290036142 | Global gradient norm: 51.74\n",
      "Step 8272) Time = 86.935059\n",
      "Train loss = 0.00521 | mse = 0.00427 | KL = 0.00094\n",
      "Validation loss = 0.00514 | mse = 0.00417 | KL = 0.00097\n",
      "================================================\n",
      "Learning rate: 0.0009147124947048724 | Global gradient norm: 51.91\n",
      "Step 8460) Time = 85.882253\n",
      "Train loss = 0.00499 | mse = 0.00413 | KL = 0.00086\n",
      "Validation loss = 0.00497 | mse = 0.00409 | KL = 0.00088\n",
      "================================================\n",
      "Learning rate: 0.0009129024110734463 | Global gradient norm: 52.08\n",
      "Step 8648) Time = 92.330114\n",
      "Train loss = 0.00845 | mse = 0.00564 | KL = 0.00281\n",
      "Validation loss = 0.00849 | mse = 0.00569 | KL = 0.00279\n",
      "================================================\n",
      "Learning rate: 0.0009110958781093359 | Global gradient norm: 52.24\n",
      "Step 8836) Time = 90.223318\n",
      "Train loss = 0.00504 | mse = 0.00414 | KL = 0.00090\n",
      "Validation loss = 0.00497 | mse = 0.00409 | KL = 0.00088\n",
      "================================================\n",
      "Learning rate: 0.000909293070435524 | Global gradient norm: 52.39\n",
      "Step 9024) Time = 85.823059\n",
      "Train loss = 0.00744 | mse = 0.00487 | KL = 0.00257\n",
      "Validation loss = 0.00753 | mse = 0.00495 | KL = 0.00258\n",
      "================================================\n",
      "Learning rate: 0.0009074937552213669 | Global gradient norm: 52.59\n",
      "Step 9212) Time = 92.018145\n",
      "Train loss = 0.00686 | mse = 0.00443 | KL = 0.00244\n",
      "Validation loss = 0.00699 | mse = 0.00450 | KL = 0.00249\n",
      "================================================\n",
      "Learning rate: 0.0009056979324668646 | Global gradient norm: 52.72\n",
      "Step 9400) Time = 94.992109\n",
      "Train loss = 0.00460 | mse = 0.00369 | KL = 0.00090\n",
      "Validation loss = 0.00456 | mse = 0.00368 | KL = 0.00088\n",
      "================================================\n",
      "Learning rate: 0.0009039057185873389 | Global gradient norm: 52.89\n",
      "Step 9588) Time = 94.258103\n",
      "Train loss = 0.00444 | mse = 0.00365 | KL = 0.00079\n",
      "Validation loss = 0.00436 | mse = 0.00358 | KL = 0.00078\n",
      "================================================\n",
      "Learning rate: 0.000902117055375129 | Global gradient norm: 53.04\n",
      "Step 9776) Time = 99.619138\n",
      "Train loss = 0.00662 | mse = 0.00373 | KL = 0.00289\n",
      "Validation loss = 0.00679 | mse = 0.00386 | KL = 0.00292\n",
      "================================================\n",
      "Learning rate: 0.0009003319428302348 | Global gradient norm: 53.22\n",
      "Step 9964) Time = 99.313069\n",
      "Train loss = 0.00425 | mse = 0.00347 | KL = 0.00078\n",
      "Validation loss = 0.00423 | mse = 0.00345 | KL = 0.00078\n",
      "================================================\n",
      "Learning rate: 0.0008985503809526563 | Global gradient norm: 53.40\n",
      "Step 10152) Time = 104.497158\n",
      "Train loss = 0.00570 | mse = 0.00466 | KL = 0.00104\n",
      "Validation loss = 0.00580 | mse = 0.00477 | KL = 0.00103\n",
      "================================================\n",
      "Learning rate: 0.0008967723115347326 | Global gradient norm: 53.59\n",
      "Step 10340) Time = 99.145175\n",
      "Train loss = 0.00388 | mse = 0.00318 | KL = 0.00070\n",
      "Validation loss = 0.00389 | mse = 0.00318 | KL = 0.00071\n",
      "================================================\n",
      "Learning rate: 0.0008949977345764637 | Global gradient norm: 53.65\n",
      "Step 10528) Time = 96.899065\n",
      "Train loss = 0.00401 | mse = 0.00318 | KL = 0.00083\n",
      "Validation loss = 0.00406 | mse = 0.00325 | KL = 0.00082\n",
      "================================================\n",
      "Learning rate: 0.0008932267664931715 | Global gradient norm: 53.85\n",
      "Step 10716) Time = 101.236185\n",
      "Train loss = 0.00408 | mse = 0.00316 | KL = 0.00092\n",
      "Validation loss = 0.00417 | mse = 0.00323 | KL = 0.00094\n",
      "================================================\n",
      "Learning rate: 0.0008914592326618731 | Global gradient norm: 54.05\n",
      "Step 10904) Time = 103.290070\n",
      "Train loss = 0.00576 | mse = 0.00309 | KL = 0.00267\n",
      "Validation loss = 0.00574 | mse = 0.00308 | KL = 0.00266\n",
      "================================================\n",
      "Learning rate: 0.0008896951330825686 | Global gradient norm: 54.24\n",
      "Step 11092) Time = 98.322131\n",
      "Train loss = 0.00582 | mse = 0.00484 | KL = 0.00098\n",
      "Validation loss = 0.00579 | mse = 0.00480 | KL = 0.00099\n",
      "================================================\n",
      "Learning rate: 0.0008879345841705799 | Global gradient norm: 54.41\n",
      "Step 11280) Time = 108.508194\n",
      "Train loss = 0.00453 | mse = 0.00345 | KL = 0.00108\n",
      "Validation loss = 0.00454 | mse = 0.00346 | KL = 0.00108\n",
      "================================================\n",
      "Learning rate: 0.0008861774695105851 | Global gradient norm: 54.60\n",
      "Step 11468) Time = 109.566195\n",
      "Train loss = 0.00525 | mse = 0.00367 | KL = 0.00157\n",
      "Validation loss = 0.00516 | mse = 0.00361 | KL = 0.00155\n",
      "================================================\n",
      "Learning rate: 0.0008844239637255669 | Global gradient norm: 54.75\n",
      "Step 11656) Time = 106.872193\n",
      "Train loss = 0.00491 | mse = 0.00308 | KL = 0.00183\n",
      "Validation loss = 0.00490 | mse = 0.00308 | KL = 0.00181\n",
      "================================================\n",
      "Learning rate: 0.0008826738339848816 | Global gradient norm: 54.93\n",
      "Step 11844) Time = 104.177192\n",
      "Train loss = 0.00395 | mse = 0.00299 | KL = 0.00096\n",
      "Validation loss = 0.00390 | mse = 0.00297 | KL = 0.00094\n",
      "================================================\n",
      "Learning rate: 0.0008809271967038512 | Global gradient norm: 55.10\n",
      "Step 12032) Time = 102.957198\n",
      "Train loss = 0.00554 | mse = 0.00456 | KL = 0.00098\n",
      "Validation loss = 0.00544 | mse = 0.00448 | KL = 0.00096\n",
      "================================================\n",
      "Learning rate: 0.0008791839936748147 | Global gradient norm: 55.30\n",
      "Step 12220) Time = 108.951074\n",
      "Train loss = 0.00383 | mse = 0.00306 | KL = 0.00077\n",
      "Validation loss = 0.00400 | mse = 0.00319 | KL = 0.00081\n",
      "================================================\n",
      "Learning rate: 0.0008774441666901112 | Global gradient norm: 55.44\n",
      "Step 12408) Time = 110.700664\n",
      "Train loss = 0.00410 | mse = 0.00286 | KL = 0.00125\n",
      "Validation loss = 0.00411 | mse = 0.00287 | KL = 0.00124\n",
      "================================================\n",
      "Learning rate: 0.0008757079485803843 | Global gradient norm: 55.63\n",
      "Step 12596) Time = 108.624965\n",
      "Train loss = 0.00359 | mse = 0.00288 | KL = 0.00071\n",
      "Validation loss = 0.00369 | mse = 0.00297 | KL = 0.00072\n",
      "================================================\n",
      "Learning rate: 0.0008739751065149903 | Global gradient norm: 55.78\n",
      "Step 12784) Time = 109.866389\n",
      "Train loss = 0.00630 | mse = 0.00347 | KL = 0.00284\n",
      "Validation loss = 0.00630 | mse = 0.00346 | KL = 0.00284\n",
      "================================================\n",
      "Learning rate: 0.0008722456404939294 | Global gradient norm: 55.89\n",
      "Step 12972) Time = 106.730219\n",
      "Train loss = 0.00305 | mse = 0.00238 | KL = 0.00067\n",
      "Validation loss = 0.00299 | mse = 0.00231 | KL = 0.00069\n",
      "================================================\n",
      "Learning rate: 0.0008705196087248623 | Global gradient norm: 56.01\n",
      "Step 13160) Time = 107.651040\n",
      "Train loss = 0.00325 | mse = 0.00260 | KL = 0.00065\n",
      "Validation loss = 0.00324 | mse = 0.00257 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0008687970694154501 | Global gradient norm: 56.09\n",
      "Step 13348) Time = 110.459349\n",
      "Train loss = 0.00322 | mse = 0.00244 | KL = 0.00079\n",
      "Validation loss = 0.00322 | mse = 0.00242 | KL = 0.00080\n",
      "================================================\n",
      "Learning rate: 0.0008670778479427099 | Global gradient norm: 56.23\n",
      "Step 13536) Time = 107.984943\n",
      "Train loss = 0.00399 | mse = 0.00301 | KL = 0.00098\n",
      "Validation loss = 0.00394 | mse = 0.00295 | KL = 0.00099\n",
      "================================================\n",
      "Learning rate: 0.0008653620025143027 | Global gradient norm: 56.31\n",
      "Step 13724) Time = 110.480480\n",
      "Train loss = 0.00389 | mse = 0.00281 | KL = 0.00108\n",
      "Validation loss = 0.00383 | mse = 0.00277 | KL = 0.00107\n",
      "================================================\n",
      "Learning rate: 0.0008636496495455503 | Global gradient norm: 56.39\n",
      "Step 13912) Time = 109.840975\n",
      "Train loss = 0.00275 | mse = 0.00211 | KL = 0.00064\n",
      "Validation loss = 0.00282 | mse = 0.00217 | KL = 0.00065\n",
      "================================================\n",
      "Learning rate: 0.00086194061441347 | Global gradient norm: 56.47\n",
      "Step 14100) Time = 107.079031\n",
      "Train loss = 0.00323 | mse = 0.00238 | KL = 0.00086\n",
      "Validation loss = 0.00320 | mse = 0.00234 | KL = 0.00087\n",
      "================================================\n",
      "Learning rate: 0.0008602350135333836 | Global gradient norm: 56.54\n",
      "Step 14288) Time = 110.382044\n",
      "Train loss = 0.00357 | mse = 0.00284 | KL = 0.00073\n",
      "Validation loss = 0.00361 | mse = 0.00286 | KL = 0.00076\n",
      "================================================\n",
      "Learning rate: 0.0008585327886976302 | Global gradient norm: 56.60\n",
      "Step 14476) Time = 112.921171\n",
      "Train loss = 0.00363 | mse = 0.00231 | KL = 0.00132\n",
      "Validation loss = 0.00355 | mse = 0.00225 | KL = 0.00131\n",
      "================================================\n",
      "Learning rate: 0.0008568338816985488 | Global gradient norm: 56.64\n",
      "Step 14664) Time = 111.063879\n",
      "Train loss = 0.00263 | mse = 0.00182 | KL = 0.00081\n",
      "Validation loss = 0.00267 | mse = 0.00187 | KL = 0.00080\n",
      "================================================\n",
      "Learning rate: 0.0008551383507438004 | Global gradient norm: 56.72\n",
      "Step 14852) Time = 112.477191\n",
      "Train loss = 0.00326 | mse = 0.00195 | KL = 0.00130\n",
      "Validation loss = 0.00324 | mse = 0.00195 | KL = 0.00130\n",
      "================================================\n",
      "Learning rate: 0.0008534462540410459 | Global gradient norm: 56.80\n",
      "Step 15040) Time = 110.257938\n",
      "Train loss = 0.00300 | mse = 0.00232 | KL = 0.00068\n",
      "Validation loss = 0.00289 | mse = 0.00223 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0008517574169673026 | Global gradient norm: 56.78\n",
      "Step 15228) Time = 118.177832\n",
      "Train loss = 0.00306 | mse = 0.00234 | KL = 0.00071\n",
      "Validation loss = 0.00307 | mse = 0.00234 | KL = 0.00073\n",
      "================================================\n",
      "Learning rate: 0.0008500719559378922 | Global gradient norm: 56.84\n",
      "Step 15416) Time = 117.456556\n",
      "Train loss = 0.00278 | mse = 0.00218 | KL = 0.00060\n",
      "Validation loss = 0.00277 | mse = 0.00215 | KL = 0.00062\n",
      "================================================\n",
      "Learning rate: 0.0008483898127451539 | Global gradient norm: 56.94\n",
      "Step 15604) Time = 116.142225\n",
      "Train loss = 0.00341 | mse = 0.00257 | KL = 0.00085\n",
      "Validation loss = 0.00346 | mse = 0.00261 | KL = 0.00086\n",
      "================================================\n",
      "Learning rate: 0.0008467109873890877 | Global gradient norm: 56.95\n",
      "Step 15792) Time = 118.545143\n",
      "Train loss = 0.00267 | mse = 0.00168 | KL = 0.00099\n",
      "Validation loss = 0.00264 | mse = 0.00164 | KL = 0.00100\n",
      "================================================\n",
      "Learning rate: 0.0008450355380773544 | Global gradient norm: 57.00\n",
      "Step 15980) Time = 118.461875\n",
      "Train loss = 0.00268 | mse = 0.00174 | KL = 0.00094\n",
      "Validation loss = 0.00265 | mse = 0.00172 | KL = 0.00093\n",
      "================================================\n",
      "Learning rate: 0.0008433633483946323 | Global gradient norm: 57.04\n",
      "Step 16168) Time = 115.394672\n",
      "Train loss = 0.00238 | mse = 0.00182 | KL = 0.00056\n",
      "Validation loss = 0.00238 | mse = 0.00182 | KL = 0.00056\n",
      "================================================\n",
      "Learning rate: 0.0008416944765485823 | Global gradient norm: 57.09\n",
      "Step 16356) Time = 115.918021\n",
      "Train loss = 0.00314 | mse = 0.00252 | KL = 0.00063\n",
      "Validation loss = 0.00306 | mse = 0.00244 | KL = 0.00062\n",
      "================================================\n",
      "Learning rate: 0.0008400288643315434 | Global gradient norm: 57.08\n",
      "Step 16544) Time = 112.276263\n",
      "Train loss = 0.00199 | mse = 0.00143 | KL = 0.00056\n",
      "Validation loss = 0.00209 | mse = 0.00151 | KL = 0.00058\n",
      "================================================\n",
      "Learning rate: 0.0008383666281588376 | Global gradient norm: 57.15\n",
      "Step 16732) Time = 116.303109\n",
      "Train loss = 0.00402 | mse = 0.00290 | KL = 0.00112\n",
      "Validation loss = 0.00404 | mse = 0.00292 | KL = 0.00112\n",
      "================================================\n",
      "Learning rate: 0.0008367077098228037 | Global gradient norm: 57.18\n",
      "Step 16920) Time = 116.611799\n",
      "Train loss = 0.00636 | mse = 0.00288 | KL = 0.00347\n",
      "Validation loss = 0.00646 | mse = 0.00294 | KL = 0.00351\n",
      "================================================\n",
      "Learning rate: 0.0008350519929081202 | Global gradient norm: 57.24\n",
      "Step 17108) Time = 114.457643\n",
      "Train loss = 0.00215 | mse = 0.00153 | KL = 0.00062\n",
      "Validation loss = 0.00214 | mse = 0.00152 | KL = 0.00062\n",
      "================================================\n",
      "Learning rate: 0.0008333995356224477 | Global gradient norm: 57.33\n",
      "Step 17296) Time = 126.514813\n",
      "Train loss = 0.00223 | mse = 0.00170 | KL = 0.00053\n",
      "Validation loss = 0.00220 | mse = 0.00169 | KL = 0.00052\n",
      "================================================\n",
      "Learning rate: 0.0008317503961734474 | Global gradient norm: 57.40\n",
      "Step 17484) Time = 120.165727\n",
      "Train loss = 0.00234 | mse = 0.00175 | KL = 0.00059\n",
      "Validation loss = 0.00236 | mse = 0.00178 | KL = 0.00058\n",
      "================================================\n",
      "Learning rate: 0.0008301045745611191 | Global gradient norm: 57.48\n",
      "Step 17672) Time = 115.140185\n",
      "Train loss = 0.00203 | mse = 0.00158 | KL = 0.00046\n",
      "Validation loss = 0.00206 | mse = 0.00158 | KL = 0.00048\n",
      "================================================\n",
      "Learning rate: 0.0008284618961624801 | Global gradient norm: 57.51\n",
      "Step 17860) Time = 116.622707\n",
      "Train loss = 0.00256 | mse = 0.00195 | KL = 0.00061\n",
      "Validation loss = 0.00259 | mse = 0.00197 | KL = 0.00062\n",
      "================================================\n",
      "Learning rate: 0.0008268224773928523 | Global gradient norm: 57.60\n",
      "Step 18048) Time = 113.953687\n",
      "Train loss = 0.00191 | mse = 0.00142 | KL = 0.00049\n",
      "Validation loss = 0.00192 | mse = 0.00143 | KL = 0.00049\n",
      "================================================\n",
      "Learning rate: 0.0008251863764598966 | Global gradient norm: 57.66\n",
      "Step 18236) Time = 120.299491\n",
      "Train loss = 0.00201 | mse = 0.00151 | KL = 0.00050\n",
      "Validation loss = 0.00196 | mse = 0.00147 | KL = 0.00049\n",
      "================================================\n",
      "Learning rate: 0.0008235534769482911 | Global gradient norm: 57.71\n",
      "Step 18424) Time = 118.080950\n",
      "Train loss = 0.00427 | mse = 0.00305 | KL = 0.00122\n",
      "Validation loss = 0.00430 | mse = 0.00305 | KL = 0.00125\n",
      "================================================\n",
      "Learning rate: 0.0008219238952733576 | Global gradient norm: 57.76\n",
      "Step 18612) Time = 114.390547\n",
      "Train loss = 0.00224 | mse = 0.00167 | KL = 0.00057\n",
      "Validation loss = 0.00222 | mse = 0.00166 | KL = 0.00056\n",
      "================================================\n",
      "Learning rate: 0.0008202973986044526 | Global gradient norm: 57.86\n",
      "Step 18800) Time = 116.110241\n",
      "Train loss = 0.00292 | mse = 0.00179 | KL = 0.00113\n",
      "Validation loss = 0.00285 | mse = 0.00172 | KL = 0.00113\n",
      "================================================\n",
      "Learning rate: 0.0008186741615645587 | Global gradient norm: 57.94\n",
      "Step 18988) Time = 117.447631\n",
      "Train loss = 0.00244 | mse = 0.00166 | KL = 0.00078\n",
      "Validation loss = 0.00238 | mse = 0.00160 | KL = 0.00078\n",
      "================================================\n",
      "Learning rate: 0.0008170541259460151 | Global gradient norm: 58.02\n",
      "Step 19176) Time = 115.400432\n",
      "Train loss = 0.00177 | mse = 0.00130 | KL = 0.00047\n",
      "Validation loss = 0.00179 | mse = 0.00131 | KL = 0.00048\n",
      "================================================\n",
      "Learning rate: 0.0008154373499564826 | Global gradient norm: 58.12\n",
      "Step 19364) Time = 116.106184\n",
      "Train loss = 0.00207 | mse = 0.00160 | KL = 0.00046\n",
      "Validation loss = 0.00216 | mse = 0.00170 | KL = 0.00046\n",
      "================================================\n",
      "Learning rate: 0.0008138237753883004 | Global gradient norm: 58.20\n",
      "Step 19552) Time = 116.478198\n",
      "Train loss = 0.00183 | mse = 0.00137 | KL = 0.00047\n",
      "Validation loss = 0.00183 | mse = 0.00137 | KL = 0.00047\n",
      "================================================\n",
      "Learning rate: 0.0008122134022414684 | Global gradient norm: 58.29\n",
      "Step 19740) Time = 114.212198\n",
      "Train loss = 0.00168 | mse = 0.00125 | KL = 0.00043\n",
      "Validation loss = 0.00166 | mse = 0.00124 | KL = 0.00042\n",
      "================================================\n",
      "Learning rate: 0.0008106061723083258 | Global gradient norm: 58.35\n",
      "Step 19928) Time = 126.254086\n",
      "Train loss = 0.00306 | mse = 0.00167 | KL = 0.00139\n",
      "Validation loss = 0.00304 | mse = 0.00166 | KL = 0.00138\n",
      "================================================\n",
      "Learning rate: 0.0008090021437965333 | Global gradient norm: 58.49\n",
      "Step 20116) Time = 126.857197\n",
      "Train loss = 0.00230 | mse = 0.00141 | KL = 0.00089\n",
      "Validation loss = 0.00224 | mse = 0.00138 | KL = 0.00087\n",
      "================================================\n",
      "Learning rate: 0.0008074012002907693 | Global gradient norm: 58.58\n",
      "Step 20304) Time = 120.418129\n",
      "Train loss = 0.00177 | mse = 0.00122 | KL = 0.00055\n",
      "Validation loss = 0.00178 | mse = 0.00125 | KL = 0.00053\n",
      "================================================\n",
      "Learning rate: 0.0008058035164140165 | Global gradient norm: 58.66\n",
      "Step 20492) Time = 118.852080\n",
      "Train loss = 0.00286 | mse = 0.00211 | KL = 0.00074\n",
      "Validation loss = 0.00288 | mse = 0.00212 | KL = 0.00076\n",
      "================================================\n",
      "Learning rate: 0.000804208975750953 | Global gradient norm: 58.82\n",
      "Step 20680) Time = 117.335200\n",
      "Train loss = 0.00218 | mse = 0.00162 | KL = 0.00056\n",
      "Validation loss = 0.00221 | mse = 0.00165 | KL = 0.00056\n",
      "================================================\n",
      "Learning rate: 0.0008026175783015788 | Global gradient norm: 58.89\n",
      "Step 20868) Time = 118.372080\n",
      "Train loss = 0.00161 | mse = 0.00118 | KL = 0.00042\n",
      "Validation loss = 0.00159 | mse = 0.00116 | KL = 0.00043\n",
      "================================================\n",
      "Learning rate: 0.0008010293822735548 | Global gradient norm: 58.98\n",
      "Step 21056) Time = 117.417206\n",
      "Train loss = 0.00170 | mse = 0.00127 | KL = 0.00043\n",
      "Validation loss = 0.00171 | mse = 0.00128 | KL = 0.00043\n",
      "================================================\n",
      "Learning rate: 0.0007994442712515593 | Global gradient norm: 59.13\n",
      "Step 21244) Time = 114.648273\n",
      "Train loss = 0.00172 | mse = 0.00128 | KL = 0.00044\n",
      "Validation loss = 0.00174 | mse = 0.00129 | KL = 0.00045\n",
      "================================================\n",
      "Learning rate: 0.000797862303443253 | Global gradient norm: 59.17\n",
      "Step 21432) Time = 117.737079\n",
      "Train loss = 0.00172 | mse = 0.00128 | KL = 0.00044\n",
      "Validation loss = 0.00172 | mse = 0.00128 | KL = 0.00044\n",
      "================================================\n",
      "Learning rate: 0.0007962834788486362 | Global gradient norm: 59.28\n",
      "Step 21620) Time = 115.981079\n",
      "Train loss = 0.00287 | mse = 0.00227 | KL = 0.00061\n",
      "Validation loss = 0.00290 | mse = 0.00227 | KL = 0.00063\n",
      "================================================\n",
      "Learning rate: 0.0007947077392600477 | Global gradient norm: 59.42\n",
      "Step 21808) Time = 120.863131\n",
      "Train loss = 0.00235 | mse = 0.00190 | KL = 0.00045\n",
      "Validation loss = 0.00237 | mse = 0.00192 | KL = 0.00045\n",
      "================================================\n",
      "Learning rate: 0.0007931352010928094 | Global gradient norm: 59.52\n",
      "Step 21996) Time = 121.664198\n",
      "Train loss = 0.00161 | mse = 0.00119 | KL = 0.00042\n",
      "Validation loss = 0.00163 | mse = 0.00120 | KL = 0.00043\n",
      "================================================\n",
      "Learning rate: 0.0007915657479315996 | Global gradient norm: 59.70\n",
      "Step 22184) Time = 119.454081\n",
      "Train loss = 0.00376 | mse = 0.00319 | KL = 0.00057\n",
      "Validation loss = 0.00370 | mse = 0.00313 | KL = 0.00056\n",
      "================================================\n",
      "Learning rate: 0.0007899993797764182 | Global gradient norm: 59.79\n",
      "Step 22372) Time = 123.027195\n",
      "Train loss = 0.00185 | mse = 0.00139 | KL = 0.00046\n",
      "Validation loss = 0.00183 | mse = 0.00138 | KL = 0.00046\n",
      "================================================\n",
      "Learning rate: 0.0007884360966272652 | Global gradient norm: 59.92\n",
      "Step 22560) Time = 118.221202\n",
      "Train loss = 0.00172 | mse = 0.00121 | KL = 0.00051\n",
      "Validation loss = 0.00174 | mse = 0.00122 | KL = 0.00052\n",
      "================================================\n",
      "Learning rate: 0.0007868760148994625 | Global gradient norm: 60.04\n",
      "Step 22748) Time = 114.499139\n",
      "Train loss = 0.00177 | mse = 0.00134 | KL = 0.00044\n",
      "Validation loss = 0.00177 | mse = 0.00133 | KL = 0.00044\n",
      "================================================\n",
      "Learning rate: 0.0007853189017623663 | Global gradient norm: 60.12\n",
      "Step 22936) Time = 117.241254\n",
      "Train loss = 0.00210 | mse = 0.00146 | KL = 0.00064\n",
      "Validation loss = 0.00215 | mse = 0.00149 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007837648154236376 | Global gradient norm: 60.16\n",
      "Step 23124) Time = 114.682271\n",
      "Train loss = 0.00167 | mse = 0.00115 | KL = 0.00051\n",
      "Validation loss = 0.00172 | mse = 0.00120 | KL = 0.00052\n",
      "================================================\n",
      "Learning rate: 0.0007822139305062592 | Global gradient norm: 60.39\n",
      "Step 23312) Time = 117.658106\n",
      "Train loss = 0.00153 | mse = 0.00108 | KL = 0.00046\n",
      "Validation loss = 0.00155 | mse = 0.00109 | KL = 0.00046\n",
      "================================================\n",
      "Learning rate: 0.0007806660141795874 | Global gradient norm: 60.47\n",
      "Step 23500) Time = 119.060182\n",
      "Train loss = 0.00272 | mse = 0.00208 | KL = 0.00064\n",
      "Validation loss = 0.00264 | mse = 0.00203 | KL = 0.00061\n",
      "================================================\n",
      "Learning rate: 0.0007791212992742658 | Global gradient norm: 60.60\n",
      "Step 23688) Time = 115.672321\n",
      "Train loss = 0.00168 | mse = 0.00124 | KL = 0.00043\n",
      "Validation loss = 0.00164 | mse = 0.00122 | KL = 0.00042\n",
      "================================================\n",
      "Learning rate: 0.0007775796111673117 | Global gradient norm: 60.74\n",
      "Step 23876) Time = 117.446138\n",
      "Train loss = 0.00173 | mse = 0.00135 | KL = 0.00037\n",
      "Validation loss = 0.00171 | mse = 0.00133 | KL = 0.00038\n",
      "================================================\n",
      "Learning rate: 0.0007760408334434032 | Global gradient norm: 60.82\n",
      "Step 24064) Time = 120.287081\n",
      "Train loss = 0.00258 | mse = 0.00163 | KL = 0.00095\n",
      "Validation loss = 0.00261 | mse = 0.00167 | KL = 0.00095\n",
      "================================================\n",
      "Learning rate: 0.0007745051989331841 | Global gradient norm: 60.97\n",
      "Step 24252) Time = 118.475081\n",
      "Train loss = 0.00219 | mse = 0.00168 | KL = 0.00051\n",
      "Validation loss = 0.00214 | mse = 0.00164 | KL = 0.00050\n",
      "================================================\n",
      "Learning rate: 0.0007729725330136716 | Global gradient norm: 61.13\n",
      "Step 24440) Time = 119.077277\n",
      "Train loss = 0.00161 | mse = 0.00125 | KL = 0.00036\n",
      "Validation loss = 0.00164 | mse = 0.00128 | KL = 0.00036\n",
      "================================================\n",
      "Learning rate: 0.0007714430685155094 | Global gradient norm: 61.26\n",
      "Step 24628) Time = 115.080197\n",
      "Train loss = 0.00145 | mse = 0.00110 | KL = 0.00035\n",
      "Validation loss = 0.00144 | mse = 0.00109 | KL = 0.00035\n",
      "================================================\n",
      "Learning rate: 0.0007699165144003928 | Global gradient norm: 61.38\n",
      "Step 24816) Time = 122.289249\n",
      "Train loss = 0.00149 | mse = 0.00114 | KL = 0.00036\n",
      "Validation loss = 0.00149 | mse = 0.00113 | KL = 0.00036\n",
      "================================================\n",
      "Learning rate: 0.0007683929288759828 | Global gradient norm: 61.48\n",
      "Step 25004) Time = 122.843263\n",
      "Train loss = 0.00239 | mse = 0.00154 | KL = 0.00085\n",
      "Validation loss = 0.00240 | mse = 0.00155 | KL = 0.00085\n",
      "================================================\n",
      "Learning rate: 0.0007668724865652621 | Global gradient norm: 61.63\n",
      "Step 25192) Time = 118.633327\n",
      "Train loss = 0.00149 | mse = 0.00116 | KL = 0.00033\n",
      "Validation loss = 0.00151 | mse = 0.00118 | KL = 0.00033\n",
      "================================================\n",
      "Learning rate: 0.0007653549546375871 | Global gradient norm: 61.75\n",
      "Step 25380) Time = 119.908475\n",
      "Train loss = 0.00174 | mse = 0.00135 | KL = 0.00040\n",
      "Validation loss = 0.00183 | mse = 0.00141 | KL = 0.00042\n",
      "================================================\n",
      "Learning rate: 0.0007638404495082796 | Global gradient norm: 61.89\n",
      "Step 25568) Time = 122.279375\n",
      "Train loss = 0.00156 | mse = 0.00115 | KL = 0.00041\n",
      "Validation loss = 0.00161 | mse = 0.00120 | KL = 0.00041\n",
      "================================================\n",
      "Learning rate: 0.0007623289711773396 | Global gradient norm: 62.04\n",
      "Step 25756) Time = 120.260212\n",
      "Train loss = 0.00189 | mse = 0.00148 | KL = 0.00041\n",
      "Validation loss = 0.00189 | mse = 0.00150 | KL = 0.00040\n",
      "================================================\n",
      "Learning rate: 0.0007608204032294452 | Global gradient norm: 62.11\n",
      "Step 25944) Time = 124.029436\n",
      "Train loss = 0.00137 | mse = 0.00099 | KL = 0.00038\n",
      "Validation loss = 0.00132 | mse = 0.00095 | KL = 0.00037\n",
      "================================================\n",
      "Learning rate: 0.0007593148620799184 | Global gradient norm: 62.29\n",
      "Step 26132) Time = 122.129226\n",
      "Train loss = 0.00167 | mse = 0.00130 | KL = 0.00037\n",
      "Validation loss = 0.00160 | mse = 0.00125 | KL = 0.00035\n",
      "================================================\n",
      "Learning rate: 0.000757812347728759 | Global gradient norm: 62.39\n",
      "Step 26320) Time = 122.017488\n",
      "Train loss = 0.00162 | mse = 0.00110 | KL = 0.00052\n",
      "Validation loss = 0.00157 | mse = 0.00106 | KL = 0.00051\n",
      "================================================\n",
      "Learning rate: 0.0007563128019683063 | Global gradient norm: 62.52\n",
      "Step 26508) Time = 122.276848\n",
      "Train loss = 0.00134 | mse = 0.00101 | KL = 0.00033\n",
      "Validation loss = 0.00128 | mse = 0.00096 | KL = 0.00032\n",
      "================================================\n",
      "Learning rate: 0.0007548162247985601 | Global gradient norm: 62.66\n",
      "Step 26696) Time = 120.711457\n",
      "Train loss = 0.00153 | mse = 0.00119 | KL = 0.00034\n",
      "Validation loss = 0.00154 | mse = 0.00120 | KL = 0.00034\n",
      "================================================\n",
      "Learning rate: 0.0007533224998041987 | Global gradient norm: 62.81\n",
      "Step 26884) Time = 131.057383\n",
      "Train loss = 0.00153 | mse = 0.00104 | KL = 0.00049\n",
      "Validation loss = 0.00148 | mse = 0.00099 | KL = 0.00049\n",
      "================================================\n",
      "Learning rate: 0.0007518318598158658 | Global gradient norm: 62.92\n",
      "Step 27072) Time = 123.425051\n",
      "Train loss = 0.00149 | mse = 0.00116 | KL = 0.00033\n",
      "Validation loss = 0.00147 | mse = 0.00115 | KL = 0.00032\n",
      "================================================\n",
      "Learning rate: 0.0007503441302105784 | Global gradient norm: 63.06\n",
      "Step 27260) Time = 118.683690\n",
      "Train loss = 0.00204 | mse = 0.00166 | KL = 0.00038\n",
      "Validation loss = 0.00202 | mse = 0.00165 | KL = 0.00038\n",
      "================================================\n",
      "Learning rate: 0.0007488593109883368 | Global gradient norm: 63.17\n",
      "Step 27448) Time = 131.272537\n",
      "Train loss = 0.00135 | mse = 0.00100 | KL = 0.00035\n",
      "Validation loss = 0.00137 | mse = 0.00102 | KL = 0.00035\n",
      "================================================\n",
      "Learning rate: 0.0007473774021491408 | Global gradient norm: 63.26\n",
      "Step 27636) Time = 124.263256\n",
      "Train loss = 0.00147 | mse = 0.00107 | KL = 0.00040\n",
      "Validation loss = 0.00142 | mse = 0.00102 | KL = 0.00040\n",
      "================================================\n",
      "Learning rate: 0.0007458985201083124 | Global gradient norm: 63.41\n",
      "Step 27824) Time = 119.262649\n",
      "Train loss = 0.00185 | mse = 0.00150 | KL = 0.00036\n",
      "Validation loss = 0.00187 | mse = 0.00150 | KL = 0.00036\n",
      "================================================\n",
      "Learning rate: 0.0007444226066581905 | Global gradient norm: 63.58\n",
      "Step 28012) Time = 123.519964\n",
      "Train loss = 0.00125 | mse = 0.00093 | KL = 0.00032\n",
      "Validation loss = 0.00123 | mse = 0.00092 | KL = 0.00032\n",
      "================================================\n",
      "Learning rate: 0.0007429494871757925 | Global gradient norm: 63.68\n",
      "Step 28200) Time = 120.878493\n",
      "Train loss = 0.00132 | mse = 0.00096 | KL = 0.00036\n",
      "Validation loss = 0.00127 | mse = 0.00092 | KL = 0.00035\n",
      "================================================\n",
      "Learning rate: 0.000741479336284101 | Global gradient norm: 63.87\n",
      "Step 28388) Time = 122.263596\n",
      "Train loss = 0.00128 | mse = 0.00093 | KL = 0.00036\n",
      "Validation loss = 0.00132 | mse = 0.00096 | KL = 0.00036\n",
      "================================================\n",
      "Learning rate: 0.0007400120375677943 | Global gradient norm: 63.98\n",
      "Step 28576) Time = 124.081589\n",
      "Train loss = 0.00127 | mse = 0.00093 | KL = 0.00035\n",
      "Validation loss = 0.00130 | mse = 0.00094 | KL = 0.00036\n",
      "================================================\n",
      "Learning rate: 0.0007385477074421942 | Global gradient norm: 64.11\n",
      "Step 28764) Time = 123.473253\n",
      "Train loss = 0.00251 | mse = 0.00212 | KL = 0.00039\n",
      "Validation loss = 0.00250 | mse = 0.00211 | KL = 0.00039\n",
      "================================================\n",
      "Learning rate: 0.0007370862876996398 | Global gradient norm: 64.22\n",
      "Step 28952) Time = 122.234067\n",
      "Train loss = 0.00156 | mse = 0.00111 | KL = 0.00045\n",
      "Validation loss = 0.00157 | mse = 0.00112 | KL = 0.00045\n",
      "================================================\n",
      "Learning rate: 0.0007356277201324701 | Global gradient norm: 64.33\n",
      "Step 29140) Time = 122.858467\n",
      "Train loss = 0.00126 | mse = 0.00089 | KL = 0.00036\n",
      "Validation loss = 0.00124 | mse = 0.00088 | KL = 0.00036\n",
      "================================================\n",
      "Learning rate: 0.0007341720047406852 | Global gradient norm: 64.46\n",
      "Step 29328) Time = 119.506411\n",
      "Train loss = 0.00133 | mse = 0.00094 | KL = 0.00039\n",
      "Validation loss = 0.00128 | mse = 0.00090 | KL = 0.00037\n",
      "================================================\n",
      "Learning rate: 0.000732719199731946 | Global gradient norm: 64.60\n",
      "Step 29516) Time = 131.037681\n",
      "Train loss = 0.00204 | mse = 0.00163 | KL = 0.00041\n",
      "Validation loss = 0.00210 | mse = 0.00167 | KL = 0.00043\n",
      "================================================\n",
      "Learning rate: 0.0007312693633139133 | Global gradient norm: 64.70\n",
      "Step 29704) Time = 132.110693\n",
      "Train loss = 0.00140 | mse = 0.00106 | KL = 0.00035\n",
      "Validation loss = 0.00135 | mse = 0.00101 | KL = 0.00034\n",
      "================================================\n",
      "Learning rate: 0.0007298222626559436 | Global gradient norm: 64.85\n",
      "Step 29892) Time = 126.022847\n",
      "Train loss = 0.00130 | mse = 0.00095 | KL = 0.00035\n",
      "Validation loss = 0.00126 | mse = 0.00092 | KL = 0.00033\n",
      "================================================\n",
      "Learning rate: 0.0007283780723810196 | Global gradient norm: 64.89\n",
      "Step 30080) Time = 121.511360\n",
      "Train loss = 0.00157 | mse = 0.00116 | KL = 0.00041\n",
      "Validation loss = 0.00153 | mse = 0.00113 | KL = 0.00040\n",
      "================================================\n",
      "Learning rate: 0.0007269367342814803 | Global gradient norm: 65.06\n",
      "Step 30268) Time = 120.282499\n",
      "Train loss = 0.00140 | mse = 0.00097 | KL = 0.00043\n",
      "Validation loss = 0.00146 | mse = 0.00102 | KL = 0.00044\n",
      "================================================\n",
      "Learning rate: 0.0007254982483573258 | Global gradient norm: 65.18\n",
      "Step 30456) Time = 122.459448\n",
      "Train loss = 0.00181 | mse = 0.00129 | KL = 0.00053\n",
      "Validation loss = 0.00177 | mse = 0.00126 | KL = 0.00051\n",
      "================================================\n",
      "Learning rate: 0.0007240626728162169 | Global gradient norm: 65.30\n",
      "Step 30644) Time = 125.900162\n",
      "Train loss = 0.00128 | mse = 0.00084 | KL = 0.00044\n",
      "Validation loss = 0.00127 | mse = 0.00082 | KL = 0.00044\n",
      "================================================\n",
      "Learning rate: 0.0007226298912428319 | Global gradient norm: 65.38\n",
      "Step 30832) Time = 125.417085\n",
      "Train loss = 0.00211 | mse = 0.00108 | KL = 0.00104\n",
      "Validation loss = 0.00209 | mse = 0.00105 | KL = 0.00103\n",
      "================================================\n",
      "Learning rate: 0.0007211999036371708 | Global gradient norm: 65.49\n",
      "Step 31020) Time = 124.916269\n",
      "Train loss = 0.00174 | mse = 0.00132 | KL = 0.00042\n",
      "Validation loss = 0.00173 | mse = 0.00131 | KL = 0.00042\n",
      "================================================\n",
      "Learning rate: 0.0007197727682068944 | Global gradient norm: 65.64\n",
      "Step 31208) Time = 121.245204\n",
      "Train loss = 0.00116 | mse = 0.00086 | KL = 0.00029\n",
      "Validation loss = 0.00114 | mse = 0.00084 | KL = 0.00030\n",
      "================================================\n",
      "Learning rate: 0.0007183485431596637 | Global gradient norm: 65.76\n",
      "Step 31396) Time = 129.888142\n",
      "Train loss = 0.00147 | mse = 0.00114 | KL = 0.00032\n",
      "Validation loss = 0.00143 | mse = 0.00112 | KL = 0.00030\n",
      "================================================\n",
      "Learning rate: 0.0007169270538724959 | Global gradient norm: 65.91\n",
      "Step 31584) Time = 121.683195\n",
      "Train loss = 0.00105 | mse = 0.00078 | KL = 0.00027\n",
      "Validation loss = 0.00105 | mse = 0.00079 | KL = 0.00026\n",
      "================================================\n",
      "Learning rate: 0.000715508358553052 | Global gradient norm: 66.26\n",
      "Step 31772) Time = 115.240188\n",
      "Train loss = 0.00159 | mse = 0.00104 | KL = 0.00055\n",
      "Validation loss = 0.00158 | mse = 0.00103 | KL = 0.00055\n",
      "================================================\n",
      "Learning rate: 0.0007140924572013319 | Global gradient norm: 66.32\n",
      "Step 31960) Time = 120.503263\n",
      "Train loss = 0.00128 | mse = 0.00089 | KL = 0.00039\n",
      "Validation loss = 0.00130 | mse = 0.00090 | KL = 0.00040\n",
      "================================================\n",
      "Learning rate: 0.0007126794662326574 | Global gradient norm: 66.37\n",
      "Step 32148) Time = 119.649312\n",
      "Train loss = 0.00170 | mse = 0.00132 | KL = 0.00038\n",
      "Validation loss = 0.00170 | mse = 0.00132 | KL = 0.00038\n",
      "================================================\n",
      "Learning rate: 0.000711269152816385 | Global gradient norm: 66.45\n",
      "Step 32336) Time = 118.099200\n",
      "Train loss = 0.00110 | mse = 0.00080 | KL = 0.00030\n",
      "Validation loss = 0.00109 | mse = 0.00079 | KL = 0.00030\n",
      "================================================\n",
      "Learning rate: 0.0007098616915754974 | Global gradient norm: 66.46\n",
      "Step 32524) Time = 120.064322\n",
      "Train loss = 0.00298 | mse = 0.00259 | KL = 0.00039\n",
      "Validation loss = 0.00297 | mse = 0.00259 | KL = 0.00039\n",
      "================================================\n",
      "Learning rate: 0.0007084569660946727 | Global gradient norm: 66.53\n",
      "Step 32712) Time = 118.626281\n",
      "Train loss = 0.00122 | mse = 0.00092 | KL = 0.00031\n",
      "Validation loss = 0.00122 | mse = 0.00092 | KL = 0.00030\n",
      "================================================\n",
      "Learning rate: 0.0007070550927892327 | Global gradient norm: 66.59\n",
      "Step 32900) Time = 118.121080\n",
      "Train loss = 0.00145 | mse = 0.00114 | KL = 0.00031\n",
      "Validation loss = 0.00146 | mse = 0.00114 | KL = 0.00032\n",
      "================================================\n",
      "Learning rate: 0.0007056558970361948 | Global gradient norm: 66.67\n",
      "Step 33088) Time = 121.271126\n",
      "Train loss = 0.00116 | mse = 0.00087 | KL = 0.00029\n",
      "Validation loss = 0.00112 | mse = 0.00085 | KL = 0.00027\n",
      "================================================\n",
      "Learning rate: 0.0007042596116662025 | Global gradient norm: 66.71\n",
      "Step 33276) Time = 115.675199\n",
      "Train loss = 0.00139 | mse = 0.00110 | KL = 0.00029\n",
      "Validation loss = 0.00140 | mse = 0.00111 | KL = 0.00029\n",
      "================================================\n",
      "Learning rate: 0.0007028660038486123 | Global gradient norm: 66.77\n",
      "Step 33464) Time = 122.676203\n",
      "Train loss = 0.00104 | mse = 0.00078 | KL = 0.00027\n",
      "Validation loss = 0.00105 | mse = 0.00078 | KL = 0.00027\n",
      "================================================\n",
      "Learning rate: 0.000701475131791085 | Global gradient norm: 66.86\n",
      "Step 33652) Time = 122.238210\n",
      "Train loss = 0.00110 | mse = 0.00084 | KL = 0.00026\n",
      "Validation loss = 0.00111 | mse = 0.00084 | KL = 0.00027\n",
      "================================================\n",
      "Learning rate: 0.0007000870537012815 | Global gradient norm: 66.81\n",
      "Step 33840) Time = 118.723098\n",
      "Train loss = 0.00135 | mse = 0.00095 | KL = 0.00040\n",
      "Validation loss = 0.00134 | mse = 0.00094 | KL = 0.00040\n",
      "================================================\n",
      "Learning rate: 0.000698701711371541 | Global gradient norm: 66.93\n",
      "Step 34028) Time = 121.458082\n",
      "Train loss = 0.00127 | mse = 0.00099 | KL = 0.00028\n",
      "Validation loss = 0.00122 | mse = 0.00094 | KL = 0.00028\n",
      "================================================\n",
      "Learning rate: 0.0006973191048018634 | Global gradient norm: 67.01\n",
      "Step 34216) Time = 119.889246\n",
      "Train loss = 0.00115 | mse = 0.00083 | KL = 0.00032\n",
      "Validation loss = 0.00116 | mse = 0.00085 | KL = 0.00031\n",
      "================================================\n",
      "Learning rate: 0.0006959392339922488 | Global gradient norm: 67.10\n",
      "Step 34404) Time = 116.849122\n",
      "Train loss = 0.00109 | mse = 0.00079 | KL = 0.00030\n",
      "Validation loss = 0.00109 | mse = 0.00079 | KL = 0.00030\n",
      "================================================\n",
      "Learning rate: 0.0006945620407350361 | Global gradient norm: 67.20\n",
      "Step 34592) Time = 126.096086\n",
      "Train loss = 0.00104 | mse = 0.00073 | KL = 0.00031\n",
      "Validation loss = 0.00108 | mse = 0.00076 | KL = 0.00032\n",
      "================================================\n",
      "Learning rate: 0.0006931876414455473 | Global gradient norm: 67.27\n",
      "Step 34780) Time = 127.850327\n",
      "Train loss = 0.00159 | mse = 0.00128 | KL = 0.00031\n",
      "Validation loss = 0.00160 | mse = 0.00129 | KL = 0.00031\n",
      "================================================\n",
      "Learning rate: 0.0006918159779161215 | Global gradient norm: 67.30\n",
      "Step 34968) Time = 123.099084\n",
      "Train loss = 0.00117 | mse = 0.00091 | KL = 0.00026\n",
      "Validation loss = 0.00117 | mse = 0.00090 | KL = 0.00027\n",
      "================================================\n",
      "Learning rate: 0.0006904469919390976 | Global gradient norm: 67.33\n",
      "Step 35156) Time = 129.413124\n",
      "Train loss = 0.00151 | mse = 0.00110 | KL = 0.00042\n",
      "Validation loss = 0.00149 | mse = 0.00108 | KL = 0.00041\n",
      "================================================\n",
      "Learning rate: 0.0006890807417221367 | Global gradient norm: 67.43\n",
      "Step 35344) Time = 128.713324\n",
      "Train loss = 0.00113 | mse = 0.00086 | KL = 0.00027\n",
      "Validation loss = 0.00112 | mse = 0.00085 | KL = 0.00027\n",
      "================================================\n",
      "Learning rate: 0.0006877171690575778 | Global gradient norm: 67.55\n",
      "Step 35532) Time = 121.883082\n",
      "Train loss = 0.00165 | mse = 0.00127 | KL = 0.00038\n",
      "Validation loss = 0.00168 | mse = 0.00129 | KL = 0.00039\n",
      "================================================\n",
      "Learning rate: 0.0006863563321530819 | Global gradient norm: 67.63\n",
      "Step 35720) Time = 51.107159\n",
      "Train loss = 0.00148 | mse = 0.00122 | KL = 0.00026\n",
      "Validation loss = 0.00145 | mse = 0.00120 | KL = 0.00025\n",
      "================================================\n",
      "Learning rate: 0.000684998114593327 | Global gradient norm: 67.66\n",
      "Step 35908) Time = 118.300215\n",
      "Train loss = 0.00113 | mse = 0.00086 | KL = 0.00027\n",
      "Validation loss = 0.00113 | mse = 0.00086 | KL = 0.00026\n",
      "================================================\n",
      "Learning rate: 0.0006836426327936351 | Global gradient norm: 67.75\n",
      "Step 36096) Time = 117.881019\n",
      "Train loss = 0.00117 | mse = 0.00092 | KL = 0.00025\n",
      "Validation loss = 0.00116 | mse = 0.00091 | KL = 0.00026\n",
      "================================================\n",
      "Learning rate: 0.0006822898867540061 | Global gradient norm: 67.87\n",
      "Step 36284) Time = 119.598370\n",
      "Train loss = 0.00093 | mse = 0.00071 | KL = 0.00022\n",
      "Validation loss = 0.00098 | mse = 0.00074 | KL = 0.00023\n",
      "================================================\n",
      "Learning rate: 0.0006809397018514574 | Global gradient norm: 67.88\n",
      "Step 36472) Time = 118.346737\n",
      "Train loss = 0.00136 | mse = 0.00107 | KL = 0.00029\n",
      "Validation loss = 0.00138 | mse = 0.00108 | KL = 0.00030\n",
      "================================================\n",
      "Learning rate: 0.0006795923109166324 | Global gradient norm: 67.93\n",
      "Step 36660) Time = 118.336089\n",
      "Train loss = 0.00137 | mse = 0.00101 | KL = 0.00036\n",
      "Validation loss = 0.00134 | mse = 0.00098 | KL = 0.00035\n",
      "================================================\n",
      "Learning rate: 0.0006782474811188877 | Global gradient norm: 68.00\n",
      "Step 36848) Time = 119.164872\n",
      "Train loss = 0.00122 | mse = 0.00092 | KL = 0.00030\n",
      "Validation loss = 0.00118 | mse = 0.00089 | KL = 0.00030\n",
      "================================================\n",
      "Learning rate: 0.0006769053870812058 | Global gradient norm: 68.06\n",
      "Step 37036) Time = 117.304174\n",
      "Train loss = 0.00102 | mse = 0.00076 | KL = 0.00026\n",
      "Validation loss = 0.00102 | mse = 0.00076 | KL = 0.00027\n",
      "================================================\n",
      "Learning rate: 0.0006755658541806042 | Global gradient norm: 68.13\n",
      "Step 37224) Time = 119.829295\n",
      "Train loss = 0.00135 | mse = 0.00083 | KL = 0.00052\n",
      "Validation loss = 0.00132 | mse = 0.00080 | KL = 0.00052\n",
      "================================================\n",
      "Learning rate: 0.0006742291152477264 | Global gradient norm: 68.23\n",
      "Step 37412) Time = 118.860660\n",
      "Train loss = 0.00109 | mse = 0.00083 | KL = 0.00026\n",
      "Validation loss = 0.00113 | mse = 0.00085 | KL = 0.00027\n",
      "================================================\n",
      "Learning rate: 0.0006728949374519289 | Global gradient norm: 68.36\n",
      "Step 37600) Time = 117.011291\n",
      "Train loss = 0.00200 | mse = 0.00152 | KL = 0.00047\n",
      "Validation loss = 0.00200 | mse = 0.00153 | KL = 0.00047\n",
      "================================================\n",
      "Learning rate: 0.0006715633790008724 | Global gradient norm: 68.41\n",
      "Step 37788) Time = 119.218892\n",
      "Train loss = 0.00248 | mse = 0.00196 | KL = 0.00052\n",
      "Validation loss = 0.00244 | mse = 0.00194 | KL = 0.00051\n",
      "================================================\n",
      "Learning rate: 0.000670234439894557 | Global gradient norm: 68.52\n",
      "Step 37976) Time = 120.776843\n",
      "Train loss = 0.00132 | mse = 0.00106 | KL = 0.00027\n",
      "Validation loss = 0.00131 | mse = 0.00104 | KL = 0.00027\n",
      "================================================\n",
      "Learning rate: 0.0006689082365483046 | Global gradient norm: 68.62\n",
      "Step 38164) Time = 117.887737\n",
      "Train loss = 0.00143 | mse = 0.00112 | KL = 0.00031\n",
      "Validation loss = 0.00141 | mse = 0.00111 | KL = 0.00031\n",
      "================================================\n",
      "Learning rate: 0.0006675845943391323 | Global gradient norm: 68.76\n",
      "Step 38352) Time = 119.823092\n",
      "Train loss = 0.00089 | mse = 0.00067 | KL = 0.00022\n",
      "Validation loss = 0.00089 | mse = 0.00068 | KL = 0.00021\n",
      "================================================\n",
      "Learning rate: 0.0006662635132670403 | Global gradient norm: 68.73\n",
      "Step 38540) Time = 118.857923\n",
      "Train loss = 0.00110 | mse = 0.00078 | KL = 0.00032\n",
      "Validation loss = 0.00110 | mse = 0.00077 | KL = 0.00033\n",
      "================================================\n",
      "Learning rate: 0.0006649451097473502 | Global gradient norm: 68.85\n",
      "Step 38728) Time = 124.459776\n",
      "Train loss = 0.00109 | mse = 0.00078 | KL = 0.00030\n",
      "Validation loss = 0.00107 | mse = 0.00076 | KL = 0.00030\n",
      "================================================\n",
      "Learning rate: 0.0006636292673647404 | Global gradient norm: 68.95\n",
      "Step 38916) Time = 120.872386\n",
      "Train loss = 0.00104 | mse = 0.00078 | KL = 0.00026\n",
      "Validation loss = 0.00100 | mse = 0.00075 | KL = 0.00025\n",
      "================================================\n",
      "Learning rate: 0.0006623161025345325 | Global gradient norm: 69.03\n",
      "Step 39104) Time = 118.806588\n",
      "Train loss = 0.00138 | mse = 0.00106 | KL = 0.00031\n",
      "Validation loss = 0.00141 | mse = 0.00109 | KL = 0.00032\n",
      "================================================\n",
      "Learning rate: 0.0006610055570490658 | Global gradient norm: 69.11\n",
      "Step 39292) Time = 119.689047\n",
      "Train loss = 0.00091 | mse = 0.00068 | KL = 0.00023\n",
      "Validation loss = 0.00095 | mse = 0.00070 | KL = 0.00024\n",
      "================================================\n",
      "Learning rate: 0.0006596975144930184 | Global gradient norm: 69.19\n",
      "Step 39480) Time = 118.768760\n",
      "Train loss = 0.00092 | mse = 0.00068 | KL = 0.00023\n",
      "Validation loss = 0.00092 | mse = 0.00069 | KL = 0.00023\n",
      "================================================\n",
      "Learning rate: 0.0006583920330740511 | Global gradient norm: 69.29\n",
      "Step 39668) Time = 116.342997\n",
      "Train loss = 0.00129 | mse = 0.00106 | KL = 0.00023\n",
      "Validation loss = 0.00127 | mse = 0.00104 | KL = 0.00023\n",
      "================================================\n",
      "Learning rate: 0.0006570892874151468 | Global gradient norm: 69.33\n",
      "Step 39856) Time = 121.396102\n",
      "Train loss = 0.00159 | mse = 0.00131 | KL = 0.00028\n",
      "Validation loss = 0.00159 | mse = 0.00130 | KL = 0.00029\n",
      "================================================\n",
      "Learning rate: 0.00065578892827034 | Global gradient norm: 69.42\n",
      "Step 40044) Time = 119.798198\n",
      "Train loss = 0.00101 | mse = 0.00075 | KL = 0.00025\n",
      "Validation loss = 0.00101 | mse = 0.00077 | KL = 0.00025\n",
      "================================================\n",
      "Learning rate: 0.000654491304885596 | Global gradient norm: 69.47\n",
      "Step 40232) Time = 124.024215\n",
      "Train loss = 0.00108 | mse = 0.00076 | KL = 0.00031\n",
      "Validation loss = 0.00107 | mse = 0.00076 | KL = 0.00031\n",
      "================================================\n",
      "Learning rate: 0.0006531961262226105 | Global gradient norm: 69.54\n",
      "Step 40420) Time = 118.685490\n",
      "Train loss = 0.00105 | mse = 0.00077 | KL = 0.00028\n",
      "Validation loss = 0.00109 | mse = 0.00082 | KL = 0.00028\n",
      "================================================\n",
      "Learning rate: 0.000651903566904366 | Global gradient norm: 69.65\n",
      "Step 40608) Time = 116.688905\n",
      "Train loss = 0.00108 | mse = 0.00085 | KL = 0.00023\n",
      "Validation loss = 0.00107 | mse = 0.00084 | KL = 0.00023\n",
      "================================================\n",
      "Learning rate: 0.0006506135687232018 | Global gradient norm: 69.70\n",
      "Step 40796) Time = 119.161398\n",
      "Train loss = 0.00129 | mse = 0.00098 | KL = 0.00031\n",
      "Validation loss = 0.00126 | mse = 0.00096 | KL = 0.00030\n",
      "================================================\n",
      "Learning rate: 0.0006493261898867786 | Global gradient norm: 69.83\n",
      "Step 40984) Time = 120.207173\n",
      "Train loss = 0.00103 | mse = 0.00079 | KL = 0.00023\n",
      "Validation loss = 0.00104 | mse = 0.00080 | KL = 0.00023\n",
      "================================================\n",
      "Learning rate: 0.0006480411975644529 | Global gradient norm: 69.92\n",
      "Step 41172) Time = 117.961701\n",
      "Train loss = 0.00098 | mse = 0.00073 | KL = 0.00025\n",
      "Validation loss = 0.00100 | mse = 0.00075 | KL = 0.00025\n",
      "================================================\n",
      "Learning rate: 0.0006467588827945292 | Global gradient norm: 69.99\n",
      "Step 41360) Time = 128.227034\n",
      "Train loss = 0.00152 | mse = 0.00128 | KL = 0.00024\n",
      "Validation loss = 0.00151 | mse = 0.00127 | KL = 0.00024\n",
      "================================================\n",
      "Learning rate: 0.0006454790709540248 | Global gradient norm: 70.10\n",
      "Step 41548) Time = 123.287867\n",
      "Train loss = 0.00108 | mse = 0.00085 | KL = 0.00023\n",
      "Validation loss = 0.00106 | mse = 0.00082 | KL = 0.00023\n",
      "================================================\n",
      "Learning rate: 0.0006442017620429397 | Global gradient norm: 70.21\n",
      "Step 41736) Time = 117.740675\n",
      "Train loss = 0.00088 | mse = 0.00067 | KL = 0.00021\n",
      "Validation loss = 0.00090 | mse = 0.00069 | KL = 0.00021\n",
      "================================================\n",
      "Learning rate: 0.0006429270142689347 | Global gradient norm: 70.26\n",
      "Step 41924) Time = 119.519201\n",
      "Train loss = 0.00099 | mse = 0.00076 | KL = 0.00023\n",
      "Validation loss = 0.00098 | mse = 0.00075 | KL = 0.00023\n",
      "================================================\n",
      "Learning rate: 0.0006416547694243491 | Global gradient norm: 70.34\n",
      "Step 42112) Time = 116.663327\n",
      "Train loss = 0.00118 | mse = 0.00091 | KL = 0.00027\n",
      "Validation loss = 0.00118 | mse = 0.00092 | KL = 0.00027\n",
      "================================================\n",
      "Learning rate: 0.0006403850857168436 | Global gradient norm: 70.46\n",
      "Step 42300) Time = 118.913080\n",
      "Train loss = 0.00108 | mse = 0.00068 | KL = 0.00039\n",
      "Validation loss = 0.00109 | mse = 0.00069 | KL = 0.00039\n",
      "================================================\n",
      "Learning rate: 0.0006391179049387574 | Global gradient norm: 70.51\n",
      "Step 42488) Time = 121.342195\n",
      "Train loss = 0.00143 | mse = 0.00119 | KL = 0.00024\n",
      "Validation loss = 0.00147 | mse = 0.00122 | KL = 0.00025\n",
      "================================================\n",
      "Learning rate: 0.0006378531106747687 | Global gradient norm: 70.64\n",
      "Step 42676) Time = 116.427196\n",
      "Train loss = 0.00164 | mse = 0.00132 | KL = 0.00033\n",
      "Validation loss = 0.00161 | mse = 0.00128 | KL = 0.00032\n",
      "================================================\n",
      "Learning rate: 0.000636590993963182 | Global gradient norm: 70.72\n",
      "Step 42864) Time = 129.188160\n",
      "Train loss = 0.00134 | mse = 0.00109 | KL = 0.00025\n",
      "Validation loss = 0.00134 | mse = 0.00108 | KL = 0.00026\n",
      "================================================\n",
      "Learning rate: 0.0006353312637656927 | Global gradient norm: 70.83\n",
      "Step 43052) Time = 122.482131\n",
      "Train loss = 0.00096 | mse = 0.00074 | KL = 0.00022\n",
      "Validation loss = 0.00097 | mse = 0.00075 | KL = 0.00022\n",
      "================================================\n",
      "Learning rate: 0.0006340740364976227 | Global gradient norm: 70.94\n",
      "Step 43240) Time = 116.929139\n",
      "Train loss = 0.00099 | mse = 0.00077 | KL = 0.00023\n",
      "Validation loss = 0.00103 | mse = 0.00079 | KL = 0.00023\n",
      "================================================\n",
      "Learning rate: 0.0006328193703666329 | Global gradient norm: 71.05\n",
      "Step 43428) Time = 120.623196\n",
      "Train loss = 0.00106 | mse = 0.00085 | KL = 0.00021\n",
      "Validation loss = 0.00104 | mse = 0.00084 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0006315671489574015 | Global gradient norm: 71.11\n",
      "Step 43616) Time = 117.197200\n",
      "Train loss = 0.00090 | mse = 0.00068 | KL = 0.00021\n",
      "Validation loss = 0.00091 | mse = 0.00070 | KL = 0.00021\n",
      "================================================\n",
      "Learning rate: 0.0006303173722699285 | Global gradient norm: 71.14\n",
      "Step 43804) Time = 119.088279\n",
      "Train loss = 0.00105 | mse = 0.00080 | KL = 0.00025\n",
      "Validation loss = 0.00112 | mse = 0.00087 | KL = 0.00024\n",
      "================================================\n",
      "Learning rate: 0.0006290700985118747 | Global gradient norm: 71.25\n",
      "Step 43992) Time = 121.005082\n",
      "Train loss = 0.00233 | mse = 0.00203 | KL = 0.00030\n",
      "Validation loss = 0.00233 | mse = 0.00203 | KL = 0.00030\n",
      "================================================\n",
      "Learning rate: 0.0006278252112679183 | Global gradient norm: 71.27\n",
      "Step 44180) Time = 116.100188\n",
      "Train loss = 0.00093 | mse = 0.00066 | KL = 0.00027\n",
      "Validation loss = 0.00094 | mse = 0.00067 | KL = 0.00028\n",
      "================================================\n",
      "Learning rate: 0.0006265829433687031 | Global gradient norm: 71.40\n",
      "Step 44368) Time = 119.800081\n",
      "Train loss = 0.00133 | mse = 0.00108 | KL = 0.00025\n",
      "Validation loss = 0.00135 | mse = 0.00109 | KL = 0.00026\n",
      "================================================\n",
      "Learning rate: 0.0006253430619835854 | Global gradient norm: 71.53\n",
      "Step 44556) Time = 120.707182\n",
      "Train loss = 0.00098 | mse = 0.00074 | KL = 0.00024\n",
      "Validation loss = 0.00099 | mse = 0.00074 | KL = 0.00025\n",
      "================================================\n",
      "Learning rate: 0.000624105567112565 | Global gradient norm: 71.59\n",
      "Step 44744) Time = 118.642199\n",
      "Train loss = 0.00088 | mse = 0.00064 | KL = 0.00023\n",
      "Validation loss = 0.00088 | mse = 0.00064 | KL = 0.00024\n",
      "================================================\n",
      "Learning rate: 0.000622870575170964 | Global gradient norm: 71.75\n",
      "Step 44932) Time = 117.578151\n",
      "Train loss = 0.00083 | mse = 0.00063 | KL = 0.00020\n",
      "Validation loss = 0.00084 | mse = 0.00064 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0006216380279511213 | Global gradient norm: 71.89\n",
      "Step 45120) Time = 115.011184\n",
      "Train loss = 0.00093 | mse = 0.00068 | KL = 0.00025\n",
      "Validation loss = 0.00091 | mse = 0.00066 | KL = 0.00025\n",
      "================================================\n",
      "Learning rate: 0.0006204079836606979 | Global gradient norm: 72.02\n",
      "Step 45308) Time = 118.719080\n",
      "Train loss = 0.00103 | mse = 0.00081 | KL = 0.00022\n",
      "Validation loss = 0.00099 | mse = 0.00077 | KL = 0.00022\n",
      "================================================\n",
      "Learning rate: 0.0006191802676767111 | Global gradient norm: 72.09\n",
      "Step 45496) Time = 126.572300\n",
      "Train loss = 0.00123 | mse = 0.00099 | KL = 0.00024\n",
      "Validation loss = 0.00118 | mse = 0.00094 | KL = 0.00024\n",
      "================================================\n",
      "Learning rate: 0.0006179549964144826 | Global gradient norm: 72.18\n",
      "Step 45684) Time = 124.567633\n",
      "Train loss = 0.00100 | mse = 0.00079 | KL = 0.00021\n",
      "Validation loss = 0.00102 | mse = 0.00081 | KL = 0.00022\n",
      "================================================\n",
      "Learning rate: 0.0006167322280816734 | Global gradient norm: 72.27\n",
      "Step 45872) Time = 128.884280\n",
      "Train loss = 0.00107 | mse = 0.00087 | KL = 0.00020\n",
      "Validation loss = 0.00110 | mse = 0.00090 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0006155117880553007 | Global gradient norm: 72.30\n",
      "Step 46060) Time = 122.610233\n",
      "Train loss = 0.00175 | mse = 0.00143 | KL = 0.00033\n",
      "Validation loss = 0.00180 | mse = 0.00146 | KL = 0.00034\n",
      "================================================\n",
      "Learning rate: 0.0006142938509583473 | Global gradient norm: 72.40\n",
      "Step 46248) Time = 115.824229\n",
      "Train loss = 0.00094 | mse = 0.00075 | KL = 0.00020\n",
      "Validation loss = 0.00094 | mse = 0.00074 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0006130782421678305 | Global gradient norm: 72.47\n",
      "Step 46436) Time = 119.887192\n",
      "Train loss = 0.00096 | mse = 0.00075 | KL = 0.00021\n",
      "Validation loss = 0.00095 | mse = 0.00074 | KL = 0.00021\n",
      "================================================\n",
      "Learning rate: 0.0006118651363067329 | Global gradient norm: 72.54\n",
      "Step 46624) Time = 123.379273\n",
      "Train loss = 0.00090 | mse = 0.00070 | KL = 0.00021\n",
      "Validation loss = 0.00092 | mse = 0.00070 | KL = 0.00021\n",
      "================================================\n",
      "Learning rate: 0.00061065424233675 | Global gradient norm: 72.63\n",
      "Step 46812) Time = 118.742244\n",
      "Train loss = 0.00089 | mse = 0.00068 | KL = 0.00021\n",
      "Validation loss = 0.00091 | mse = 0.00068 | KL = 0.00022\n",
      "================================================\n",
      "Learning rate: 0.0006094459677115083 | Global gradient norm: 72.73\n",
      "Step 47000) Time = 119.760258\n",
      "Train loss = 0.00093 | mse = 0.00074 | KL = 0.00019\n",
      "Validation loss = 0.00097 | mse = 0.00078 | KL = 0.00019\n",
      "================================================\n",
      "Learning rate: 0.0006082399049773812 | Global gradient norm: 72.75\n",
      "Step 47188) Time = 116.005267\n",
      "Train loss = 0.00204 | mse = 0.00146 | KL = 0.00058\n",
      "Validation loss = 0.00208 | mse = 0.00149 | KL = 0.00059\n",
      "================================================\n",
      "Learning rate: 0.0006070363451726735 | Global gradient norm: 72.84\n",
      "Step 47376) Time = 128.219280\n",
      "Train loss = 0.00085 | mse = 0.00065 | KL = 0.00020\n",
      "Validation loss = 0.00084 | mse = 0.00065 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0006058351718820632 | Global gradient norm: 72.92\n",
      "Step 47564) Time = 123.766084\n",
      "Train loss = 0.00085 | mse = 0.00060 | KL = 0.00025\n",
      "Validation loss = 0.00086 | mse = 0.00061 | KL = 0.00025\n",
      "================================================\n",
      "Learning rate: 0.0006046362686902285 | Global gradient norm: 72.98\n",
      "Step 47752) Time = 118.117080\n",
      "Train loss = 0.00111 | mse = 0.00083 | KL = 0.00028\n",
      "Validation loss = 0.00109 | mse = 0.00081 | KL = 0.00028\n",
      "================================================\n",
      "Learning rate: 0.0006034398102201521 | Global gradient norm: 73.05\n",
      "Step 47940) Time = 121.162082\n",
      "Train loss = 0.00095 | mse = 0.00071 | KL = 0.00025\n",
      "Validation loss = 0.00097 | mse = 0.00072 | KL = 0.00025\n",
      "================================================\n",
      "Learning rate: 0.0006022457382641733 | Global gradient norm: 73.14\n",
      "Step 48128) Time = 119.635080\n",
      "Train loss = 0.00088 | mse = 0.00067 | KL = 0.00021\n",
      "Validation loss = 0.00089 | mse = 0.00068 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0006010540528222919 | Global gradient norm: 73.22\n",
      "Step 48316) Time = 117.784080\n",
      "Train loss = 0.00102 | mse = 0.00081 | KL = 0.00021\n",
      "Validation loss = 0.00098 | mse = 0.00078 | KL = 0.00021\n",
      "================================================\n",
      "Learning rate: 0.0005998646374791861 | Global gradient norm: 73.28\n",
      "Step 48504) Time = 129.266087\n",
      "Train loss = 0.00095 | mse = 0.00074 | KL = 0.00021\n",
      "Validation loss = 0.00094 | mse = 0.00074 | KL = 0.00021\n",
      "================================================\n",
      "Learning rate: 0.0005986776086501777 | Global gradient norm: 73.39\n",
      "Step 48692) Time = 127.923319\n",
      "Train loss = 0.00098 | mse = 0.00079 | KL = 0.00019\n",
      "Validation loss = 0.00097 | mse = 0.00078 | KL = 0.00019\n",
      "================================================\n",
      "Learning rate: 0.0005974929081276059 | Global gradient norm: 73.41\n",
      "Step 48880) Time = 122.013168\n",
      "Train loss = 0.00116 | mse = 0.00096 | KL = 0.00020\n",
      "Validation loss = 0.00111 | mse = 0.00091 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0005963105941191316 | Global gradient norm: 73.46\n",
      "Step 49068) Time = 119.410553\n",
      "Train loss = 0.00098 | mse = 0.00078 | KL = 0.00020\n",
      "Validation loss = 0.00099 | mse = 0.00080 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0005951306084170938 | Global gradient norm: 73.58\n",
      "Step 49256) Time = 117.240103\n",
      "Train loss = 0.00085 | mse = 0.00065 | KL = 0.00019\n",
      "Validation loss = 0.00082 | mse = 0.00064 | KL = 0.00018\n",
      "================================================\n",
      "Learning rate: 0.0005939530092291534 | Global gradient norm: 73.66\n",
      "Step 49444) Time = 120.335150\n",
      "Train loss = 0.00093 | mse = 0.00074 | KL = 0.00020\n",
      "Validation loss = 0.00092 | mse = 0.00073 | KL = 0.00019\n",
      "================================================\n",
      "Learning rate: 0.0005927776219323277 | Global gradient norm: 73.67\n",
      "Step 49632) Time = 122.582154\n",
      "Train loss = 0.00099 | mse = 0.00072 | KL = 0.00027\n",
      "Validation loss = 0.00102 | mse = 0.00075 | KL = 0.00027\n",
      "================================================\n",
      "Learning rate: 0.0005916046211495996 | Global gradient norm: 73.82\n",
      "Step 49820) Time = 122.148317\n",
      "Train loss = 0.00077 | mse = 0.00060 | KL = 0.00018\n",
      "Validation loss = 0.00079 | mse = 0.00061 | KL = 0.00019\n",
      "================================================\n",
      "Learning rate: 0.0005904340068809688 | Global gradient norm: 73.87\n",
      "Step 50008) Time = 122.754083\n",
      "Train loss = 0.00115 | mse = 0.00092 | KL = 0.00024\n",
      "Validation loss = 0.00116 | mse = 0.00092 | KL = 0.00024\n",
      "================================================\n",
      "Learning rate: 0.0005892655462957919 | Global gradient norm: 73.96\n",
      "Step 50196) Time = 118.046144\n",
      "Train loss = 0.00088 | mse = 0.00069 | KL = 0.00019\n",
      "Validation loss = 0.00092 | mse = 0.00072 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0005880995886400342 | Global gradient norm: 74.03\n",
      "Step 50384) Time = 118.028191\n",
      "Train loss = 0.00091 | mse = 0.00072 | KL = 0.00019\n",
      "Validation loss = 0.00093 | mse = 0.00073 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0005869357846677303 | Global gradient norm: 74.06\n",
      "Step 50572) Time = 118.952193\n",
      "Train loss = 0.00123 | mse = 0.00097 | KL = 0.00026\n",
      "Validation loss = 0.00120 | mse = 0.00095 | KL = 0.00025\n",
      "================================================\n",
      "Learning rate: 0.0005857743672095239 | Global gradient norm: 74.16\n",
      "Step 50760) Time = 116.549079\n",
      "Train loss = 0.00116 | mse = 0.00090 | KL = 0.00026\n",
      "Validation loss = 0.00115 | mse = 0.00089 | KL = 0.00026\n",
      "================================================\n",
      "Learning rate: 0.0005846152198500931 | Global gradient norm: 74.22\n",
      "Step 50948) Time = 123.679283\n",
      "Train loss = 0.00080 | mse = 0.00062 | KL = 0.00018\n",
      "Validation loss = 0.00081 | mse = 0.00062 | KL = 0.00019\n",
      "================================================\n",
      "Learning rate: 0.0005834584007970989 | Global gradient norm: 74.30\n",
      "Step 51136) Time = 123.652145\n",
      "Train loss = 0.00111 | mse = 0.00088 | KL = 0.00023\n",
      "Validation loss = 0.00111 | mse = 0.00087 | KL = 0.00024\n",
      "================================================\n",
      "Learning rate: 0.0005823037936352193 | Global gradient norm: 74.37\n",
      "Step 51324) Time = 119.833081\n",
      "Train loss = 0.00079 | mse = 0.00060 | KL = 0.00019\n",
      "Validation loss = 0.00076 | mse = 0.00057 | KL = 0.00018\n",
      "================================================\n",
      "Learning rate: 0.0005811515147797763 | Global gradient norm: 74.44\n",
      "Step 51512) Time = 123.931906\n",
      "Train loss = 0.00076 | mse = 0.00056 | KL = 0.00019\n",
      "Validation loss = 0.00076 | mse = 0.00056 | KL = 0.00019\n",
      "================================================\n",
      "Learning rate: 0.0005800015642307699 | Global gradient norm: 74.56\n",
      "Step 51700) Time = 131.260400\n",
      "Train loss = 0.00086 | mse = 0.00067 | KL = 0.00020\n",
      "Validation loss = 0.00084 | mse = 0.00065 | KL = 0.00019\n",
      "================================================\n",
      "Learning rate: 0.000578853883780539 | Global gradient norm: 74.66\n",
      "Step 51888) Time = 129.217862\n",
      "Train loss = 0.00100 | mse = 0.00079 | KL = 0.00021\n",
      "Validation loss = 0.00100 | mse = 0.00079 | KL = 0.00021\n",
      "================================================\n",
      "Learning rate: 0.0005777084152214229 | Global gradient norm: 74.68\n",
      "Step 52076) Time = 126.255646\n",
      "Train loss = 0.00076 | mse = 0.00056 | KL = 0.00020\n",
      "Validation loss = 0.00076 | mse = 0.00056 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0005765652167610824 | Global gradient norm: 74.78\n",
      "Step 52264) Time = 122.382736\n",
      "Train loss = 0.00091 | mse = 0.00072 | KL = 0.00020\n",
      "Validation loss = 0.00090 | mse = 0.00070 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0005754242883995175 | Global gradient norm: 74.85\n",
      "Step 52452) Time = 120.848597\n",
      "Train loss = 0.00083 | mse = 0.00065 | KL = 0.00018\n",
      "Validation loss = 0.00085 | mse = 0.00067 | KL = 0.00018\n",
      "================================================\n",
      "Learning rate: 0.0005742856301367283 | Global gradient norm: 74.91\n",
      "Step 52640) Time = 123.567920\n",
      "Train loss = 0.00085 | mse = 0.00065 | KL = 0.00020\n",
      "Validation loss = 0.00086 | mse = 0.00067 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0005731492419727147 | Global gradient norm: 75.00\n",
      "Step 52828) Time = 119.853255\n",
      "Train loss = 0.00089 | mse = 0.00069 | KL = 0.00020\n",
      "Validation loss = 0.00092 | mse = 0.00072 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0005720150656998158 | Global gradient norm: 75.14\n",
      "Step 53016) Time = 120.014188\n",
      "Train loss = 0.00089 | mse = 0.00072 | KL = 0.00017\n",
      "Validation loss = 0.00087 | mse = 0.00070 | KL = 0.00017\n",
      "================================================\n",
      "Learning rate: 0.0005708831013180315 | Global gradient norm: 75.26\n",
      "Step 53204) Time = 121.665271\n",
      "Train loss = 0.00080 | mse = 0.00064 | KL = 0.00016\n",
      "Validation loss = 0.00079 | mse = 0.00064 | KL = 0.00016\n",
      "================================================\n",
      "Learning rate: 0.000569753407035023 | Global gradient norm: 75.35\n",
      "Step 53392) Time = 117.386292\n",
      "Train loss = 0.00119 | mse = 0.00103 | KL = 0.00016\n",
      "Validation loss = 0.00122 | mse = 0.00106 | KL = 0.00017\n",
      "================================================\n",
      "Learning rate: 0.0005686260410584509 | Global gradient norm: 75.35\n",
      "Step 53580) Time = 120.023225\n",
      "Train loss = 0.00090 | mse = 0.00068 | KL = 0.00022\n",
      "Validation loss = 0.00092 | mse = 0.00070 | KL = 0.00022\n",
      "================================================\n",
      "Learning rate: 0.0005675007705576718 | Global gradient norm: 75.46\n",
      "Step 53768) Time = 117.268150\n",
      "Train loss = 0.00136 | mse = 0.00116 | KL = 0.00021\n",
      "Validation loss = 0.00133 | mse = 0.00113 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0005663778283633292 | Global gradient norm: 75.55\n",
      "Step 53956) Time = 121.697083\n",
      "Train loss = 0.00102 | mse = 0.00064 | KL = 0.00038\n",
      "Validation loss = 0.00100 | mse = 0.00062 | KL = 0.00037\n",
      "================================================\n",
      "Learning rate: 0.0005652570398524404 | Global gradient norm: 75.65\n",
      "Step 54144) Time = 119.845319\n",
      "Train loss = 0.00076 | mse = 0.00059 | KL = 0.00017\n",
      "Validation loss = 0.00078 | mse = 0.00060 | KL = 0.00018\n",
      "================================================\n",
      "Learning rate: 0.0005641385796479881 | Global gradient norm: 75.72\n",
      "Step 54332) Time = 116.679256\n",
      "Train loss = 0.00090 | mse = 0.00072 | KL = 0.00018\n",
      "Validation loss = 0.00089 | mse = 0.00071 | KL = 0.00018\n",
      "================================================\n",
      "Learning rate: 0.0005630222149193287 | Global gradient norm: 75.83\n",
      "Step 54520) Time = 119.866081\n",
      "Train loss = 0.00107 | mse = 0.00081 | KL = 0.00026\n",
      "Validation loss = 0.00110 | mse = 0.00085 | KL = 0.00026\n",
      "================================================\n",
      "Learning rate: 0.0005619081202894449 | Global gradient norm: 75.84\n",
      "Step 54708) Time = 121.474082\n",
      "Train loss = 0.00077 | mse = 0.00057 | KL = 0.00020\n",
      "Validation loss = 0.00079 | mse = 0.00059 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.000560796179343015 | Global gradient norm: 75.95\n",
      "Step 54896) Time = 117.833159\n",
      "Train loss = 0.00120 | mse = 0.00100 | KL = 0.00020\n",
      "Validation loss = 0.00117 | mse = 0.00097 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0005596864502876997 | Global gradient norm: 75.99\n",
      "Step 55084) Time = 120.880122\n",
      "Train loss = 0.00078 | mse = 0.00060 | KL = 0.00018\n",
      "Validation loss = 0.00079 | mse = 0.00061 | KL = 0.00018\n",
      "================================================\n",
      "Learning rate: 0.0005585789913311601 | Global gradient norm: 76.09\n",
      "Step 55272) Time = 118.842252\n",
      "Train loss = 0.00105 | mse = 0.00085 | KL = 0.00020\n",
      "Validation loss = 0.00107 | mse = 0.00087 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0005574736278504133 | Global gradient norm: 76.19\n",
      "Step 55460) Time = 120.830190\n",
      "Train loss = 0.00098 | mse = 0.00081 | KL = 0.00017\n",
      "Validation loss = 0.00097 | mse = 0.00080 | KL = 0.00017\n",
      "================================================\n",
      "Learning rate: 0.0005563705344684422 | Global gradient norm: 76.23\n",
      "Step 55648) Time = 125.664085\n",
      "Train loss = 0.00072 | mse = 0.00054 | KL = 0.00018\n",
      "Validation loss = 0.00073 | mse = 0.00055 | KL = 0.00018\n",
      "================================================\n",
      "Learning rate: 0.000555269478354603 | Global gradient norm: 76.32\n",
      "Step 55836) Time = 125.722127\n",
      "Train loss = 0.00077 | mse = 0.00059 | KL = 0.00019\n",
      "Validation loss = 0.00073 | mse = 0.00055 | KL = 0.00018\n",
      "================================================\n",
      "Learning rate: 0.0005541707505472004 | Global gradient norm: 76.33\n",
      "Step 56024) Time = 123.724191\n",
      "Train loss = 0.00078 | mse = 0.00058 | KL = 0.00020\n",
      "Validation loss = 0.00078 | mse = 0.00058 | KL = 0.00021\n",
      "================================================\n",
      "Learning rate: 0.0005530741764232516 | Global gradient norm: 76.51\n",
      "Step 56212) Time = 120.823203\n",
      "Train loss = 0.00078 | mse = 0.00060 | KL = 0.00018\n",
      "Validation loss = 0.00077 | mse = 0.00059 | KL = 0.00018\n",
      "================================================\n",
      "Learning rate: 0.0005519797559827566 | Global gradient norm: 76.62\n",
      "Step 56400) Time = 118.075109\n",
      "Train loss = 0.00124 | mse = 0.00105 | KL = 0.00019\n",
      "Validation loss = 0.00124 | mse = 0.00105 | KL = 0.00019\n",
      "================================================\n",
      "Learning rate: 0.0005508874892257154 | Global gradient norm: 76.65\n",
      "Step 56588) Time = 121.606008\n",
      "Train loss = 0.00071 | mse = 0.00053 | KL = 0.00018\n",
      "Validation loss = 0.00072 | mse = 0.00055 | KL = 0.00017\n",
      "================================================\n",
      "Learning rate: 0.0005497973179444671 | Global gradient norm: 76.75\n",
      "Step 56776) Time = 119.353117\n",
      "Train loss = 0.00096 | mse = 0.00074 | KL = 0.00022\n",
      "Validation loss = 0.00094 | mse = 0.00072 | KL = 0.00022\n",
      "================================================\n",
      "Learning rate: 0.0005487094167619944 | Global gradient norm: 76.87\n",
      "Step 56964) Time = 128.473198\n",
      "Train loss = 0.00100 | mse = 0.00081 | KL = 0.00019\n",
      "Validation loss = 0.00102 | mse = 0.00083 | KL = 0.00019\n",
      "================================================\n",
      "Learning rate: 0.0005476235528476536 | Global gradient norm: 76.92\n",
      "Step 57152) Time = 122.648211\n",
      "Train loss = 0.00070 | mse = 0.00055 | KL = 0.00015\n",
      "Validation loss = 0.00071 | mse = 0.00055 | KL = 0.00016\n",
      "================================================\n",
      "Learning rate: 0.0005465400172397494 | Global gradient norm: 76.96\n",
      "Step 57340) Time = 116.925149\n",
      "Train loss = 0.00082 | mse = 0.00063 | KL = 0.00018\n",
      "Validation loss = 0.00084 | mse = 0.00065 | KL = 0.00018\n",
      "================================================\n",
      "Learning rate: 0.0005454584606923163 | Global gradient norm: 77.03\n",
      "Step 57528) Time = 127.699212\n",
      "Train loss = 0.00086 | mse = 0.00065 | KL = 0.00021\n",
      "Validation loss = 0.00086 | mse = 0.00065 | KL = 0.00021\n",
      "================================================\n",
      "Learning rate: 0.0005443791160359979 | Global gradient norm: 77.12\n",
      "Step 57716) Time = 120.605174\n",
      "Train loss = 0.00084 | mse = 0.00059 | KL = 0.00025\n",
      "Validation loss = 0.00085 | mse = 0.00060 | KL = 0.00024\n",
      "================================================\n",
      "Learning rate: 0.0005433018668554723 | Global gradient norm: 77.17\n",
      "Step 57904) Time = 114.865148\n",
      "Train loss = 0.00111 | mse = 0.00091 | KL = 0.00021\n",
      "Validation loss = 0.00109 | mse = 0.00090 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0005422268295660615 | Global gradient norm: 77.22\n",
      "Step 58092) Time = 125.397319\n",
      "Train loss = 0.00071 | mse = 0.00053 | KL = 0.00018\n",
      "Validation loss = 0.00075 | mse = 0.00056 | KL = 0.00019\n",
      "================================================\n",
      "Learning rate: 0.0005411537713371217 | Global gradient norm: 77.31\n",
      "Step 58280) Time = 121.288151\n",
      "Train loss = 0.00084 | mse = 0.00066 | KL = 0.00018\n",
      "Validation loss = 0.00085 | mse = 0.00067 | KL = 0.00018\n",
      "================================================\n",
      "Learning rate: 0.0005400829832069576 | Global gradient norm: 77.35\n",
      "Step 58468) Time = 117.806016\n",
      "Train loss = 0.00080 | mse = 0.00061 | KL = 0.00018\n",
      "Validation loss = 0.00079 | mse = 0.00061 | KL = 0.00018\n",
      "================================================\n",
      "Learning rate: 0.0005390142323449254 | Global gradient norm: 77.41\n",
      "Step 58656) Time = 123.048268\n",
      "Train loss = 0.00133 | mse = 0.00098 | KL = 0.00035\n",
      "Validation loss = 0.00131 | mse = 0.00097 | KL = 0.00034\n",
      "================================================\n",
      "Learning rate: 0.000537947635166347 | Global gradient norm: 77.47\n",
      "Step 58844) Time = 119.082264\n",
      "Train loss = 0.00248 | mse = 0.00191 | KL = 0.00057\n",
      "Validation loss = 0.00247 | mse = 0.00190 | KL = 0.00057\n",
      "================================================\n",
      "Learning rate: 0.0005368831334635615 | Global gradient norm: 77.57\n",
      "Step 59032) Time = 121.056061\n",
      "Train loss = 0.00098 | mse = 0.00080 | KL = 0.00018\n",
      "Validation loss = 0.00095 | mse = 0.00078 | KL = 0.00017\n",
      "================================================\n",
      "Learning rate: 0.0005358207272365689 | Global gradient norm: 77.70\n",
      "Step 59220) Time = 121.887250\n",
      "Train loss = 0.00069 | mse = 0.00053 | KL = 0.00016\n",
      "Validation loss = 0.00069 | mse = 0.00053 | KL = 0.00016\n",
      "================================================\n",
      "Learning rate: 0.0005347604164853692 | Global gradient norm: 77.83\n",
      "Step 59408) Time = 120.782116\n",
      "Train loss = 0.00082 | mse = 0.00067 | KL = 0.00015\n",
      "Validation loss = 0.00082 | mse = 0.00067 | KL = 0.00015\n",
      "================================================\n",
      "Learning rate: 0.0005337022012099624 | Global gradient norm: 77.93\n",
      "Step 59596) Time = 120.904064\n",
      "Train loss = 0.00089 | mse = 0.00075 | KL = 0.00015\n",
      "Validation loss = 0.00088 | mse = 0.00073 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0005326461396180093 | Global gradient norm: 77.96\n",
      "Step 59784) Time = 122.084310\n",
      "Train loss = 0.00069 | mse = 0.00054 | KL = 0.00015\n",
      "Validation loss = 0.00068 | mse = 0.00052 | KL = 0.00016\n",
      "================================================\n",
      "Learning rate: 0.0005315921735018492 | Global gradient norm: 77.99\n",
      "Step 59972) Time = 120.137290\n",
      "Train loss = 0.00149 | mse = 0.00121 | KL = 0.00027\n",
      "Validation loss = 0.00146 | mse = 0.00120 | KL = 0.00026\n",
      "================================================\n",
      "Learning rate: 0.0005305401864461601 | Global gradient norm: 78.04\n",
      "Step 60160) Time = 124.243197\n",
      "Train loss = 0.00072 | mse = 0.00054 | KL = 0.00018\n",
      "Validation loss = 0.00070 | mse = 0.00053 | KL = 0.00017\n",
      "================================================\n",
      "Learning rate: 0.0005294903530739248 | Global gradient norm: 78.10\n",
      "Step 60348) Time = 122.524637\n",
      "Train loss = 0.00100 | mse = 0.00080 | KL = 0.00020\n",
      "Validation loss = 0.00099 | mse = 0.00079 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0005284426151774824 | Global gradient norm: 78.16\n",
      "Step 60536) Time = 122.451253\n",
      "Train loss = 0.00075 | mse = 0.00059 | KL = 0.00015\n",
      "Validation loss = 0.00072 | mse = 0.00057 | KL = 0.00015\n",
      "================================================\n",
      "Learning rate: 0.0005273969145491719 | Global gradient norm: 78.20\n",
      "Step 60724) Time = 120.636680\n",
      "Train loss = 0.00072 | mse = 0.00056 | KL = 0.00016\n",
      "Validation loss = 0.00071 | mse = 0.00055 | KL = 0.00016\n",
      "================================================\n",
      "Learning rate: 0.0005263532511889935 | Global gradient norm: 78.26\n",
      "Step 60912) Time = 117.073979\n",
      "Train loss = 0.00080 | mse = 0.00061 | KL = 0.00019\n",
      "Validation loss = 0.00080 | mse = 0.00062 | KL = 0.00018\n",
      "================================================\n",
      "Learning rate: 0.0005253117415122688 | Global gradient norm: 78.30\n",
      "Step 61100) Time = 121.310553\n",
      "Train loss = 0.00086 | mse = 0.00069 | KL = 0.00017\n",
      "Validation loss = 0.00085 | mse = 0.00068 | KL = 0.00017\n",
      "================================================\n",
      "Learning rate: 0.0005242722108960152 | Global gradient norm: 78.36\n",
      "Step 61288) Time = 121.674455\n",
      "Train loss = 0.00067 | mse = 0.00052 | KL = 0.00015\n",
      "Validation loss = 0.00070 | mse = 0.00054 | KL = 0.00016\n",
      "================================================\n",
      "Learning rate: 0.0005232348339632154 | Global gradient norm: 78.46\n",
      "Step 61476) Time = 118.070499\n",
      "Train loss = 0.00076 | mse = 0.00060 | KL = 0.00016\n",
      "Validation loss = 0.00077 | mse = 0.00061 | KL = 0.00017\n",
      "================================================\n",
      "Learning rate: 0.0005221994360908866 | Global gradient norm: 78.51\n",
      "Step 61664) Time = 128.465838\n",
      "Train loss = 0.00072 | mse = 0.00054 | KL = 0.00018\n",
      "Validation loss = 0.00073 | mse = 0.00055 | KL = 0.00018\n",
      "================================================\n",
      "Learning rate: 0.0005211660754866898 | Global gradient norm: 78.57\n",
      "Step 61852) Time = 128.239903\n",
      "Train loss = 0.00076 | mse = 0.00061 | KL = 0.00015\n",
      "Validation loss = 0.00076 | mse = 0.00061 | KL = 0.00015\n",
      "================================================\n",
      "Learning rate: 0.0005201348103582859 | Global gradient norm: 78.61\n",
      "Step 62040) Time = 122.910840\n",
      "Train loss = 0.00075 | mse = 0.00059 | KL = 0.00016\n",
      "Validation loss = 0.00077 | mse = 0.00060 | KL = 0.00017\n",
      "================================================\n",
      "Learning rate: 0.0005191055242903531 | Global gradient norm: 78.65\n",
      "Step 62228) Time = 120.490238\n",
      "Train loss = 0.00084 | mse = 0.00066 | KL = 0.00018\n",
      "Validation loss = 0.00085 | mse = 0.00068 | KL = 0.00017\n",
      "================================================\n",
      "Learning rate: 0.0005180782754905522 | Global gradient norm: 78.72\n",
      "Step 62416) Time = 118.658088\n",
      "Train loss = 0.00064 | mse = 0.00050 | KL = 0.00015\n",
      "Validation loss = 0.00066 | mse = 0.00051 | KL = 0.00015\n",
      "================================================\n",
      "Learning rate: 0.0005170530639588833 | Global gradient norm: 78.80\n",
      "Step 62604) Time = 123.244805\n",
      "Train loss = 0.00077 | mse = 0.00063 | KL = 0.00014\n",
      "Validation loss = 0.00077 | mse = 0.00063 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0005160300061106682 | Global gradient norm: 78.81\n",
      "Step 62792) Time = 123.226182\n",
      "Train loss = 0.00076 | mse = 0.00061 | KL = 0.00015\n",
      "Validation loss = 0.00079 | mse = 0.00064 | KL = 0.00016\n",
      "================================================\n",
      "Learning rate: 0.0005150088109076023 | Global gradient norm: 78.85\n",
      "Step 62980) Time = 119.070245\n",
      "Train loss = 0.00065 | mse = 0.00049 | KL = 0.00015\n",
      "Validation loss = 0.00064 | mse = 0.00049 | KL = 0.00015\n",
      "================================================\n",
      "Learning rate: 0.0005139897693879902 | Global gradient norm: 78.92\n",
      "Step 63168) Time = 121.825881\n",
      "Train loss = 0.00064 | mse = 0.00049 | KL = 0.00015\n",
      "Validation loss = 0.00063 | mse = 0.00048 | KL = 0.00015\n",
      "================================================\n",
      "Learning rate: 0.0005129726487211883 | Global gradient norm: 78.95\n",
      "Step 63356) Time = 119.868642\n",
      "Train loss = 0.00093 | mse = 0.00074 | KL = 0.00019\n",
      "Validation loss = 0.00094 | mse = 0.00075 | KL = 0.00019\n",
      "================================================\n",
      "Learning rate: 0.0005119576235301793 | Global gradient norm: 79.03\n",
      "Step 63544) Time = 122.876986\n",
      "Train loss = 0.00072 | mse = 0.00051 | KL = 0.00021\n",
      "Validation loss = 0.00074 | mse = 0.00052 | KL = 0.00022\n",
      "================================================\n",
      "Learning rate: 0.0005109445191919804 | Global gradient norm: 79.09\n",
      "Step 63732) Time = 122.294473\n",
      "Train loss = 0.00114 | mse = 0.00098 | KL = 0.00017\n",
      "Validation loss = 0.00115 | mse = 0.00098 | KL = 0.00017\n",
      "================================================\n",
      "Learning rate: 0.0005099334521219134 | Global gradient norm: 79.10\n",
      "Step 63920) Time = 119.669976\n",
      "Train loss = 0.00093 | mse = 0.00072 | KL = 0.00020\n",
      "Validation loss = 0.00093 | mse = 0.00073 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0005089243059046566 | Global gradient norm: 79.17\n",
      "Step 64108) Time = 118.681988\n",
      "Train loss = 0.00070 | mse = 0.00054 | KL = 0.00016\n",
      "Validation loss = 0.00071 | mse = 0.00055 | KL = 0.00016\n",
      "================================================\n",
      "Learning rate: 0.0005079173133708537 | Global gradient norm: 79.26\n",
      "Step 64296) Time = 120.065081\n",
      "Train loss = 0.00072 | mse = 0.00052 | KL = 0.00019\n",
      "Validation loss = 0.00072 | mse = 0.00053 | KL = 0.00019\n",
      "================================================\n",
      "Learning rate: 0.0005069121834821999 | Global gradient norm: 79.33\n",
      "Step 64484) Time = 118.017080\n",
      "Train loss = 0.00080 | mse = 0.00055 | KL = 0.00025\n",
      "Validation loss = 0.00080 | mse = 0.00055 | KL = 0.00025\n",
      "================================================\n",
      "Learning rate: 0.000505909149069339 | Global gradient norm: 79.40\n",
      "Step 64672) Time = 119.115149\n",
      "Train loss = 0.00077 | mse = 0.00061 | KL = 0.00016\n",
      "Validation loss = 0.00074 | mse = 0.00059 | KL = 0.00015\n",
      "================================================\n",
      "Learning rate: 0.0005049079773016274 | Global gradient norm: 79.45\n",
      "Step 64860) Time = 121.247082\n",
      "Train loss = 0.00084 | mse = 0.00063 | KL = 0.00021\n",
      "Validation loss = 0.00083 | mse = 0.00063 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0005039089592173696 | Global gradient norm: 79.49\n",
      "Step 65048) Time = 117.705173\n",
      "Train loss = 0.00062 | mse = 0.00047 | KL = 0.00015\n",
      "Validation loss = 0.00060 | mse = 0.00046 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0005029117455706 | Global gradient norm: 79.59\n",
      "Step 65236) Time = 119.396188\n",
      "Train loss = 0.00065 | mse = 0.00051 | KL = 0.00014\n",
      "Validation loss = 0.00066 | mse = 0.00052 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0005019166273996234 | Global gradient norm: 79.65\n",
      "Step 65424) Time = 116.280082\n",
      "Train loss = 0.00062 | mse = 0.00048 | KL = 0.00014\n",
      "Validation loss = 0.00060 | mse = 0.00046 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0005009234300814569 | Global gradient norm: 79.69\n",
      "Step 65612) Time = 121.703083\n",
      "Train loss = 0.00067 | mse = 0.00047 | KL = 0.00020\n",
      "Validation loss = 0.00070 | mse = 0.00050 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0004999321536161005 | Global gradient norm: 79.78\n",
      "Step 65800) Time = 121.061312\n",
      "Train loss = 0.00067 | mse = 0.00052 | KL = 0.00014\n",
      "Validation loss = 0.00069 | mse = 0.00054 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0004989428562112153 | Global gradient norm: 79.81\n",
      "Step 65988) Time = 119.059084\n",
      "Train loss = 0.00089 | mse = 0.00073 | KL = 0.00016\n",
      "Validation loss = 0.00089 | mse = 0.00073 | KL = 0.00016\n",
      "================================================\n",
      "Learning rate: 0.000497955537866801 | Global gradient norm: 79.90\n",
      "Step 66176) Time = 119.071178\n",
      "Train loss = 0.00084 | mse = 0.00069 | KL = 0.00015\n",
      "Validation loss = 0.00084 | mse = 0.00069 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0004969701985828578 | Global gradient norm: 79.95\n",
      "Step 66364) Time = 121.468294\n",
      "Train loss = 0.00076 | mse = 0.00062 | KL = 0.00014\n",
      "Validation loss = 0.00077 | mse = 0.00063 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0004959867801517248 | Global gradient norm: 79.95\n",
      "Step 66552) Time = 118.268147\n",
      "Train loss = 0.00066 | mse = 0.00050 | KL = 0.00016\n",
      "Validation loss = 0.00068 | mse = 0.00051 | KL = 0.00017\n",
      "================================================\n",
      "Learning rate: 0.0004950053407810628 | Global gradient norm: 80.08\n",
      "Step 66740) Time = 120.896082\n",
      "Train loss = 0.00066 | mse = 0.00051 | KL = 0.00015\n",
      "Validation loss = 0.00068 | mse = 0.00053 | KL = 0.00015\n",
      "================================================\n",
      "Learning rate: 0.0004940257640555501 | Global gradient norm: 80.12\n",
      "Step 66928) Time = 118.665080\n",
      "Train loss = 0.00133 | mse = 0.00084 | KL = 0.00050\n",
      "Validation loss = 0.00135 | mse = 0.00084 | KL = 0.00050\n",
      "================================================\n",
      "Learning rate: 0.0004930481663905084 | Global gradient norm: 80.18\n",
      "Step 67116) Time = 129.878215\n",
      "Train loss = 0.00103 | mse = 0.00083 | KL = 0.00021\n",
      "Validation loss = 0.00103 | mse = 0.00081 | KL = 0.00021\n",
      "================================================\n",
      "Learning rate: 0.0004920724895782769 | Global gradient norm: 80.25\n",
      "Step 67304) Time = 124.879254\n",
      "Train loss = 0.00071 | mse = 0.00058 | KL = 0.00014\n",
      "Validation loss = 0.00072 | mse = 0.00058 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0004910987918265164 | Global gradient norm: 80.22\n",
      "Step 67492) Time = 120.882306\n",
      "Train loss = 0.00064 | mse = 0.00048 | KL = 0.00016\n",
      "Validation loss = 0.00067 | mse = 0.00050 | KL = 0.00017\n",
      "================================================\n",
      "Learning rate: 0.000490127014927566 | Global gradient norm: 80.34\n",
      "Step 67680) Time = 124.418128\n",
      "Train loss = 0.00062 | mse = 0.00048 | KL = 0.00014\n",
      "Validation loss = 0.00061 | mse = 0.00048 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.0004891572170890868 | Global gradient norm: 80.38\n",
      "Step 67868) Time = 122.073082\n",
      "Train loss = 0.00063 | mse = 0.00047 | KL = 0.00016\n",
      "Validation loss = 0.00063 | mse = 0.00048 | KL = 0.00015\n",
      "================================================\n",
      "Learning rate: 0.00048818925279192626 | Global gradient norm: 80.39\n",
      "Step 68056) Time = 117.336080\n",
      "Train loss = 0.00069 | mse = 0.00048 | KL = 0.00020\n",
      "Validation loss = 0.00072 | mse = 0.00051 | KL = 0.00021\n",
      "================================================\n",
      "Learning rate: 0.00048722318024374545 | Global gradient norm: 80.47\n",
      "Step 68244) Time = 118.452163\n",
      "Train loss = 0.00126 | mse = 0.00111 | KL = 0.00015\n",
      "Validation loss = 0.00128 | mse = 0.00112 | KL = 0.00016\n",
      "================================================\n",
      "Learning rate: 0.00048625902854837477 | Global gradient norm: 80.56\n",
      "Step 68432) Time = 116.085078\n",
      "Train loss = 0.00065 | mse = 0.00051 | KL = 0.00014\n",
      "Validation loss = 0.00065 | mse = 0.00050 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.00048529679770581424 | Global gradient norm: 80.60\n",
      "Step 68620) Time = 120.079082\n",
      "Train loss = 0.00077 | mse = 0.00064 | KL = 0.00014\n",
      "Validation loss = 0.00079 | mse = 0.00065 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.00048433654592372477 | Global gradient norm: 80.63\n",
      "Step 68808) Time = 123.426084\n",
      "Train loss = 0.00067 | mse = 0.00053 | KL = 0.00014\n",
      "Validation loss = 0.00069 | mse = 0.00055 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0004833781276829541 | Global gradient norm: 80.66\n",
      "Step 68996) Time = 118.685192\n",
      "Train loss = 0.00059 | mse = 0.00045 | KL = 0.00014\n",
      "Validation loss = 0.00060 | mse = 0.00045 | KL = 0.00015\n",
      "================================================\n",
      "Learning rate: 0.00048242160119116306 | Global gradient norm: 80.71\n",
      "Step 69184) Time = 119.082080\n",
      "Train loss = 0.00064 | mse = 0.00049 | KL = 0.00015\n",
      "Validation loss = 0.00065 | mse = 0.00050 | KL = 0.00015\n",
      "================================================\n",
      "Learning rate: 0.00048146696644835174 | Global gradient norm: 80.80\n",
      "Step 69372) Time = 120.059486\n",
      "Train loss = 0.00070 | mse = 0.00051 | KL = 0.00019\n",
      "Validation loss = 0.00070 | mse = 0.00051 | KL = 0.00019\n",
      "================================================\n",
      "Learning rate: 0.0004805142234545201 | Global gradient norm: 80.90\n",
      "Step 69560) Time = 116.681113\n",
      "Train loss = 0.00070 | mse = 0.00056 | KL = 0.00014\n",
      "Validation loss = 0.00072 | mse = 0.00057 | KL = 0.00015\n",
      "================================================\n",
      "Learning rate: 0.0004795634013134986 | Global gradient norm: 80.89\n",
      "Step 69748) Time = 119.675045\n",
      "Train loss = 0.00065 | mse = 0.00051 | KL = 0.00014\n",
      "Validation loss = 0.00066 | mse = 0.00052 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0004786144127137959 | Global gradient norm: 80.98\n",
      "Step 69936) Time = 117.841373\n",
      "Train loss = 0.00059 | mse = 0.00046 | KL = 0.00013\n",
      "Validation loss = 0.00061 | mse = 0.00048 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.00047766731586307287 | Global gradient norm: 81.04\n",
      "Step 70124) Time = 120.512150\n",
      "Train loss = 0.00064 | mse = 0.00051 | KL = 0.00013\n",
      "Validation loss = 0.00062 | mse = 0.00050 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.0004767220816574991 | Global gradient norm: 81.11\n",
      "Step 70312) Time = 121.668521\n",
      "Train loss = 0.00059 | mse = 0.00045 | KL = 0.00013\n",
      "Validation loss = 0.00059 | mse = 0.00046 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.0004757787683047354 | Global gradient norm: 81.13\n",
      "Step 70500) Time = 119.435270\n",
      "Train loss = 0.00091 | mse = 0.00076 | KL = 0.00016\n",
      "Validation loss = 0.00093 | mse = 0.00077 | KL = 0.00016\n",
      "================================================\n",
      "Learning rate: 0.00047483723028562963 | Global gradient norm: 81.17\n",
      "Step 70688) Time = 120.093567\n",
      "Train loss = 0.00070 | mse = 0.00056 | KL = 0.00014\n",
      "Validation loss = 0.00071 | mse = 0.00057 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0004738976713269949 | Global gradient norm: 81.23\n",
      "Step 70876) Time = 121.188234\n",
      "Train loss = 0.00063 | mse = 0.00049 | KL = 0.00014\n",
      "Validation loss = 0.00060 | mse = 0.00047 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.000472959887702018 | Global gradient norm: 81.26\n",
      "Step 71064) Time = 118.486116\n",
      "Train loss = 0.00062 | mse = 0.00047 | KL = 0.00015\n",
      "Validation loss = 0.00061 | mse = 0.00047 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.00047202405403368175 | Global gradient norm: 81.31\n",
      "Step 71252) Time = 49.219550\n",
      "Train loss = 0.00086 | mse = 0.00071 | KL = 0.00015\n",
      "Validation loss = 0.00084 | mse = 0.00069 | KL = 0.00015\n",
      "================================================\n",
      "Learning rate: 0.0004710899374913424 | Global gradient norm: 81.38\n",
      "Step 71440) Time = 117.267214\n",
      "Train loss = 0.00089 | mse = 0.00075 | KL = 0.00014\n",
      "Validation loss = 0.00088 | mse = 0.00074 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0004701577709056437 | Global gradient norm: 81.41\n",
      "Step 71628) Time = 119.627713\n",
      "Train loss = 0.00063 | mse = 0.00050 | KL = 0.00014\n",
      "Validation loss = 0.00062 | mse = 0.00049 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.00046922737965360284 | Global gradient norm: 81.46\n",
      "Step 71816) Time = 117.896352\n",
      "Train loss = 0.00058 | mse = 0.00045 | KL = 0.00013\n",
      "Validation loss = 0.00060 | mse = 0.00047 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.00046829888015054166 | Global gradient norm: 81.47\n",
      "Step 72004) Time = 122.191475\n",
      "Train loss = 0.00124 | mse = 0.00102 | KL = 0.00022\n",
      "Validation loss = 0.00123 | mse = 0.00101 | KL = 0.00022\n",
      "================================================\n",
      "Learning rate: 0.0004673722432926297 | Global gradient norm: 81.52\n",
      "Step 72192) Time = 120.930718\n",
      "Train loss = 0.00060 | mse = 0.00046 | KL = 0.00015\n",
      "Validation loss = 0.00058 | mse = 0.00044 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.00046644738176837564 | Global gradient norm: 81.60\n",
      "Step 72380) Time = 117.885298\n",
      "Train loss = 0.00068 | mse = 0.00055 | KL = 0.00013\n",
      "Validation loss = 0.00067 | mse = 0.00054 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.00046552432468160987 | Global gradient norm: 81.61\n",
      "Step 72568) Time = 120.475090\n",
      "Train loss = 0.00099 | mse = 0.00082 | KL = 0.00017\n",
      "Validation loss = 0.00096 | mse = 0.00080 | KL = 0.00017\n",
      "================================================\n",
      "Learning rate: 0.0004646031593438238 | Global gradient norm: 81.64\n",
      "Step 72756) Time = 118.333816\n",
      "Train loss = 0.00064 | mse = 0.00050 | KL = 0.00014\n",
      "Validation loss = 0.00065 | mse = 0.00050 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.00046368376933969557 | Global gradient norm: 81.71\n",
      "Step 72944) Time = 118.177017\n",
      "Train loss = 0.00066 | mse = 0.00052 | KL = 0.00014\n",
      "Validation loss = 0.00066 | mse = 0.00052 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.00046276621287688613 | Global gradient norm: 81.76\n",
      "Step 73132) Time = 119.790709\n",
      "Train loss = 0.00064 | mse = 0.00050 | KL = 0.00014\n",
      "Validation loss = 0.00066 | mse = 0.00052 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0004618505190592259 | Global gradient norm: 81.82\n",
      "Step 73320) Time = 117.689818\n",
      "Train loss = 0.00060 | mse = 0.00047 | KL = 0.00013\n",
      "Validation loss = 0.00060 | mse = 0.00048 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.00046093660057522357 | Global gradient norm: 81.88\n",
      "Step 73508) Time = 121.261804\n",
      "Train loss = 0.00062 | mse = 0.00049 | KL = 0.00012\n",
      "Validation loss = 0.00062 | mse = 0.00050 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.0004600244574248791 | Global gradient norm: 81.93\n",
      "Step 73696) Time = 119.723836\n",
      "Train loss = 0.00076 | mse = 0.00057 | KL = 0.00018\n",
      "Validation loss = 0.00075 | mse = 0.00057 | KL = 0.00018\n",
      "================================================\n",
      "Learning rate: 0.00045911414781585336 | Global gradient norm: 81.97\n",
      "Step 73884) Time = 122.948914\n",
      "Train loss = 0.00058 | mse = 0.00044 | KL = 0.00014\n",
      "Validation loss = 0.00060 | mse = 0.00046 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0004582056717481464 | Global gradient norm: 82.02\n",
      "Step 74072) Time = 119.856239\n",
      "Train loss = 0.00054 | mse = 0.00042 | KL = 0.00012\n",
      "Validation loss = 0.00057 | mse = 0.00045 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.0004572990001179278 | Global gradient norm: 82.06\n",
      "Step 74260) Time = 117.711964\n",
      "Train loss = 0.00067 | mse = 0.00054 | KL = 0.00013\n",
      "Validation loss = 0.00067 | mse = 0.00054 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.00045639407471753657 | Global gradient norm: 82.09\n",
      "Step 74448) Time = 117.999277\n",
      "Train loss = 0.00064 | mse = 0.00046 | KL = 0.00018\n",
      "Validation loss = 0.00065 | mse = 0.00048 | KL = 0.00017\n",
      "================================================\n",
      "Learning rate: 0.0004554909828584641 | Global gradient norm: 82.21\n",
      "Step 74636) Time = 121.043222\n",
      "Train loss = 0.00086 | mse = 0.00072 | KL = 0.00014\n",
      "Validation loss = 0.00088 | mse = 0.00074 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0004545896372292191 | Global gradient norm: 82.22\n",
      "Step 74824) Time = 116.793192\n",
      "Train loss = 0.00056 | mse = 0.00044 | KL = 0.00013\n",
      "Validation loss = 0.00056 | mse = 0.00044 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0004536900669336319 | Global gradient norm: 82.25\n",
      "Step 75012) Time = 121.854872\n",
      "Train loss = 0.00066 | mse = 0.00046 | KL = 0.00020\n",
      "Validation loss = 0.00066 | mse = 0.00045 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.0004527922719717026 | Global gradient norm: 82.34\n",
      "Step 75200) Time = 119.033198\n",
      "Train loss = 0.00060 | mse = 0.00048 | KL = 0.00012\n",
      "Validation loss = 0.00061 | mse = 0.00049 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.00045189631055109203 | Global gradient norm: 82.35\n",
      "Step 75388) Time = 118.399112\n",
      "Train loss = 0.00089 | mse = 0.00074 | KL = 0.00014\n",
      "Validation loss = 0.00091 | mse = 0.00076 | KL = 0.00015\n",
      "================================================\n",
      "Learning rate: 0.00045100206625647843 | Global gradient norm: 82.41\n",
      "Step 75576) Time = 122.164208\n",
      "Train loss = 0.00071 | mse = 0.00056 | KL = 0.00015\n",
      "Validation loss = 0.00069 | mse = 0.00055 | KL = 0.00015\n",
      "================================================\n",
      "Learning rate: 0.0004501096555031836 | Global gradient norm: 82.51\n",
      "Step 75764) Time = 120.496274\n",
      "Train loss = 0.00058 | mse = 0.00047 | KL = 0.00011\n",
      "Validation loss = 0.00058 | mse = 0.00047 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0004492189618758857 | Global gradient norm: 82.51\n",
      "Step 75952) Time = 119.523130\n",
      "Train loss = 0.00079 | mse = 0.00058 | KL = 0.00021\n",
      "Validation loss = 0.00078 | mse = 0.00058 | KL = 0.00021\n",
      "================================================\n",
      "Learning rate: 0.00044833001447841525 | Global gradient norm: 82.51\n",
      "Step 76140) Time = 117.969080\n",
      "Train loss = 0.00077 | mse = 0.00060 | KL = 0.00016\n",
      "Validation loss = 0.00075 | mse = 0.00060 | KL = 0.00016\n",
      "================================================\n",
      "Learning rate: 0.00044744284241460264 | Global gradient norm: 82.56\n",
      "Step 76328) Time = 114.585115\n",
      "Train loss = 0.00060 | mse = 0.00046 | KL = 0.00014\n",
      "Validation loss = 0.00060 | mse = 0.00047 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.0004465574456844479 | Global gradient norm: 82.62\n",
      "Step 76516) Time = 118.831189\n",
      "Train loss = 0.00080 | mse = 0.00060 | KL = 0.00020\n",
      "Validation loss = 0.00079 | mse = 0.00059 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.000445673824287951 | Global gradient norm: 82.68\n",
      "Step 76704) Time = 117.824080\n",
      "Train loss = 0.00067 | mse = 0.00053 | KL = 0.00014\n",
      "Validation loss = 0.00065 | mse = 0.00052 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.0004447918909136206 | Global gradient norm: 82.72\n",
      "Step 76892) Time = 118.214306\n",
      "Train loss = 0.00069 | mse = 0.00056 | KL = 0.00013\n",
      "Validation loss = 0.00071 | mse = 0.00058 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.0004439117619767785 | Global gradient norm: 82.79\n",
      "Step 77080) Time = 120.514170\n",
      "Train loss = 0.00056 | mse = 0.00044 | KL = 0.00012\n",
      "Validation loss = 0.00053 | mse = 0.00042 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.000443033262854442 | Global gradient norm: 82.85\n",
      "Step 77268) Time = 116.053079\n",
      "Train loss = 0.00091 | mse = 0.00079 | KL = 0.00012\n",
      "Validation loss = 0.00092 | mse = 0.00080 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0004421566263772547 | Global gradient norm: 82.89\n",
      "Step 77456) Time = 122.855083\n",
      "Train loss = 0.00061 | mse = 0.00047 | KL = 0.00014\n",
      "Validation loss = 0.00060 | mse = 0.00046 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0004412816488184035 | Global gradient norm: 82.94\n",
      "Step 77644) Time = 120.198081\n",
      "Train loss = 0.00084 | mse = 0.00070 | KL = 0.00014\n",
      "Validation loss = 0.00081 | mse = 0.00068 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.0004404084465932101 | Global gradient norm: 82.98\n",
      "Step 77832) Time = 117.253186\n",
      "Train loss = 0.00063 | mse = 0.00050 | KL = 0.00013\n",
      "Validation loss = 0.00065 | mse = 0.00052 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.00043953696149401367 | Global gradient norm: 83.06\n",
      "Step 78020) Time = 123.382084\n",
      "Train loss = 0.00060 | mse = 0.00045 | KL = 0.00015\n",
      "Validation loss = 0.00059 | mse = 0.00045 | KL = 0.00015\n",
      "================================================\n",
      "Learning rate: 0.00043866722262464464 | Global gradient norm: 83.08\n",
      "Step 78208) Time = 122.123190\n",
      "Train loss = 0.00057 | mse = 0.00044 | KL = 0.00013\n",
      "Validation loss = 0.00057 | mse = 0.00044 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.00043779914267361164 | Global gradient norm: 83.14\n",
      "Step 78396) Time = 118.558196\n",
      "Train loss = 0.00192 | mse = 0.00143 | KL = 0.00049\n",
      "Validation loss = 0.00191 | mse = 0.00143 | KL = 0.00049\n",
      "================================================\n",
      "Learning rate: 0.00043693286716006696 | Global gradient norm: 83.16\n",
      "Step 78584) Time = 118.917080\n",
      "Train loss = 0.00062 | mse = 0.00048 | KL = 0.00014\n",
      "Validation loss = 0.00060 | mse = 0.00046 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.00043606822146102786 | Global gradient norm: 83.24\n",
      "Step 78772) Time = 117.569317\n",
      "Train loss = 0.00066 | mse = 0.00051 | KL = 0.00016\n",
      "Validation loss = 0.00067 | mse = 0.00050 | KL = 0.00016\n",
      "================================================\n",
      "Learning rate: 0.00043520532199181616 | Global gradient norm: 83.33\n",
      "Step 78960) Time = 120.625325\n",
      "Train loss = 0.00064 | mse = 0.00052 | KL = 0.00011\n",
      "Validation loss = 0.00065 | mse = 0.00054 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00043434416875243187 | Global gradient norm: 83.31\n",
      "Step 79148) Time = 120.236135\n",
      "Train loss = 0.00095 | mse = 0.00071 | KL = 0.00024\n",
      "Validation loss = 0.00093 | mse = 0.00070 | KL = 0.00024\n",
      "================================================\n",
      "Learning rate: 0.0004334846744313836 | Global gradient norm: 83.38\n",
      "Step 79336) Time = 116.858328\n",
      "Train loss = 0.00080 | mse = 0.00069 | KL = 0.00011\n",
      "Validation loss = 0.00081 | mse = 0.00069 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00043262686813250184 | Global gradient norm: 83.43\n",
      "Step 79524) Time = 126.969249\n",
      "Train loss = 0.00075 | mse = 0.00064 | KL = 0.00012\n",
      "Validation loss = 0.00076 | mse = 0.00064 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.00043177074985578656 | Global gradient norm: 83.47\n",
      "Step 79712) Time = 122.245082\n",
      "Train loss = 0.00072 | mse = 0.00059 | KL = 0.00013\n",
      "Validation loss = 0.00072 | mse = 0.00059 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.00043091634870506823 | Global gradient norm: 83.54\n",
      "Step 79900) Time = 118.504081\n",
      "Train loss = 0.00056 | mse = 0.00045 | KL = 0.00011\n",
      "Validation loss = 0.00054 | mse = 0.00043 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0004300636937841773 | Global gradient norm: 83.56\n",
      "Step 80088) Time = 120.680147\n",
      "Train loss = 0.00052 | mse = 0.00041 | KL = 0.00011\n",
      "Validation loss = 0.00054 | mse = 0.00042 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0004292126395739615 | Global gradient norm: 83.55\n",
      "Step 80276) Time = 117.479256\n",
      "Train loss = 0.00074 | mse = 0.00057 | KL = 0.00017\n",
      "Validation loss = 0.00076 | mse = 0.00058 | KL = 0.00018\n",
      "================================================\n",
      "Learning rate: 0.00042836330248974264 | Global gradient norm: 83.66\n",
      "Step 80464) Time = 119.645257\n",
      "Train loss = 0.00060 | mse = 0.00048 | KL = 0.00011\n",
      "Validation loss = 0.00059 | mse = 0.00048 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00042751565342769027 | Global gradient norm: 83.74\n",
      "Step 80652) Time = 123.314122\n",
      "Train loss = 0.00063 | mse = 0.00052 | KL = 0.00011\n",
      "Validation loss = 0.00064 | mse = 0.00053 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00042666972149163485 | Global gradient norm: 83.78\n",
      "Step 80840) Time = 119.875081\n",
      "Train loss = 0.00093 | mse = 0.00072 | KL = 0.00021\n",
      "Validation loss = 0.00095 | mse = 0.00074 | KL = 0.00021\n",
      "================================================\n",
      "Learning rate: 0.00042582539026625454 | Global gradient norm: 83.81\n",
      "Step 81028) Time = 120.417610\n",
      "Train loss = 0.00059 | mse = 0.00048 | KL = 0.00012\n",
      "Validation loss = 0.00061 | mse = 0.00049 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0004249827761668712 | Global gradient norm: 83.84\n",
      "Step 81216) Time = 122.529275\n",
      "Train loss = 0.00053 | mse = 0.00041 | KL = 0.00012\n",
      "Validation loss = 0.00052 | mse = 0.00041 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0004241417918819934 | Global gradient norm: 83.93\n",
      "Step 81404) Time = 120.302627\n",
      "Train loss = 0.00052 | mse = 0.00041 | KL = 0.00011\n",
      "Validation loss = 0.00052 | mse = 0.00041 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0004233025247231126 | Global gradient norm: 83.92\n",
      "Step 81592) Time = 120.262758\n",
      "Train loss = 0.00055 | mse = 0.00044 | KL = 0.00011\n",
      "Validation loss = 0.00054 | mse = 0.00042 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0004224648582749069 | Global gradient norm: 83.96\n",
      "Step 81780) Time = 119.459997\n",
      "Train loss = 0.00055 | mse = 0.00042 | KL = 0.00012\n",
      "Validation loss = 0.00055 | mse = 0.00043 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0004216288507450372 | Global gradient norm: 84.02\n",
      "Step 81968) Time = 128.446670\n",
      "Train loss = 0.00058 | mse = 0.00046 | KL = 0.00011\n",
      "Validation loss = 0.00058 | mse = 0.00046 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.000420794531237334 | Global gradient norm: 84.05\n",
      "Step 82156) Time = 122.517592\n",
      "Train loss = 0.00067 | mse = 0.00056 | KL = 0.00012\n",
      "Validation loss = 0.00067 | mse = 0.00056 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00041996187064796686 | Global gradient norm: 84.14\n",
      "Step 82344) Time = 118.958059\n",
      "Train loss = 0.00102 | mse = 0.00082 | KL = 0.00020\n",
      "Validation loss = 0.00102 | mse = 0.00082 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.00041913081076927483 | Global gradient norm: 84.13\n",
      "Step 82532) Time = 121.317806\n",
      "Train loss = 0.00066 | mse = 0.00054 | KL = 0.00011\n",
      "Validation loss = 0.00068 | mse = 0.00056 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.00041830140980891883 | Global gradient norm: 84.19\n",
      "Step 82720) Time = 121.637972\n",
      "Train loss = 0.00120 | mse = 0.00108 | KL = 0.00012\n",
      "Validation loss = 0.00120 | mse = 0.00108 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0004174737259745598 | Global gradient norm: 84.22\n",
      "Step 82908) Time = 118.646542\n",
      "Train loss = 0.00054 | mse = 0.00043 | KL = 0.00011\n",
      "Validation loss = 0.00055 | mse = 0.00043 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0004166476137470454 | Global gradient norm: 84.24\n",
      "Step 83096) Time = 121.132155\n",
      "Train loss = 0.00056 | mse = 0.00041 | KL = 0.00014\n",
      "Validation loss = 0.00057 | mse = 0.00043 | KL = 0.00015\n",
      "================================================\n",
      "Learning rate: 0.00041582316043786705 | Global gradient norm: 84.29\n",
      "Step 83284) Time = 119.893676\n",
      "Train loss = 0.00065 | mse = 0.00053 | KL = 0.00012\n",
      "Validation loss = 0.00068 | mse = 0.00056 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0004150003078393638 | Global gradient norm: 84.35\n",
      "Step 83472) Time = 119.588987\n",
      "Train loss = 0.00053 | mse = 0.00042 | KL = 0.00011\n",
      "Validation loss = 0.00055 | mse = 0.00044 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0004141791141591966 | Global gradient norm: 84.41\n",
      "Step 83660) Time = 121.231301\n",
      "Train loss = 0.00061 | mse = 0.00047 | KL = 0.00013\n",
      "Validation loss = 0.00062 | mse = 0.00048 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.00041335952118970454 | Global gradient norm: 84.43\n",
      "Step 83848) Time = 117.494684\n",
      "Train loss = 0.00092 | mse = 0.00078 | KL = 0.00015\n",
      "Validation loss = 0.00091 | mse = 0.00077 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.00041254155803471804 | Global gradient norm: 84.46\n",
      "Step 84036) Time = 120.482017\n",
      "Train loss = 0.00053 | mse = 0.00042 | KL = 0.00011\n",
      "Validation loss = 0.00054 | mse = 0.00043 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0004117252246942371 | Global gradient norm: 84.51\n",
      "Step 84224) Time = 130.199857\n",
      "Train loss = 0.00052 | mse = 0.00042 | KL = 0.00010\n",
      "Validation loss = 0.00052 | mse = 0.00041 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0004109104338567704 | Global gradient norm: 84.51\n",
      "Step 84412) Time = 130.979891\n",
      "Train loss = 0.00054 | mse = 0.00043 | KL = 0.00011\n",
      "Validation loss = 0.00053 | mse = 0.00041 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00041009733104147017 | Global gradient norm: 84.57\n",
      "Step 84600) Time = 123.587242\n",
      "Train loss = 0.00051 | mse = 0.00041 | KL = 0.00011\n",
      "Validation loss = 0.00052 | mse = 0.00041 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00040928582893684506 | Global gradient norm: 84.61\n",
      "Step 84788) Time = 120.647665\n",
      "Train loss = 0.00053 | mse = 0.00042 | KL = 0.00011\n",
      "Validation loss = 0.00053 | mse = 0.00043 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00040847595664672554 | Global gradient norm: 84.64\n",
      "Step 84976) Time = 117.628828\n",
      "Train loss = 0.00069 | mse = 0.00056 | KL = 0.00012\n",
      "Validation loss = 0.00069 | mse = 0.00057 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.0004076676268596202 | Global gradient norm: 84.67\n",
      "Step 85164) Time = 121.795244\n",
      "Train loss = 0.00059 | mse = 0.00047 | KL = 0.00011\n",
      "Validation loss = 0.00058 | mse = 0.00047 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00040686092688702047 | Global gradient norm: 84.70\n",
      "Step 85352) Time = 120.135547\n",
      "Train loss = 0.00067 | mse = 0.00055 | KL = 0.00012\n",
      "Validation loss = 0.00067 | mse = 0.00055 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0004060558567289263 | Global gradient norm: 84.76\n",
      "Step 85540) Time = 121.047231\n",
      "Train loss = 0.00057 | mse = 0.00046 | KL = 0.00011\n",
      "Validation loss = 0.00059 | mse = 0.00047 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.00040525232907384634 | Global gradient norm: 84.77\n",
      "Step 85728) Time = 119.301738\n",
      "Train loss = 0.00056 | mse = 0.00045 | KL = 0.00011\n",
      "Validation loss = 0.00056 | mse = 0.00044 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00040445043123327196 | Global gradient norm: 84.84\n",
      "Step 85916) Time = 116.693441\n",
      "Train loss = 0.00057 | mse = 0.00047 | KL = 0.00011\n",
      "Validation loss = 0.00054 | mse = 0.00043 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0004036500758957118 | Global gradient norm: 84.90\n",
      "Step 86104) Time = 119.299163\n",
      "Train loss = 0.00054 | mse = 0.00043 | KL = 0.00012\n",
      "Validation loss = 0.00055 | mse = 0.00043 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0004028513212688267 | Global gradient norm: 84.94\n",
      "Step 86292) Time = 124.144156\n",
      "Train loss = 0.00056 | mse = 0.00042 | KL = 0.00014\n",
      "Validation loss = 0.00056 | mse = 0.00042 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.00040205413824878633 | Global gradient norm: 84.99\n",
      "Step 86480) Time = 119.875380\n",
      "Train loss = 0.00066 | mse = 0.00055 | KL = 0.00011\n",
      "Validation loss = 0.00064 | mse = 0.00054 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00040125861414708197 | Global gradient norm: 85.08\n",
      "Step 86668) Time = 124.482326\n",
      "Train loss = 0.00052 | mse = 0.00042 | KL = 0.00010\n",
      "Validation loss = 0.00050 | mse = 0.00041 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00040046454523690045 | Global gradient norm: 85.09\n",
      "Step 86856) Time = 121.467169\n",
      "Train loss = 0.00053 | mse = 0.00041 | KL = 0.00012\n",
      "Validation loss = 0.00055 | mse = 0.00043 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.00039967207703739405 | Global gradient norm: 85.05\n",
      "Step 87044) Time = 119.111232\n",
      "Train loss = 0.00109 | mse = 0.00087 | KL = 0.00022\n",
      "Validation loss = 0.00109 | mse = 0.00087 | KL = 0.00022\n",
      "================================================\n",
      "Learning rate: 0.00039888120954856277 | Global gradient norm: 85.11\n",
      "Step 87232) Time = 119.782165\n",
      "Train loss = 0.00069 | mse = 0.00058 | KL = 0.00012\n",
      "Validation loss = 0.00067 | mse = 0.00055 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0003980919427704066 | Global gradient norm: 85.16\n",
      "Step 87420) Time = 116.436079\n",
      "Train loss = 0.00051 | mse = 0.00040 | KL = 0.00011\n",
      "Validation loss = 0.00054 | mse = 0.00042 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0003973041893914342 | Global gradient norm: 85.25\n",
      "Step 87608) Time = 124.643203\n",
      "Train loss = 0.00050 | mse = 0.00040 | KL = 0.00010\n",
      "Validation loss = 0.00051 | mse = 0.00041 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00039651794941164553 | Global gradient norm: 85.28\n",
      "Step 87796) Time = 121.254082\n",
      "Train loss = 0.00054 | mse = 0.00043 | KL = 0.00010\n",
      "Validation loss = 0.00053 | mse = 0.00043 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00039573333924636245 | Global gradient norm: 85.36\n",
      "Step 87984) Time = 116.541148\n",
      "Train loss = 0.00048 | mse = 0.00039 | KL = 0.00009\n",
      "Validation loss = 0.00050 | mse = 0.00041 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.00039495027158409357 | Global gradient norm: 85.38\n",
      "Step 88172) Time = 117.644187\n",
      "Train loss = 0.00073 | mse = 0.00062 | KL = 0.00011\n",
      "Validation loss = 0.00075 | mse = 0.00064 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00039416871732100844 | Global gradient norm: 85.39\n",
      "Step 88360) Time = 116.295079\n",
      "Train loss = 0.00067 | mse = 0.00054 | KL = 0.00012\n",
      "Validation loss = 0.00065 | mse = 0.00053 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0003933887055609375 | Global gradient norm: 85.45\n",
      "Step 88548) Time = 122.659129\n",
      "Train loss = 0.00050 | mse = 0.00039 | KL = 0.00010\n",
      "Validation loss = 0.00050 | mse = 0.00040 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0003926102945115417 | Global gradient norm: 85.47\n",
      "Step 88736) Time = 121.695192\n",
      "Train loss = 0.00060 | mse = 0.00049 | KL = 0.00010\n",
      "Validation loss = 0.00060 | mse = 0.00050 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0003918333677574992 | Global gradient norm: 85.51\n",
      "Step 88924) Time = 116.697278\n",
      "Train loss = 0.00062 | mse = 0.00051 | KL = 0.00011\n",
      "Validation loss = 0.00064 | mse = 0.00053 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0003910579835064709 | Global gradient norm: 85.55\n",
      "Step 89112) Time = 119.667193\n",
      "Train loss = 0.00064 | mse = 0.00054 | KL = 0.00011\n",
      "Validation loss = 0.00065 | mse = 0.00054 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0003902841708622873 | Global gradient norm: 85.59\n",
      "Step 89300) Time = 122.077083\n",
      "Train loss = 0.00074 | mse = 0.00060 | KL = 0.00013\n",
      "Validation loss = 0.00073 | mse = 0.00060 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.00038951190072111785 | Global gradient norm: 85.63\n",
      "Step 89488) Time = 119.599292\n",
      "Train loss = 0.00053 | mse = 0.00043 | KL = 0.00010\n",
      "Validation loss = 0.00054 | mse = 0.00043 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0003887411148753017 | Global gradient norm: 85.67\n",
      "Step 89676) Time = 119.036170\n",
      "Train loss = 0.00065 | mse = 0.00054 | KL = 0.00011\n",
      "Validation loss = 0.00061 | mse = 0.00051 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.00038797190063633025 | Global gradient norm: 85.68\n",
      "Step 89864) Time = 121.134194\n",
      "Train loss = 0.00064 | mse = 0.00052 | KL = 0.00012\n",
      "Validation loss = 0.00063 | mse = 0.00051 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.00038720417069271207 | Global gradient norm: 85.71\n",
      "Step 90052) Time = 118.019141\n",
      "Train loss = 0.00061 | mse = 0.00049 | KL = 0.00012\n",
      "Validation loss = 0.00059 | mse = 0.00047 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0003864379250444472 | Global gradient norm: 85.75\n",
      "Step 90240) Time = 117.382143\n",
      "Train loss = 0.00056 | mse = 0.00044 | KL = 0.00012\n",
      "Validation loss = 0.00057 | mse = 0.00045 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.0003856732218991965 | Global gradient norm: 85.79\n",
      "Step 90428) Time = 114.118205\n",
      "Train loss = 0.00064 | mse = 0.00053 | KL = 0.00011\n",
      "Validation loss = 0.00065 | mse = 0.00053 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00038491006125696003 | Global gradient norm: 85.83\n",
      "Step 90616) Time = 120.458204\n",
      "Train loss = 0.00070 | mse = 0.00058 | KL = 0.00012\n",
      "Validation loss = 0.00069 | mse = 0.00058 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0003841484140139073 | Global gradient norm: 85.88\n",
      "Step 90804) Time = 121.678153\n",
      "Train loss = 0.00061 | mse = 0.00048 | KL = 0.00012\n",
      "Validation loss = 0.00057 | mse = 0.00045 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.00038338822196237743 | Global gradient norm: 85.92\n",
      "Step 90992) Time = 118.799264\n",
      "Train loss = 0.00072 | mse = 0.00061 | KL = 0.00011\n",
      "Validation loss = 0.00072 | mse = 0.00062 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00038262957241386175 | Global gradient norm: 85.97\n",
      "Step 91180) Time = 118.838201\n",
      "Train loss = 0.00093 | mse = 0.00071 | KL = 0.00021\n",
      "Validation loss = 0.00092 | mse = 0.00071 | KL = 0.00021\n",
      "================================================\n",
      "Learning rate: 0.0003818724362645298 | Global gradient norm: 85.99\n",
      "Step 91368) Time = 121.297145\n",
      "Train loss = 0.00097 | mse = 0.00077 | KL = 0.00020\n",
      "Validation loss = 0.00097 | mse = 0.00077 | KL = 0.00020\n",
      "================================================\n",
      "Learning rate: 0.00038111675530672073 | Global gradient norm: 86.01\n",
      "Step 91556) Time = 120.451146\n",
      "Train loss = 0.00056 | mse = 0.00045 | KL = 0.00011\n",
      "Validation loss = 0.00056 | mse = 0.00045 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0003803625877480954 | Global gradient norm: 86.07\n",
      "Step 91744) Time = 122.765186\n",
      "Train loss = 0.00055 | mse = 0.00044 | KL = 0.00010\n",
      "Validation loss = 0.00056 | mse = 0.00045 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0003796099335886538 | Global gradient norm: 86.11\n",
      "Step 91932) Time = 118.739263\n",
      "Train loss = 0.00057 | mse = 0.00046 | KL = 0.00011\n",
      "Validation loss = 0.00055 | mse = 0.00044 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0003788587637245655 | Global gradient norm: 86.17\n",
      "Step 92120) Time = 120.247202\n",
      "Train loss = 0.00057 | mse = 0.00046 | KL = 0.00010\n",
      "Validation loss = 0.00058 | mse = 0.00047 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.00037810904905200005 | Global gradient norm: 86.23\n",
      "Step 92308) Time = 121.185495\n",
      "Train loss = 0.00050 | mse = 0.00041 | KL = 0.00010\n",
      "Validation loss = 0.00050 | mse = 0.00040 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.00037736090598627925 | Global gradient norm: 86.26\n",
      "Step 92496) Time = 120.247401\n",
      "Train loss = 0.00051 | mse = 0.00041 | KL = 0.00010\n",
      "Validation loss = 0.00050 | mse = 0.00040 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.00037661410169675946 | Global gradient norm: 86.33\n",
      "Step 92684) Time = 119.745078\n",
      "Train loss = 0.00059 | mse = 0.00048 | KL = 0.00011\n",
      "Validation loss = 0.00060 | mse = 0.00048 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0003758688399102539 | Global gradient norm: 86.39\n",
      "Step 92872) Time = 120.672366\n",
      "Train loss = 0.00053 | mse = 0.00044 | KL = 0.00009\n",
      "Validation loss = 0.00051 | mse = 0.00042 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00037512509152293205 | Global gradient norm: 86.43\n",
      "Step 93060) Time = 119.866424\n",
      "Train loss = 0.00051 | mse = 0.00040 | KL = 0.00011\n",
      "Validation loss = 0.00050 | mse = 0.00039 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00037438279832713306 | Global gradient norm: 86.44\n",
      "Step 93248) Time = 120.638815\n",
      "Train loss = 0.00052 | mse = 0.00042 | KL = 0.00010\n",
      "Validation loss = 0.00051 | mse = 0.00042 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0003736419603228569 | Global gradient norm: 86.45\n",
      "Step 93436) Time = 116.647775\n",
      "Train loss = 0.00064 | mse = 0.00052 | KL = 0.00012\n",
      "Validation loss = 0.00062 | mse = 0.00051 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0003729025484062731 | Global gradient norm: 86.48\n",
      "Step 93624) Time = 121.632024\n",
      "Train loss = 0.00057 | mse = 0.00046 | KL = 0.00011\n",
      "Validation loss = 0.00057 | mse = 0.00046 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0003721646498888731 | Global gradient norm: 86.55\n",
      "Step 93812) Time = 126.447977\n",
      "Train loss = 0.00058 | mse = 0.00049 | KL = 0.00010\n",
      "Validation loss = 0.00058 | mse = 0.00048 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.00037142823566682637 | Global gradient norm: 86.59\n",
      "Step 94000) Time = 127.449018\n",
      "Train loss = 0.00062 | mse = 0.00050 | KL = 0.00012\n",
      "Validation loss = 0.00061 | mse = 0.00049 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.000370693247532472 | Global gradient norm: 86.65\n",
      "Step 94188) Time = 124.250244\n",
      "Train loss = 0.00052 | mse = 0.00041 | KL = 0.00010\n",
      "Validation loss = 0.00052 | mse = 0.00041 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.00036995968548581004 | Global gradient norm: 86.70\n",
      "Step 94376) Time = 121.265517\n",
      "Train loss = 0.00054 | mse = 0.00045 | KL = 0.00009\n",
      "Validation loss = 0.00057 | mse = 0.00048 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.0003692276368383318 | Global gradient norm: 86.72\n",
      "Step 94564) Time = 119.073837\n",
      "Train loss = 0.00049 | mse = 0.00040 | KL = 0.00009\n",
      "Validation loss = 0.00048 | mse = 0.00038 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.000368497014278546 | Global gradient norm: 86.78\n",
      "Step 94752) Time = 121.561365\n",
      "Train loss = 0.00051 | mse = 0.00041 | KL = 0.00010\n",
      "Validation loss = 0.00050 | mse = 0.00040 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.0003677678178064525 | Global gradient norm: 86.74\n",
      "Step 94940) Time = 117.695547\n",
      "Train loss = 0.00076 | mse = 0.00058 | KL = 0.00018\n",
      "Validation loss = 0.00078 | mse = 0.00059 | KL = 0.00019\n",
      "================================================\n",
      "Learning rate: 0.000367040018318221 | Global gradient norm: 86.82\n",
      "Step 95128) Time = 119.652743\n",
      "Train loss = 0.00061 | mse = 0.00050 | KL = 0.00010\n",
      "Validation loss = 0.00060 | mse = 0.00050 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.00036631376133300364 | Global gradient norm: 86.84\n",
      "Step 95316) Time = 125.082221\n",
      "Train loss = 0.00136 | mse = 0.00119 | KL = 0.00017\n",
      "Validation loss = 0.00138 | mse = 0.00120 | KL = 0.00018\n",
      "================================================\n",
      "Learning rate: 0.00036558890133164823 | Global gradient norm: 86.87\n",
      "Step 95504) Time = 122.272177\n",
      "Train loss = 0.00062 | mse = 0.00052 | KL = 0.00010\n",
      "Validation loss = 0.00064 | mse = 0.00054 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0003648654674179852 | Global gradient norm: 86.92\n",
      "Step 95692) Time = 123.084552\n",
      "Train loss = 0.00058 | mse = 0.00048 | KL = 0.00010\n",
      "Validation loss = 0.00059 | mse = 0.00048 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0003641434304881841 | Global gradient norm: 86.98\n",
      "Step 95880) Time = 119.273016\n",
      "Train loss = 0.00048 | mse = 0.00039 | KL = 0.00009\n",
      "Validation loss = 0.00048 | mse = 0.00039 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.0003634228778537363 | Global gradient norm: 87.01\n",
      "Step 96068) Time = 119.662321\n",
      "Train loss = 0.00051 | mse = 0.00042 | KL = 0.00009\n",
      "Validation loss = 0.00051 | mse = 0.00042 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.0003627037222031504 | Global gradient norm: 87.03\n",
      "Step 96256) Time = 124.877688\n",
      "Train loss = 0.00050 | mse = 0.00040 | KL = 0.00010\n",
      "Validation loss = 0.00047 | mse = 0.00038 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00036198602174408734 | Global gradient norm: 87.06\n",
      "Step 96444) Time = 121.247231\n",
      "Train loss = 0.00055 | mse = 0.00041 | KL = 0.00014\n",
      "Validation loss = 0.00055 | mse = 0.00042 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.0003612697182688862 | Global gradient norm: 87.11\n",
      "Step 96632) Time = 121.615646\n",
      "Train loss = 0.00049 | mse = 0.00040 | KL = 0.00009\n",
      "Validation loss = 0.00050 | mse = 0.00040 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00036055484088137746 | Global gradient norm: 87.12\n",
      "Step 96820) Time = 122.287939\n",
      "Train loss = 0.00058 | mse = 0.00049 | KL = 0.00010\n",
      "Validation loss = 0.00059 | mse = 0.00049 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0003598413313739002 | Global gradient norm: 87.18\n",
      "Step 97008) Time = 121.471369\n",
      "Train loss = 0.00055 | mse = 0.00046 | KL = 0.00010\n",
      "Validation loss = 0.00055 | mse = 0.00045 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00035912927705794573 | Global gradient norm: 87.16\n",
      "Step 97196) Time = 120.707642\n",
      "Train loss = 0.00069 | mse = 0.00058 | KL = 0.00012\n",
      "Validation loss = 0.00068 | mse = 0.00057 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00035841864882968366 | Global gradient norm: 87.20\n",
      "Step 97384) Time = 122.699579\n",
      "Train loss = 0.00059 | mse = 0.00048 | KL = 0.00010\n",
      "Validation loss = 0.00058 | mse = 0.00048 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.00035770938848145306 | Global gradient norm: 87.23\n",
      "Step 97572) Time = 119.647090\n",
      "Train loss = 0.00054 | mse = 0.00044 | KL = 0.00010\n",
      "Validation loss = 0.00054 | mse = 0.00044 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.00035700155422091484 | Global gradient norm: 87.26\n",
      "Step 97760) Time = 122.872886\n",
      "Train loss = 0.00050 | mse = 0.00041 | KL = 0.00010\n",
      "Validation loss = 0.00053 | mse = 0.00043 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.00035629511694423854 | Global gradient norm: 87.30\n",
      "Step 97948) Time = 122.298279\n",
      "Train loss = 0.00067 | mse = 0.00055 | KL = 0.00012\n",
      "Validation loss = 0.00068 | mse = 0.00056 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.00035559007665142417 | Global gradient norm: 87.34\n",
      "Step 98136) Time = 119.067241\n",
      "Train loss = 0.00062 | mse = 0.00053 | KL = 0.00009\n",
      "Validation loss = 0.00061 | mse = 0.00052 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00035488640423864126 | Global gradient norm: 87.35\n",
      "Step 98324) Time = 126.666200\n",
      "Train loss = 0.00052 | mse = 0.00042 | KL = 0.00010\n",
      "Validation loss = 0.00054 | mse = 0.00044 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0003541841288097203 | Global gradient norm: 87.41\n",
      "Step 98512) Time = 125.421085\n",
      "Train loss = 0.00049 | mse = 0.00040 | KL = 0.00009\n",
      "Validation loss = 0.00050 | mse = 0.00041 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00035348330857232213 | Global gradient norm: 87.45\n",
      "Step 98700) Time = 121.517082\n",
      "Train loss = 0.00059 | mse = 0.00050 | KL = 0.00009\n",
      "Validation loss = 0.00062 | mse = 0.00053 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.00035278379800729454 | Global gradient norm: 87.45\n",
      "Step 98888) Time = 126.092086\n",
      "Train loss = 0.00059 | mse = 0.00043 | KL = 0.00016\n",
      "Validation loss = 0.00058 | mse = 0.00042 | KL = 0.00016\n",
      "================================================\n",
      "Learning rate: 0.0003520857135299593 | Global gradient norm: 87.50\n",
      "Step 99076) Time = 125.443180\n",
      "Train loss = 0.00055 | mse = 0.00044 | KL = 0.00011\n",
      "Validation loss = 0.00056 | mse = 0.00045 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0003513889678288251 | Global gradient norm: 87.54\n",
      "Step 99264) Time = 121.685169\n",
      "Train loss = 0.00058 | mse = 0.00049 | KL = 0.00009\n",
      "Validation loss = 0.00058 | mse = 0.00049 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.0003506936482153833 | Global gradient norm: 87.58\n",
      "Step 99452) Time = 120.470114\n",
      "Train loss = 0.00072 | mse = 0.00063 | KL = 0.00009\n",
      "Validation loss = 0.00072 | mse = 0.00063 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.0003499997255858034 | Global gradient norm: 87.60\n",
      "Step 99640) Time = 118.690075\n",
      "Train loss = 0.00052 | mse = 0.00043 | KL = 0.00010\n",
      "Validation loss = 0.00052 | mse = 0.00042 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0003493071417324245 | Global gradient norm: 87.65\n",
      "Step 99828) Time = 129.257087\n",
      "Train loss = 0.00049 | mse = 0.00039 | KL = 0.00009\n",
      "Validation loss = 0.00050 | mse = 0.00041 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00034861586755141616 | Global gradient norm: 87.70\n",
      "Step 100016) Time = 129.474335\n",
      "Train loss = 0.00059 | mse = 0.00049 | KL = 0.00010\n",
      "Validation loss = 0.00059 | mse = 0.00049 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.00034792604856193066 | Global gradient norm: 87.70\n",
      "Step 100204) Time = 128.634332\n",
      "Train loss = 0.00050 | mse = 0.00040 | KL = 0.00010\n",
      "Validation loss = 0.00050 | mse = 0.00040 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0003472375974524766 | Global gradient norm: 87.75\n",
      "Step 100392) Time = 121.705262\n",
      "Train loss = 0.00052 | mse = 0.00042 | KL = 0.00010\n",
      "Validation loss = 0.00056 | mse = 0.00046 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0003465504851192236 | Global gradient norm: 87.79\n",
      "Step 100580) Time = 118.889204\n",
      "Train loss = 0.00052 | mse = 0.00043 | KL = 0.00009\n",
      "Validation loss = 0.00050 | mse = 0.00041 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.0003458647115621716 | Global gradient norm: 87.82\n",
      "Step 100768) Time = 123.667224\n",
      "Train loss = 0.00048 | mse = 0.00038 | KL = 0.00010\n",
      "Validation loss = 0.00050 | mse = 0.00040 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0003451802476774901 | Global gradient norm: 87.82\n",
      "Step 100956) Time = 119.468245\n",
      "Train loss = 0.00049 | mse = 0.00040 | KL = 0.00010\n",
      "Validation loss = 0.00047 | mse = 0.00037 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0003444972389843315 | Global gradient norm: 87.86\n",
      "Step 101144) Time = 116.437204\n",
      "Train loss = 0.00056 | mse = 0.00046 | KL = 0.00010\n",
      "Validation loss = 0.00057 | mse = 0.00046 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0003438155399635434 | Global gradient norm: 87.91\n",
      "Step 101332) Time = 119.042121\n",
      "Train loss = 0.00049 | mse = 0.00039 | KL = 0.00010\n",
      "Validation loss = 0.00048 | mse = 0.00039 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.0003431352088227868 | Global gradient norm: 87.94\n",
      "Step 101520) Time = 116.678205\n",
      "Train loss = 0.00050 | mse = 0.00042 | KL = 0.00009\n",
      "Validation loss = 0.00050 | mse = 0.00041 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00034245618735440075 | Global gradient norm: 87.99\n",
      "Step 101708) Time = 120.468082\n",
      "Train loss = 0.00059 | mse = 0.00049 | KL = 0.00011\n",
      "Validation loss = 0.00059 | mse = 0.00048 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00034177853376604617 | Global gradient norm: 88.01\n",
      "Step 101896) Time = 118.736214\n",
      "Train loss = 0.00063 | mse = 0.00054 | KL = 0.00010\n",
      "Validation loss = 0.00065 | mse = 0.00054 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0003411022189538926 | Global gradient norm: 88.05\n",
      "Step 102084) Time = 117.243126\n",
      "Train loss = 0.00053 | mse = 0.00041 | KL = 0.00013\n",
      "Validation loss = 0.00053 | mse = 0.00040 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.0003404272720217705 | Global gradient norm: 88.06\n",
      "Step 102272) Time = 124.310084\n",
      "Train loss = 0.00053 | mse = 0.00044 | KL = 0.00010\n",
      "Validation loss = 0.00053 | mse = 0.00043 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.000339753576554358 | Global gradient norm: 88.10\n",
      "Step 102460) Time = 120.059201\n",
      "Train loss = 0.00097 | mse = 0.00083 | KL = 0.00014\n",
      "Validation loss = 0.00096 | mse = 0.00082 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.00033908127807080746 | Global gradient norm: 88.14\n",
      "Step 102648) Time = 117.066149\n",
      "Train loss = 0.00057 | mse = 0.00047 | KL = 0.00009\n",
      "Validation loss = 0.00056 | mse = 0.00047 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.0003384103183634579 | Global gradient norm: 88.16\n",
      "Step 102836) Time = 119.845273\n",
      "Train loss = 0.00049 | mse = 0.00040 | KL = 0.00009\n",
      "Validation loss = 0.00050 | mse = 0.00041 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.0003377406392246485 | Global gradient norm: 88.18\n",
      "Step 103024) Time = 115.279308\n",
      "Train loss = 0.00057 | mse = 0.00047 | KL = 0.00010\n",
      "Validation loss = 0.00054 | mse = 0.00045 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.00033707235706970096 | Global gradient norm: 88.23\n",
      "Step 103212) Time = 122.334147\n",
      "Train loss = 0.00055 | mse = 0.00046 | KL = 0.00009\n",
      "Validation loss = 0.00056 | mse = 0.00047 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00033640535548329353 | Global gradient norm: 88.26\n",
      "Step 103400) Time = 121.047586\n",
      "Train loss = 0.00054 | mse = 0.00043 | KL = 0.00011\n",
      "Validation loss = 0.00054 | mse = 0.00043 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00033573966356925666 | Global gradient norm: 88.29\n",
      "Step 103588) Time = 118.085237\n",
      "Train loss = 0.00049 | mse = 0.00040 | KL = 0.00010\n",
      "Validation loss = 0.00049 | mse = 0.00039 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.00033507528132759035 | Global gradient norm: 88.32\n",
      "Step 103776) Time = 120.479766\n",
      "Train loss = 0.00049 | mse = 0.00039 | KL = 0.00010\n",
      "Validation loss = 0.00049 | mse = 0.00039 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.00033441223786212504 | Global gradient norm: 88.38\n",
      "Step 103964) Time = 126.276177\n",
      "Train loss = 0.00049 | mse = 0.00040 | KL = 0.00008\n",
      "Validation loss = 0.00050 | mse = 0.00041 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00033375047496519983 | Global gradient norm: 88.40\n",
      "Step 104152) Time = 126.063838\n",
      "Train loss = 0.00103 | mse = 0.00090 | KL = 0.00013\n",
      "Validation loss = 0.00104 | mse = 0.00091 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.00033309002174064517 | Global gradient norm: 88.44\n",
      "Step 104340) Time = 122.833205\n",
      "Train loss = 0.00051 | mse = 0.00043 | KL = 0.00008\n",
      "Validation loss = 0.00051 | mse = 0.00043 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0003324309072922915 | Global gradient norm: 88.49\n",
      "Step 104528) Time = 120.315962\n",
      "Train loss = 0.00210 | mse = 0.00198 | KL = 0.00013\n",
      "Validation loss = 0.00210 | mse = 0.00197 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0003317731025163084 | Global gradient norm: 88.45\n",
      "Step 104716) Time = 119.888592\n",
      "Train loss = 0.00051 | mse = 0.00042 | KL = 0.00009\n",
      "Validation loss = 0.00051 | mse = 0.00041 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00033111657830886543 | Global gradient norm: 88.50\n",
      "Step 104904) Time = 122.259835\n",
      "Train loss = 0.00047 | mse = 0.00038 | KL = 0.00009\n",
      "Validation loss = 0.00047 | mse = 0.00038 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.0003304613346699625 | Global gradient norm: 88.56\n",
      "Step 105092) Time = 119.085572\n",
      "Train loss = 0.00054 | mse = 0.00045 | KL = 0.00008\n",
      "Validation loss = 0.00052 | mse = 0.00044 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0003298074589110911 | Global gradient norm: 88.59\n",
      "Step 105280) Time = 123.070795\n",
      "Train loss = 0.00052 | mse = 0.00040 | KL = 0.00012\n",
      "Validation loss = 0.00054 | mse = 0.00042 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.00032915480551309884 | Global gradient norm: 88.65\n",
      "Step 105468) Time = 122.273777\n",
      "Train loss = 0.00050 | mse = 0.00042 | KL = 0.00008\n",
      "Validation loss = 0.00051 | mse = 0.00043 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0003285034908913076 | Global gradient norm: 88.63\n",
      "Step 105656) Time = 118.491265\n",
      "Train loss = 0.00049 | mse = 0.00040 | KL = 0.00010\n",
      "Validation loss = 0.00048 | mse = 0.00038 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00032785339863039553 | Global gradient norm: 88.67\n",
      "Step 105844) Time = 123.859850\n",
      "Train loss = 0.00054 | mse = 0.00046 | KL = 0.00008\n",
      "Validation loss = 0.00055 | mse = 0.00047 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00032720467424951494 | Global gradient norm: 88.69\n",
      "Step 106032) Time = 120.072326\n",
      "Train loss = 0.00052 | mse = 0.00043 | KL = 0.00009\n",
      "Validation loss = 0.00052 | mse = 0.00044 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.000326557201333344 | Global gradient norm: 88.72\n",
      "Step 106220) Time = 117.835136\n",
      "Train loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "Validation loss = 0.00047 | mse = 0.00038 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0003259110089857131 | Global gradient norm: 88.74\n",
      "Step 106408) Time = 120.895663\n",
      "Train loss = 0.00047 | mse = 0.00039 | KL = 0.00008\n",
      "Validation loss = 0.00047 | mse = 0.00039 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00032526603899896145 | Global gradient norm: 88.77\n",
      "Step 106596) Time = 119.085714\n",
      "Train loss = 0.00049 | mse = 0.00040 | KL = 0.00008\n",
      "Validation loss = 0.00048 | mse = 0.00040 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00032462243689224124 | Global gradient norm: 88.77\n",
      "Step 106784) Time = 49.545347\n",
      "Train loss = 0.00054 | mse = 0.00042 | KL = 0.00012\n",
      "Validation loss = 0.00055 | mse = 0.00043 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0003239800571464002 | Global gradient norm: 88.83\n",
      "Step 106972) Time = 118.345810\n",
      "Train loss = 0.00048 | mse = 0.00039 | KL = 0.00008\n",
      "Validation loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00032333898707292974 | Global gradient norm: 88.82\n",
      "Step 107160) Time = 117.581890\n",
      "Train loss = 0.00076 | mse = 0.00052 | KL = 0.00024\n",
      "Validation loss = 0.00075 | mse = 0.00052 | KL = 0.00023\n",
      "================================================\n",
      "Learning rate: 0.000322699110256508 | Global gradient norm: 88.85\n",
      "Step 107348) Time = 115.802382\n",
      "Train loss = 0.00062 | mse = 0.00053 | KL = 0.00009\n",
      "Validation loss = 0.00061 | mse = 0.00052 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00032206057221628726 | Global gradient norm: 88.88\n",
      "Step 107536) Time = 125.092973\n",
      "Train loss = 0.00051 | mse = 0.00042 | KL = 0.00009\n",
      "Validation loss = 0.00051 | mse = 0.00043 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00032142328564077616 | Global gradient norm: 88.89\n",
      "Step 107724) Time = 118.129487\n",
      "Train loss = 0.00105 | mse = 0.00092 | KL = 0.00012\n",
      "Validation loss = 0.00104 | mse = 0.00092 | KL = 0.00012\n",
      "================================================\n",
      "Learning rate: 0.0003207872505299747 | Global gradient norm: 88.92\n",
      "Step 107912) Time = 116.732443\n",
      "Train loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "Validation loss = 0.00045 | mse = 0.00037 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0003201524668838829 | Global gradient norm: 88.94\n",
      "Step 108100) Time = 119.820098\n",
      "Train loss = 0.00050 | mse = 0.00041 | KL = 0.00009\n",
      "Validation loss = 0.00050 | mse = 0.00041 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00031951890559867024 | Global gradient norm: 88.97\n",
      "Step 108288) Time = 118.060632\n",
      "Train loss = 0.00058 | mse = 0.00048 | KL = 0.00009\n",
      "Validation loss = 0.00056 | mse = 0.00047 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.0003188866830896586 | Global gradient norm: 89.00\n",
      "Step 108476) Time = 120.067605\n",
      "Train loss = 0.00045 | mse = 0.00037 | KL = 0.00008\n",
      "Validation loss = 0.00044 | mse = 0.00036 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0003182556538376957 | Global gradient norm: 89.04\n",
      "Step 108664) Time = 121.022253\n",
      "Train loss = 0.00048 | mse = 0.00039 | KL = 0.00009\n",
      "Validation loss = 0.00048 | mse = 0.00039 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00031762587605044246 | Global gradient norm: 89.08\n",
      "Step 108852) Time = 118.871398\n",
      "Train loss = 0.00061 | mse = 0.00047 | KL = 0.00014\n",
      "Validation loss = 0.00062 | mse = 0.00048 | KL = 0.00014\n",
      "================================================\n",
      "Learning rate: 0.0003169972915202379 | Global gradient norm: 89.12\n",
      "Step 109040) Time = 119.699591\n",
      "Train loss = 0.00070 | mse = 0.00061 | KL = 0.00008\n",
      "Validation loss = 0.00071 | mse = 0.00062 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00031637007487006485 | Global gradient norm: 89.14\n",
      "Step 109228) Time = 122.411011\n",
      "Train loss = 0.00067 | mse = 0.00059 | KL = 0.00008\n",
      "Validation loss = 0.00070 | mse = 0.00061 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00031574402237311006 | Global gradient norm: 89.14\n",
      "Step 109416) Time = 118.738081\n",
      "Train loss = 0.00060 | mse = 0.00047 | KL = 0.00012\n",
      "Validation loss = 0.00063 | mse = 0.00050 | KL = 0.00013\n",
      "================================================\n",
      "Learning rate: 0.0003151192213408649 | Global gradient norm: 89.18\n",
      "Step 109604) Time = 121.304082\n",
      "Train loss = 0.00049 | mse = 0.00040 | KL = 0.00008\n",
      "Validation loss = 0.00050 | mse = 0.00041 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00031449570087715983 | Global gradient norm: 89.21\n",
      "Step 109792) Time = 119.430233\n",
      "Train loss = 0.00047 | mse = 0.00039 | KL = 0.00009\n",
      "Validation loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00031387334456667304 | Global gradient norm: 89.25\n",
      "Step 109980) Time = 118.360079\n",
      "Train loss = 0.00049 | mse = 0.00041 | KL = 0.00008\n",
      "Validation loss = 0.00048 | mse = 0.00039 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0003132522397208959 | Global gradient norm: 89.29\n",
      "Step 110168) Time = 118.480082\n",
      "Train loss = 0.00044 | mse = 0.00037 | KL = 0.00008\n",
      "Validation loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0003126323572359979 | Global gradient norm: 89.33\n",
      "Step 110356) Time = 114.695077\n",
      "Train loss = 0.00047 | mse = 0.00037 | KL = 0.00009\n",
      "Validation loss = 0.00049 | mse = 0.00040 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0003120137262158096 | Global gradient norm: 89.36\n",
      "Step 110544) Time = 118.103233\n",
      "Train loss = 0.00047 | mse = 0.00039 | KL = 0.00008\n",
      "Validation loss = 0.00048 | mse = 0.00040 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00031139628845267 | Global gradient norm: 89.39\n",
      "Step 110732) Time = 122.793211\n",
      "Train loss = 0.00056 | mse = 0.00048 | KL = 0.00009\n",
      "Validation loss = 0.00055 | mse = 0.00047 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00031078010215424 | Global gradient norm: 89.41\n",
      "Step 110920) Time = 120.068243\n",
      "Train loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "Validation loss = 0.00047 | mse = 0.00039 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00031016510911285877 | Global gradient norm: 89.45\n",
      "Step 111108) Time = 118.192080\n",
      "Train loss = 0.00045 | mse = 0.00037 | KL = 0.00008\n",
      "Validation loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00030955136753618717 | Global gradient norm: 89.49\n",
      "Step 111296) Time = 115.635078\n",
      "Train loss = 0.00049 | mse = 0.00041 | KL = 0.00008\n",
      "Validation loss = 0.00046 | mse = 0.00039 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0003089388192165643 | Global gradient norm: 89.48\n",
      "Step 111484) Time = 129.063204\n",
      "Train loss = 0.00047 | mse = 0.00039 | KL = 0.00008\n",
      "Validation loss = 0.00048 | mse = 0.00040 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0003083274932578206 | Global gradient norm: 89.52\n",
      "Step 111672) Time = 120.873208\n",
      "Train loss = 0.00053 | mse = 0.00045 | KL = 0.00008\n",
      "Validation loss = 0.00054 | mse = 0.00045 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00030771736055612564 | Global gradient norm: 89.56\n",
      "Step 111860) Time = 116.247124\n",
      "Train loss = 0.00043 | mse = 0.00036 | KL = 0.00008\n",
      "Validation loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0003071084793191403 | Global gradient norm: 89.59\n",
      "Step 112048) Time = 120.127129\n",
      "Train loss = 0.00053 | mse = 0.00045 | KL = 0.00008\n",
      "Validation loss = 0.00053 | mse = 0.00045 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0003065007913392037 | Global gradient norm: 89.65\n",
      "Step 112236) Time = 120.062081\n",
      "Train loss = 0.00047 | mse = 0.00039 | KL = 0.00008\n",
      "Validation loss = 0.00045 | mse = 0.00038 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00030589423840865493 | Global gradient norm: 89.68\n",
      "Step 112424) Time = 119.584273\n",
      "Train loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "Validation loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00030528896604664624 | Global gradient norm: 89.74\n",
      "Step 112612) Time = 124.198084\n",
      "Train loss = 0.00067 | mse = 0.00059 | KL = 0.00008\n",
      "Validation loss = 0.00067 | mse = 0.00059 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00030468482873402536 | Global gradient norm: 89.70\n",
      "Step 112800) Time = 120.997210\n",
      "Train loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "Validation loss = 0.00046 | mse = 0.00037 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0003040819428861141 | Global gradient norm: 89.76\n",
      "Step 112988) Time = 123.691200\n",
      "Train loss = 0.00076 | mse = 0.00068 | KL = 0.00008\n",
      "Validation loss = 0.00077 | mse = 0.00068 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00030348016298376024 | Global gradient norm: 89.75\n",
      "Step 113176) Time = 124.124084\n",
      "Train loss = 0.00044 | mse = 0.00036 | KL = 0.00008\n",
      "Validation loss = 0.00045 | mse = 0.00037 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00030287966364994645 | Global gradient norm: 89.80\n",
      "Step 113364) Time = 122.234135\n",
      "Train loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "Validation loss = 0.00047 | mse = 0.00039 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00030228032846935093 | Global gradient norm: 89.84\n",
      "Step 113552) Time = 120.248200\n",
      "Train loss = 0.00075 | mse = 0.00065 | KL = 0.00010\n",
      "Validation loss = 0.00075 | mse = 0.00066 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.00030168212833814323 | Global gradient norm: 89.86\n",
      "Step 113740) Time = 119.057120\n",
      "Train loss = 0.00056 | mse = 0.00048 | KL = 0.00008\n",
      "Validation loss = 0.00057 | mse = 0.00049 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00030108512146398425 | Global gradient norm: 89.84\n",
      "Step 113928) Time = 114.999078\n",
      "Train loss = 0.00045 | mse = 0.00037 | KL = 0.00008\n",
      "Validation loss = 0.00044 | mse = 0.00037 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00030048933695070446 | Global gradient norm: 89.91\n",
      "Step 114116) Time = 118.719080\n",
      "Train loss = 0.00043 | mse = 0.00036 | KL = 0.00007\n",
      "Validation loss = 0.00042 | mse = 0.00035 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00029989477479830384 | Global gradient norm: 89.91\n",
      "Step 114304) Time = 120.431254\n",
      "Train loss = 0.00047 | mse = 0.00039 | KL = 0.00008\n",
      "Validation loss = 0.00044 | mse = 0.00036 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00029930134769529104 | Global gradient norm: 89.95\n",
      "Step 114492) Time = 117.165118\n",
      "Train loss = 0.00059 | mse = 0.00047 | KL = 0.00011\n",
      "Validation loss = 0.00059 | mse = 0.00048 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.00029870905564166605 | Global gradient norm: 89.95\n",
      "Step 114680) Time = 118.569201\n",
      "Train loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "Validation loss = 0.00048 | mse = 0.00040 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0002981179568450898 | Global gradient norm: 90.00\n",
      "Step 114868) Time = 117.880766\n",
      "Train loss = 0.00045 | mse = 0.00037 | KL = 0.00007\n",
      "Validation loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002975280804093927 | Global gradient norm: 90.03\n",
      "Step 115056) Time = 121.262105\n",
      "Train loss = 0.00051 | mse = 0.00044 | KL = 0.00008\n",
      "Validation loss = 0.00052 | mse = 0.00044 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00029693933902308345 | Global gradient norm: 90.05\n",
      "Step 115244) Time = 122.857477\n",
      "Train loss = 0.00048 | mse = 0.00040 | KL = 0.00008\n",
      "Validation loss = 0.00045 | mse = 0.00038 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00029635170358233154 | Global gradient norm: 90.06\n",
      "Step 115432) Time = 120.853643\n",
      "Train loss = 0.00050 | mse = 0.00042 | KL = 0.00009\n",
      "Validation loss = 0.00049 | mse = 0.00040 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.0002957652905024588 | Global gradient norm: 90.08\n",
      "Step 115620) Time = 121.536261\n",
      "Train loss = 0.00050 | mse = 0.00042 | KL = 0.00008\n",
      "Validation loss = 0.00049 | mse = 0.00042 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0002951800124719739 | Global gradient norm: 90.09\n",
      "Step 115808) Time = 121.062923\n",
      "Train loss = 0.00056 | mse = 0.00047 | KL = 0.00008\n",
      "Validation loss = 0.00055 | mse = 0.00047 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00029459589859470725 | Global gradient norm: 90.12\n",
      "Step 115996) Time = 118.337332\n",
      "Train loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "Validation loss = 0.00047 | mse = 0.00039 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0002940129488706589 | Global gradient norm: 90.15\n",
      "Step 116184) Time = 120.962771\n",
      "Train loss = 0.00053 | mse = 0.00045 | KL = 0.00008\n",
      "Validation loss = 0.00053 | mse = 0.00045 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00029343116329982877 | Global gradient norm: 90.18\n",
      "Step 116372) Time = 118.531100\n",
      "Train loss = 0.00043 | mse = 0.00036 | KL = 0.00007\n",
      "Validation loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002928505127783865 | Global gradient norm: 90.20\n",
      "Step 116560) Time = 121.743935\n",
      "Train loss = 0.00045 | mse = 0.00037 | KL = 0.00007\n",
      "Validation loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.000292270997306332 | Global gradient norm: 90.25\n",
      "Step 116748) Time = 122.627975\n",
      "Train loss = 0.00057 | mse = 0.00050 | KL = 0.00007\n",
      "Validation loss = 0.00058 | mse = 0.00050 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002916926459874958 | Global gradient norm: 90.28\n",
      "Step 116936) Time = 120.039622\n",
      "Train loss = 0.00049 | mse = 0.00042 | KL = 0.00008\n",
      "Validation loss = 0.00050 | mse = 0.00042 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0002911154879257083 | Global gradient norm: 90.31\n",
      "Step 117124) Time = 121.071135\n",
      "Train loss = 0.00042 | mse = 0.00035 | KL = 0.00007\n",
      "Validation loss = 0.00043 | mse = 0.00035 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002905394067056477 | Global gradient norm: 90.31\n",
      "Step 117312) Time = 122.069666\n",
      "Train loss = 0.00054 | mse = 0.00043 | KL = 0.00011\n",
      "Validation loss = 0.00054 | mse = 0.00044 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0002899644896388054 | Global gradient norm: 90.33\n",
      "Step 117500) Time = 120.246449\n",
      "Train loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "Validation loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00028939067851752043 | Global gradient norm: 90.38\n",
      "Step 117688) Time = 122.833965\n",
      "Train loss = 0.00047 | mse = 0.00039 | KL = 0.00007\n",
      "Validation loss = 0.00047 | mse = 0.00040 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00028881803154945374 | Global gradient norm: 90.39\n",
      "Step 117876) Time = 119.647759\n",
      "Train loss = 0.00046 | mse = 0.00037 | KL = 0.00009\n",
      "Validation loss = 0.00046 | mse = 0.00037 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.0002882464905269444 | Global gradient norm: 90.42\n",
      "Step 118064) Time = 123.391537\n",
      "Train loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "Validation loss = 0.00045 | mse = 0.00038 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002876761427614838 | Global gradient norm: 90.48\n",
      "Step 118252) Time = 120.474554\n",
      "Train loss = 0.00054 | mse = 0.00047 | KL = 0.00007\n",
      "Validation loss = 0.00052 | mse = 0.00046 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002871068427339196 | Global gradient norm: 90.51\n",
      "Step 118440) Time = 118.231894\n",
      "Train loss = 0.00047 | mse = 0.00040 | KL = 0.00007\n",
      "Validation loss = 0.00046 | mse = 0.00039 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00028653876506723464 | Global gradient norm: 90.54\n",
      "Step 118628) Time = 122.256788\n",
      "Train loss = 0.00052 | mse = 0.00041 | KL = 0.00011\n",
      "Validation loss = 0.00053 | mse = 0.00042 | KL = 0.00011\n",
      "================================================\n",
      "Learning rate: 0.0002859717351384461 | Global gradient norm: 90.54\n",
      "Step 118816) Time = 124.735434\n",
      "Train loss = 0.00051 | mse = 0.00044 | KL = 0.00007\n",
      "Validation loss = 0.00051 | mse = 0.00044 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00028540584025904536 | Global gradient norm: 90.57\n",
      "Step 119004) Time = 122.115028\n",
      "Train loss = 0.00045 | mse = 0.00038 | KL = 0.00008\n",
      "Validation loss = 0.00045 | mse = 0.00038 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00028484108042903244 | Global gradient norm: 90.60\n",
      "Step 119192) Time = 123.349063\n",
      "Train loss = 0.00051 | mse = 0.00043 | KL = 0.00008\n",
      "Validation loss = 0.00050 | mse = 0.00042 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00028427745564840734 | Global gradient norm: 90.61\n",
      "Step 119380) Time = 121.829134\n",
      "Train loss = 0.00044 | mse = 0.00036 | KL = 0.00007\n",
      "Validation loss = 0.00045 | mse = 0.00037 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00028371490770950913 | Global gradient norm: 90.64\n",
      "Step 119568) Time = 122.122243\n",
      "Train loss = 0.00050 | mse = 0.00041 | KL = 0.00009\n",
      "Validation loss = 0.00048 | mse = 0.00039 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0002831534657161683 | Global gradient norm: 90.67\n",
      "Step 119756) Time = 120.496727\n",
      "Train loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "Validation loss = 0.00044 | mse = 0.00037 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0002825931878760457 | Global gradient norm: 90.70\n",
      "Step 119944) Time = 118.720262\n",
      "Train loss = 0.00057 | mse = 0.00050 | KL = 0.00007\n",
      "Validation loss = 0.00058 | mse = 0.00050 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00028203398687765 | Global gradient norm: 90.73\n",
      "Step 120132) Time = 124.195578\n",
      "Train loss = 0.00047 | mse = 0.00040 | KL = 0.00007\n",
      "Validation loss = 0.00045 | mse = 0.00038 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002814758918248117 | Global gradient norm: 90.78\n",
      "Step 120320) Time = 127.731914\n",
      "Train loss = 0.00073 | mse = 0.00066 | KL = 0.00007\n",
      "Validation loss = 0.00073 | mse = 0.00066 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00028091887361370027 | Global gradient norm: 90.81\n",
      "Step 120508) Time = 123.155082\n",
      "Train loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "Validation loss = 0.00045 | mse = 0.00038 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00028036299045197666 | Global gradient norm: 90.84\n",
      "Step 120696) Time = 127.425086\n",
      "Train loss = 0.00050 | mse = 0.00041 | KL = 0.00009\n",
      "Validation loss = 0.00049 | mse = 0.00041 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00027980824233964086 | Global gradient norm: 90.87\n",
      "Step 120884) Time = 124.885086\n",
      "Train loss = 0.00049 | mse = 0.00040 | KL = 0.00008\n",
      "Validation loss = 0.00048 | mse = 0.00040 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00027925451286137104 | Global gradient norm: 90.91\n",
      "Step 121072) Time = 123.417279\n",
      "Train loss = 0.00048 | mse = 0.00041 | KL = 0.00007\n",
      "Validation loss = 0.00048 | mse = 0.00041 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00027870191843248904 | Global gradient norm: 90.94\n",
      "Step 121260) Time = 118.342080\n",
      "Train loss = 0.00043 | mse = 0.00036 | KL = 0.00006\n",
      "Validation loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00027815040084533393 | Global gradient norm: 90.97\n",
      "Step 121448) Time = 117.301169\n",
      "Train loss = 0.00042 | mse = 0.00035 | KL = 0.00006\n",
      "Validation loss = 0.00042 | mse = 0.00035 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002775999892037362 | Global gradient norm: 91.02\n",
      "Step 121636) Time = 120.570155\n",
      "Train loss = 0.00052 | mse = 0.00046 | KL = 0.00007\n",
      "Validation loss = 0.00052 | mse = 0.00045 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00027705071261152625 | Global gradient norm: 91.01\n",
      "Step 121824) Time = 121.515084\n",
      "Train loss = 0.00046 | mse = 0.00039 | KL = 0.00007\n",
      "Validation loss = 0.00048 | mse = 0.00040 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00027650248375721276 | Global gradient norm: 91.04\n",
      "Step 122012) Time = 118.911081\n",
      "Train loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "Validation loss = 0.00045 | mse = 0.00038 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00027595533174462616 | Global gradient norm: 91.06\n",
      "Step 122200) Time = 120.195081\n",
      "Train loss = 0.00043 | mse = 0.00036 | KL = 0.00006\n",
      "Validation loss = 0.00045 | mse = 0.00038 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.000275409227469936 | Global gradient norm: 91.10\n",
      "Step 122388) Time = 119.276129\n",
      "Train loss = 0.00044 | mse = 0.00037 | KL = 0.00006\n",
      "Validation loss = 0.00044 | mse = 0.00038 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002748642582446337 | Global gradient norm: 91.13\n",
      "Step 122576) Time = 116.627146\n",
      "Train loss = 0.00049 | mse = 0.00042 | KL = 0.00007\n",
      "Validation loss = 0.00048 | mse = 0.00042 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002743203367572278 | Global gradient norm: 91.13\n",
      "Step 122764) Time = 117.474079\n",
      "Train loss = 0.00089 | mse = 0.00081 | KL = 0.00008\n",
      "Validation loss = 0.00089 | mse = 0.00081 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0002737775503192097 | Global gradient norm: 91.16\n",
      "Step 122952) Time = 114.689312\n",
      "Train loss = 0.00052 | mse = 0.00042 | KL = 0.00011\n",
      "Validation loss = 0.00051 | mse = 0.00040 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0002732357825152576 | Global gradient norm: 91.19\n",
      "Step 123140) Time = 120.220260\n",
      "Train loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "Validation loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00027269512065686285 | Global gradient norm: 91.19\n",
      "Step 123328) Time = 121.657150\n",
      "Train loss = 0.00042 | mse = 0.00035 | KL = 0.00007\n",
      "Validation loss = 0.00043 | mse = 0.00036 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002721554774325341 | Global gradient norm: 91.22\n",
      "Step 123516) Time = 119.870081\n",
      "Train loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "Validation loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002716169401537627 | Global gradient norm: 91.23\n",
      "Step 123704) Time = 121.135588\n",
      "Train loss = 0.00048 | mse = 0.00042 | KL = 0.00007\n",
      "Validation loss = 0.00049 | mse = 0.00042 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002710794215090573 | Global gradient norm: 91.26\n",
      "Step 123892) Time = 122.608083\n",
      "Train loss = 0.00054 | mse = 0.00047 | KL = 0.00007\n",
      "Validation loss = 0.00053 | mse = 0.00046 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002705430379137397 | Global gradient norm: 91.27\n",
      "Step 124080) Time = 119.631148\n",
      "Train loss = 0.00045 | mse = 0.00039 | KL = 0.00007\n",
      "Validation loss = 0.00046 | mse = 0.00040 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00027000767295248806 | Global gradient norm: 91.30\n",
      "Step 124268) Time = 119.846081\n",
      "Train loss = 0.00043 | mse = 0.00037 | KL = 0.00007\n",
      "Validation loss = 0.00043 | mse = 0.00036 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00026947338483296335 | Global gradient norm: 91.32\n",
      "Step 124456) Time = 116.218228\n",
      "Train loss = 0.00045 | mse = 0.00039 | KL = 0.00007\n",
      "Validation loss = 0.00047 | mse = 0.00039 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00026894014445133507 | Global gradient norm: 91.34\n",
      "Step 124644) Time = 121.486082\n",
      "Train loss = 0.00042 | mse = 0.00035 | KL = 0.00007\n",
      "Validation loss = 0.00042 | mse = 0.00036 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00026840795180760324 | Global gradient norm: 91.38\n",
      "Step 124832) Time = 124.000191\n",
      "Train loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "Validation loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00026787680690176785 | Global gradient norm: 91.41\n",
      "Step 125020) Time = 119.480237\n",
      "Train loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "Validation loss = 0.00043 | mse = 0.00036 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00026734673883765936 | Global gradient norm: 91.38\n",
      "Step 125208) Time = 120.633082\n",
      "Train loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "Validation loss = 0.00045 | mse = 0.00037 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002668177185114473 | Global gradient norm: 91.43\n",
      "Step 125396) Time = 120.310082\n",
      "Train loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "Validation loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00026628971681930125 | Global gradient norm: 91.45\n",
      "Step 125584) Time = 118.113080\n",
      "Train loss = 0.00043 | mse = 0.00036 | KL = 0.00007\n",
      "Validation loss = 0.00042 | mse = 0.00035 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002657627919688821 | Global gradient norm: 91.48\n",
      "Step 125772) Time = 124.366192\n",
      "Train loss = 0.00043 | mse = 0.00037 | KL = 0.00006\n",
      "Validation loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00026523691485635936 | Global gradient norm: 91.52\n",
      "Step 125960) Time = 122.830379\n",
      "Train loss = 0.00051 | mse = 0.00045 | KL = 0.00007\n",
      "Validation loss = 0.00050 | mse = 0.00044 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002647120563779026 | Global gradient norm: 91.53\n",
      "Step 126148) Time = 121.475094\n",
      "Train loss = 0.00058 | mse = 0.00049 | KL = 0.00009\n",
      "Validation loss = 0.00059 | mse = 0.00049 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.0002641882165335119 | Global gradient norm: 91.54\n",
      "Step 126336) Time = 121.448883\n",
      "Train loss = 0.00054 | mse = 0.00048 | KL = 0.00007\n",
      "Validation loss = 0.00054 | mse = 0.00047 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00026366542442701757 | Global gradient norm: 91.55\n",
      "Step 126524) Time = 118.849570\n",
      "Train loss = 0.00046 | mse = 0.00039 | KL = 0.00007\n",
      "Validation loss = 0.00045 | mse = 0.00038 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002631437382660806 | Global gradient norm: 91.57\n",
      "Step 126712) Time = 124.514465\n",
      "Train loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "Validation loss = 0.00047 | mse = 0.00039 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0002626229834277183 | Global gradient norm: 91.60\n",
      "Step 126900) Time = 123.181529\n",
      "Train loss = 0.00048 | mse = 0.00041 | KL = 0.00007\n",
      "Validation loss = 0.00047 | mse = 0.00040 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00026210330543108284 | Global gradient norm: 91.62\n",
      "Step 127088) Time = 122.652124\n",
      "Train loss = 0.00055 | mse = 0.00048 | KL = 0.00007\n",
      "Validation loss = 0.00055 | mse = 0.00049 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002615846460685134 | Global gradient norm: 91.63\n",
      "Step 127276) Time = 125.495047\n",
      "Train loss = 0.00043 | mse = 0.00036 | KL = 0.00006\n",
      "Validation loss = 0.00043 | mse = 0.00037 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00026106700534000993 | Global gradient norm: 91.66\n",
      "Step 127464) Time = 122.000805\n",
      "Train loss = 0.00061 | mse = 0.00054 | KL = 0.00007\n",
      "Validation loss = 0.00059 | mse = 0.00052 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00026055044145323336 | Global gradient norm: 91.65\n",
      "Step 127652) Time = 123.260940\n",
      "Train loss = 0.00048 | mse = 0.00041 | KL = 0.00007\n",
      "Validation loss = 0.00048 | mse = 0.00041 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00026003483799286187 | Global gradient norm: 91.68\n",
      "Step 127840) Time = 127.861071\n",
      "Train loss = 0.00049 | mse = 0.00039 | KL = 0.00010\n",
      "Validation loss = 0.00049 | mse = 0.00039 | KL = 0.00010\n",
      "================================================\n",
      "Learning rate: 0.0002595202822703868 | Global gradient norm: 91.70\n",
      "Step 128028) Time = 124.649889\n",
      "Train loss = 0.00046 | mse = 0.00039 | KL = 0.00007\n",
      "Validation loss = 0.00045 | mse = 0.00038 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002590067160781473 | Global gradient norm: 91.74\n",
      "Step 128216) Time = 123.422339\n",
      "Train loss = 0.00045 | mse = 0.00038 | KL = 0.00007\n",
      "Validation loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002584941976238042 | Global gradient norm: 91.75\n",
      "Step 128404) Time = 123.265725\n",
      "Train loss = 0.00048 | mse = 0.00041 | KL = 0.00007\n",
      "Validation loss = 0.00045 | mse = 0.00039 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002579826978035271 | Global gradient norm: 91.78\n",
      "Step 128592) Time = 122.232939\n",
      "Train loss = 0.00047 | mse = 0.00040 | KL = 0.00007\n",
      "Validation loss = 0.00049 | mse = 0.00042 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00025747218751348555 | Global gradient norm: 91.79\n",
      "Step 128780) Time = 121.482441\n",
      "Train loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "Validation loss = 0.00044 | mse = 0.00037 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00025696269585750997 | Global gradient norm: 91.80\n",
      "Step 128968) Time = 127.025417\n",
      "Train loss = 0.00046 | mse = 0.00040 | KL = 0.00006\n",
      "Validation loss = 0.00046 | mse = 0.00039 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002564542228356004 | Global gradient norm: 91.83\n",
      "Step 129156) Time = 122.655147\n",
      "Train loss = 0.00046 | mse = 0.00039 | KL = 0.00007\n",
      "Validation loss = 0.00045 | mse = 0.00038 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002559467393439263 | Global gradient norm: 91.87\n",
      "Step 129344) Time = 125.067186\n",
      "Train loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "Validation loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00025544027448631823 | Global gradient norm: 91.92\n",
      "Step 129532) Time = 123.525779\n",
      "Train loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "Validation loss = 0.00044 | mse = 0.00037 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002549347991589457 | Global gradient norm: 91.92\n",
      "Step 129720) Time = 121.858989\n",
      "Train loss = 0.00042 | mse = 0.00035 | KL = 0.00006\n",
      "Validation loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002544303424656391 | Global gradient norm: 91.92\n",
      "Step 129908) Time = 127.651706\n",
      "Train loss = 0.00042 | mse = 0.00035 | KL = 0.00007\n",
      "Validation loss = 0.00042 | mse = 0.00035 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002539268461987376 | Global gradient norm: 91.95\n",
      "Step 130096) Time = 126.025383\n",
      "Train loss = 0.00046 | mse = 0.00039 | KL = 0.00008\n",
      "Validation loss = 0.00046 | mse = 0.00038 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0002534243685659021 | Global gradient norm: 91.98\n",
      "Step 130284) Time = 124.038100\n",
      "Train loss = 0.00042 | mse = 0.00035 | KL = 0.00006\n",
      "Validation loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00025292288046330214 | Global gradient norm: 92.01\n",
      "Step 130472) Time = 123.262443\n",
      "Train loss = 0.00055 | mse = 0.00049 | KL = 0.00006\n",
      "Validation loss = 0.00055 | mse = 0.00049 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00025242241099476814 | Global gradient norm: 92.03\n",
      "Step 130660) Time = 120.260806\n",
      "Train loss = 0.00044 | mse = 0.00038 | KL = 0.00006\n",
      "Validation loss = 0.00045 | mse = 0.00038 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002519229310564697 | Global gradient norm: 92.05\n",
      "Step 130848) Time = 127.656996\n",
      "Train loss = 0.00054 | mse = 0.00047 | KL = 0.00007\n",
      "Validation loss = 0.00054 | mse = 0.00048 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00025142444064840674 | Global gradient norm: 92.08\n",
      "Step 131036) Time = 126.894422\n",
      "Train loss = 0.00043 | mse = 0.00037 | KL = 0.00006\n",
      "Validation loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002509268815629184 | Global gradient norm: 92.10\n",
      "Step 131224) Time = 126.478745\n",
      "Train loss = 0.00050 | mse = 0.00044 | KL = 0.00006\n",
      "Validation loss = 0.00053 | mse = 0.00046 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002504303411114961 | Global gradient norm: 92.13\n",
      "Step 131412) Time = 122.434378\n",
      "Train loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "Validation loss = 0.00042 | mse = 0.00035 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002499347901903093 | Global gradient norm: 92.15\n",
      "Step 131600) Time = 116.309079\n",
      "Train loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "Validation loss = 0.00041 | mse = 0.00034 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.000249440228799358 | Global gradient norm: 92.17\n",
      "Step 131788) Time = 120.678190\n",
      "Train loss = 0.00050 | mse = 0.00043 | KL = 0.00006\n",
      "Validation loss = 0.00051 | mse = 0.00045 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002489466278348118 | Global gradient norm: 92.19\n",
      "Step 131976) Time = 121.253212\n",
      "Train loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "Validation loss = 0.00042 | mse = 0.00035 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002484539872966707 | Global gradient norm: 92.24\n",
      "Step 132164) Time = 118.843147\n",
      "Train loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "Validation loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00024796236539259553 | Global gradient norm: 92.23\n",
      "Step 132352) Time = 130.314239\n",
      "Train loss = 0.00047 | mse = 0.00041 | KL = 0.00006\n",
      "Validation loss = 0.00048 | mse = 0.00041 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.000247471674811095 | Global gradient norm: 92.26\n",
      "Step 132540) Time = 130.096131\n",
      "Train loss = 0.00045 | mse = 0.00038 | KL = 0.00007\n",
      "Validation loss = 0.00045 | mse = 0.00038 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00024698200286366045 | Global gradient norm: 92.28\n",
      "Step 132728) Time = 122.022185\n",
      "Train loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "Validation loss = 0.00043 | mse = 0.00036 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00024649323313497007 | Global gradient norm: 92.30\n",
      "Step 132916) Time = 122.891083\n",
      "Train loss = 0.00043 | mse = 0.00037 | KL = 0.00006\n",
      "Validation loss = 0.00043 | mse = 0.00037 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00024600548204034567 | Global gradient norm: 92.33\n",
      "Step 133104) Time = 118.058080\n",
      "Train loss = 0.00054 | mse = 0.00046 | KL = 0.00008\n",
      "Validation loss = 0.00055 | mse = 0.00047 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.0002455186622682959 | Global gradient norm: 92.36\n",
      "Step 133292) Time = 122.494083\n",
      "Train loss = 0.00045 | mse = 0.00037 | KL = 0.00008\n",
      "Validation loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002450328611303121 | Global gradient norm: 92.38\n",
      "Step 133480) Time = 123.852196\n",
      "Train loss = 0.00046 | mse = 0.00040 | KL = 0.00006\n",
      "Validation loss = 0.00046 | mse = 0.00040 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00024454796221107244 | Global gradient norm: 92.41\n",
      "Step 133668) Time = 119.500147\n",
      "Train loss = 0.00057 | mse = 0.00051 | KL = 0.00006\n",
      "Validation loss = 0.00057 | mse = 0.00051 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00024406405282206833 | Global gradient norm: 92.42\n",
      "Step 133856) Time = 121.422082\n",
      "Train loss = 0.00045 | mse = 0.00039 | KL = 0.00006\n",
      "Validation loss = 0.00045 | mse = 0.00039 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00024358108930755407 | Global gradient norm: 92.44\n",
      "Step 134044) Time = 118.517192\n",
      "Train loss = 0.00044 | mse = 0.00037 | KL = 0.00006\n",
      "Validation loss = 0.00044 | mse = 0.00038 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00024309908621944487 | Global gradient norm: 92.46\n",
      "Step 134232) Time = 123.226209\n",
      "Train loss = 0.00043 | mse = 0.00037 | KL = 0.00006\n",
      "Validation loss = 0.00043 | mse = 0.00037 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00024261804355774075 | Global gradient norm: 92.49\n",
      "Step 134420) Time = 123.926084\n",
      "Train loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "Validation loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00024213794677052647 | Global gradient norm: 92.46\n",
      "Step 134608) Time = 122.684084\n",
      "Train loss = 0.00046 | mse = 0.00039 | KL = 0.00007\n",
      "Validation loss = 0.00045 | mse = 0.00039 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00024165881040971726 | Global gradient norm: 92.51\n",
      "Step 134796) Time = 121.262082\n",
      "Train loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "Validation loss = 0.00043 | mse = 0.00036 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00024118060537148267 | Global gradient norm: 92.53\n",
      "Step 134984) Time = 121.255202\n",
      "Train loss = 0.00046 | mse = 0.00039 | KL = 0.00007\n",
      "Validation loss = 0.00047 | mse = 0.00040 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00024070336075965315 | Global gradient norm: 92.57\n",
      "Step 135172) Time = 119.275185\n",
      "Train loss = 0.00043 | mse = 0.00038 | KL = 0.00006\n",
      "Validation loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0002402270183665678 | Global gradient norm: 92.59\n",
      "Step 135360) Time = 120.275081\n",
      "Train loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "Validation loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002397516800556332 | Global gradient norm: 92.60\n",
      "Step 135548) Time = 125.442211\n",
      "Train loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "Validation loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00023927725851535797 | Global gradient norm: 92.63\n",
      "Step 135736) Time = 124.297084\n",
      "Train loss = 0.00044 | mse = 0.00039 | KL = 0.00006\n",
      "Validation loss = 0.00046 | mse = 0.00040 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00023880376829765737 | Global gradient norm: 92.66\n",
      "Step 135924) Time = 124.413085\n",
      "Train loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "Validation loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00023833119485061616 | Global gradient norm: 92.69\n",
      "Step 136112) Time = 119.896081\n",
      "Train loss = 0.00043 | mse = 0.00037 | KL = 0.00006\n",
      "Validation loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00023785958182998002 | Global gradient norm: 92.71\n",
      "Step 136300) Time = 119.896292\n",
      "Train loss = 0.00046 | mse = 0.00040 | KL = 0.00006\n",
      "Validation loss = 0.00045 | mse = 0.00039 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002373889001319185 | Global gradient norm: 92.70\n",
      "Step 136488) Time = 119.652081\n",
      "Train loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "Validation loss = 0.00044 | mse = 0.00038 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00023691919341217726 | Global gradient norm: 92.72\n",
      "Step 136676) Time = 117.098207\n",
      "Train loss = 0.00043 | mse = 0.00037 | KL = 0.00006\n",
      "Validation loss = 0.00044 | mse = 0.00038 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00023645035980734974 | Global gradient norm: 92.73\n",
      "Step 136864) Time = 129.076214\n",
      "Train loss = 0.00043 | mse = 0.00037 | KL = 0.00006\n",
      "Validation loss = 0.00044 | mse = 0.00038 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00023598245752509683 | Global gradient norm: 92.75\n",
      "Step 137052) Time = 124.869172\n",
      "Train loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "Validation loss = 0.00043 | mse = 0.00037 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00023551548656541854 | Global gradient norm: 92.76\n",
      "Step 137240) Time = 119.473551\n",
      "Train loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "Validation loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00023504944692831486 | Global gradient norm: 92.79\n",
      "Step 137428) Time = 122.048270\n",
      "Train loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "Validation loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002345843386137858 | Global gradient norm: 92.81\n",
      "Step 137616) Time = 120.677876\n",
      "Train loss = 0.00048 | mse = 0.00042 | KL = 0.00006\n",
      "Validation loss = 0.00048 | mse = 0.00042 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002341201325180009 | Global gradient norm: 92.82\n",
      "Step 137804) Time = 120.280908\n",
      "Train loss = 0.00043 | mse = 0.00037 | KL = 0.00006\n",
      "Validation loss = 0.00044 | mse = 0.00037 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00023365682864096016 | Global gradient norm: 92.85\n",
      "Step 137992) Time = 121.474798\n",
      "Train loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00023319448519032449 | Global gradient norm: 92.87\n",
      "Step 138180) Time = 119.441784\n",
      "Train loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "Validation loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00023273304395843297 | Global gradient norm: 92.89\n",
      "Step 138368) Time = 127.260898\n",
      "Train loss = 0.00043 | mse = 0.00037 | KL = 0.00006\n",
      "Validation loss = 0.00044 | mse = 0.00038 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00023227251949720085 | Global gradient norm: 92.88\n",
      "Step 138556) Time = 121.901368\n",
      "Train loss = 0.00046 | mse = 0.00039 | KL = 0.00007\n",
      "Validation loss = 0.00046 | mse = 0.00040 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00023181288270279765 | Global gradient norm: 92.90\n",
      "Step 138744) Time = 120.263625\n",
      "Train loss = 0.00047 | mse = 0.00041 | KL = 0.00006\n",
      "Validation loss = 0.00047 | mse = 0.00042 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00023135416267905384 | Global gradient norm: 92.91\n",
      "Step 138932) Time = 124.890117\n",
      "Train loss = 0.00045 | mse = 0.00039 | KL = 0.00006\n",
      "Validation loss = 0.00045 | mse = 0.00039 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00023089633032213897 | Global gradient norm: 92.93\n",
      "Step 139120) Time = 123.473353\n",
      "Train loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "Validation loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00023043945839162916 | Global gradient norm: 92.95\n",
      "Step 139308) Time = 123.270057\n",
      "Train loss = 0.00044 | mse = 0.00038 | KL = 0.00006\n",
      "Validation loss = 0.00045 | mse = 0.00039 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002299834304722026 | Global gradient norm: 92.99\n",
      "Step 139496) Time = 123.220339\n",
      "Train loss = 0.00047 | mse = 0.00041 | KL = 0.00006\n",
      "Validation loss = 0.00048 | mse = 0.00042 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00022952834842726588 | Global gradient norm: 93.00\n",
      "Step 139684) Time = 121.262938\n",
      "Train loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00022907413949724287 | Global gradient norm: 93.03\n",
      "Step 139872) Time = 125.277534\n",
      "Train loss = 0.00046 | mse = 0.00040 | KL = 0.00006\n",
      "Validation loss = 0.00047 | mse = 0.00041 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00022862086188979447 | Global gradient norm: 93.03\n",
      "Step 140060) Time = 122.243413\n",
      "Train loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "Validation loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00022816845739725977 | Global gradient norm: 93.05\n",
      "Step 140248) Time = 119.511344\n",
      "Train loss = 0.00045 | mse = 0.00037 | KL = 0.00009\n",
      "Validation loss = 0.00047 | mse = 0.00038 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00022771696967538446 | Global gradient norm: 93.07\n",
      "Step 140436) Time = 121.080966\n",
      "Train loss = 0.00048 | mse = 0.00042 | KL = 0.00006\n",
      "Validation loss = 0.00047 | mse = 0.00041 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00022726635506842285 | Global gradient norm: 93.09\n",
      "Step 140624) Time = 123.465613\n",
      "Train loss = 0.00044 | mse = 0.00038 | KL = 0.00006\n",
      "Validation loss = 0.00043 | mse = 0.00037 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00022681662812829018 | Global gradient norm: 93.08\n",
      "Step 140812) Time = 121.425662\n",
      "Train loss = 0.00039 | mse = 0.00033 | KL = 0.00006\n",
      "Validation loss = 0.00039 | mse = 0.00033 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00022636781795881689 | Global gradient norm: 93.11\n",
      "Step 141000) Time = 130.933300\n",
      "Train loss = 0.00044 | mse = 0.00038 | KL = 0.00006\n",
      "Validation loss = 0.00043 | mse = 0.00037 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00022591985180042684 | Global gradient norm: 93.12\n",
      "Step 141188) Time = 132.014435\n",
      "Train loss = 0.00047 | mse = 0.00041 | KL = 0.00006\n",
      "Validation loss = 0.00048 | mse = 0.00042 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002254728169646114 | Global gradient norm: 93.15\n",
      "Step 141376) Time = 128.876781\n",
      "Train loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "Validation loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00022502665524370968 | Global gradient norm: 93.16\n",
      "Step 141564) Time = 120.934578\n",
      "Train loss = 0.00045 | mse = 0.00040 | KL = 0.00006\n",
      "Validation loss = 0.00045 | mse = 0.00039 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00022458135208580643 | Global gradient norm: 93.19\n",
      "Step 141752) Time = 119.449118\n",
      "Train loss = 0.00042 | mse = 0.00035 | KL = 0.00007\n",
      "Validation loss = 0.00043 | mse = 0.00036 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0002241369365947321 | Global gradient norm: 93.23\n",
      "Step 141940) Time = 124.484719\n",
      "Train loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "Validation loss = 0.00040 | mse = 0.00035 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00022369342332240194 | Global gradient norm: 93.26\n",
      "Step 142128) Time = 122.282902\n",
      "Train loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "Validation loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00022325076861307025 | Global gradient norm: 93.25\n",
      "Step 142316) Time = 50.765693\n",
      "Train loss = 0.00043 | mse = 0.00036 | KL = 0.00006\n",
      "Validation loss = 0.00044 | mse = 0.00038 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00022280901612248272 | Global gradient norm: 93.28\n",
      "Step 142504) Time = 120.550573\n",
      "Train loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "Validation loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002223680930910632 | Global gradient norm: 93.30\n",
      "Step 142692) Time = 118.932243\n",
      "Train loss = 0.00046 | mse = 0.00040 | KL = 0.00006\n",
      "Validation loss = 0.00045 | mse = 0.00040 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00022192808683030307 | Global gradient norm: 93.33\n",
      "Step 142880) Time = 115.091078\n",
      "Train loss = 0.00040 | mse = 0.00035 | KL = 0.00006\n",
      "Validation loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002214889245806262 | Global gradient norm: 93.35\n",
      "Step 143068) Time = 119.672081\n",
      "Train loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "Validation loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00022105062089394778 | Global gradient norm: 93.39\n",
      "Step 143256) Time = 119.199122\n",
      "Train loss = 0.00048 | mse = 0.00042 | KL = 0.00006\n",
      "Validation loss = 0.00047 | mse = 0.00042 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00022061323397792876 | Global gradient norm: 93.38\n",
      "Step 143444) Time = 113.988187\n",
      "Train loss = 0.00039 | mse = 0.00034 | KL = 0.00006\n",
      "Validation loss = 0.00038 | mse = 0.00032 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00022017666196916252 | Global gradient norm: 93.41\n",
      "Step 143632) Time = 115.487080\n",
      "Train loss = 0.00042 | mse = 0.00037 | KL = 0.00006\n",
      "Validation loss = 0.00042 | mse = 0.00037 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00021974096307531 | Global gradient norm: 93.43\n",
      "Step 143820) Time = 116.839253\n",
      "Train loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002193061518482864 | Global gradient norm: 93.44\n",
      "Step 144008) Time = 114.378201\n",
      "Train loss = 0.00046 | mse = 0.00040 | KL = 0.00006\n",
      "Validation loss = 0.00046 | mse = 0.00041 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00021887219918426126 | Global gradient norm: 93.45\n",
      "Step 144196) Time = 115.750078\n",
      "Train loss = 0.00046 | mse = 0.00040 | KL = 0.00006\n",
      "Validation loss = 0.00045 | mse = 0.00040 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00021843906142748892 | Global gradient norm: 93.47\n",
      "Step 144384) Time = 113.210139\n",
      "Train loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "Validation loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00021800682588946074 | Global gradient norm: 93.49\n",
      "Step 144572) Time = 116.668198\n",
      "Train loss = 0.00041 | mse = 0.00036 | KL = 0.00006\n",
      "Validation loss = 0.00041 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0002175754343625158 | Global gradient norm: 93.50\n",
      "Step 144760) Time = 117.218191\n",
      "Train loss = 0.00058 | mse = 0.00052 | KL = 0.00006\n",
      "Validation loss = 0.00057 | mse = 0.00052 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002171448722947389 | Global gradient norm: 93.52\n",
      "Step 144948) Time = 118.074263\n",
      "Train loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "Validation loss = 0.00043 | mse = 0.00037 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00021671521244570613 | Global gradient norm: 93.55\n",
      "Step 145136) Time = 123.057146\n",
      "Train loss = 0.00047 | mse = 0.00042 | KL = 0.00005\n",
      "Validation loss = 0.00048 | mse = 0.00042 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002162863384000957 | Global gradient norm: 93.55\n",
      "Step 145324) Time = 113.437129\n",
      "Train loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "Validation loss = 0.00040 | mse = 0.00035 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00021585838112514466 | Global gradient norm: 93.58\n",
      "Step 145512) Time = 116.843277\n",
      "Train loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00021543120965361595 | Global gradient norm: 93.59\n",
      "Step 145700) Time = 115.132078\n",
      "Train loss = 0.00041 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00021500491129700094 | Global gradient norm: 93.61\n",
      "Step 145888) Time = 119.803148\n",
      "Train loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00021457944239955395 | Global gradient norm: 93.64\n",
      "Step 146076) Time = 116.181146\n",
      "Train loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00021415484661702067 | Global gradient norm: 93.64\n",
      "Step 146264) Time = 120.321081\n",
      "Train loss = 0.00049 | mse = 0.00043 | KL = 0.00006\n",
      "Validation loss = 0.00050 | mse = 0.00043 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002137310802936554 | Global gradient norm: 93.67\n",
      "Step 146452) Time = 115.005197\n",
      "Train loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00021330815798137337 | Global gradient norm: 93.69\n",
      "Step 146640) Time = 127.385215\n",
      "Train loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0002128860360244289 | Global gradient norm: 93.69\n",
      "Step 146828) Time = 120.373134\n",
      "Train loss = 0.00044 | mse = 0.00038 | KL = 0.00006\n",
      "Validation loss = 0.00044 | mse = 0.00039 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0002124647726304829 | Global gradient norm: 93.70\n",
      "Step 147016) Time = 117.228301\n",
      "Train loss = 0.00042 | mse = 0.00036 | KL = 0.00005\n",
      "Validation loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00021204435324762017 | Global gradient norm: 93.71\n",
      "Step 147204) Time = 119.211126\n",
      "Train loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00021162476332392544 | Global gradient norm: 93.73\n",
      "Step 147392) Time = 121.979252\n",
      "Train loss = 0.00044 | mse = 0.00039 | KL = 0.00006\n",
      "Validation loss = 0.00043 | mse = 0.00038 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002112059883074835 | Global gradient norm: 93.74\n",
      "Step 147580) Time = 121.359198\n",
      "Train loss = 0.00041 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00021078804275020957 | Global gradient norm: 93.75\n",
      "Step 147768) Time = 121.246197\n",
      "Train loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0002103709412040189 | Global gradient norm: 93.75\n",
      "Step 147956) Time = 115.826118\n",
      "Train loss = 0.00046 | mse = 0.00041 | KL = 0.00006\n",
      "Validation loss = 0.00046 | mse = 0.00040 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00020995464001316577 | Global gradient norm: 93.78\n",
      "Step 148144) Time = 119.292080\n",
      "Train loss = 0.00046 | mse = 0.00041 | KL = 0.00006\n",
      "Validation loss = 0.00047 | mse = 0.00040 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00020953919738531113 | Global gradient norm: 93.80\n",
      "Step 148332) Time = 122.609073\n",
      "Train loss = 0.00040 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0002091245405608788 | Global gradient norm: 93.82\n",
      "Step 148520) Time = 121.682208\n",
      "Train loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00020871072774752975 | Global gradient norm: 93.81\n",
      "Step 148708) Time = 121.844409\n",
      "Train loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00020829772984143347 | Global gradient norm: 93.84\n",
      "Step 148896) Time = 122.926217\n",
      "Train loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00040 | mse = 0.00034 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00020788554684258997 | Global gradient norm: 93.86\n",
      "Step 149084) Time = 123.040844\n",
      "Train loss = 0.00055 | mse = 0.00049 | KL = 0.00006\n",
      "Validation loss = 0.00055 | mse = 0.00049 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00020747416419908404 | Global gradient norm: 93.88\n",
      "Step 149272) Time = 122.904560\n",
      "Train loss = 0.00045 | mse = 0.00039 | KL = 0.00006\n",
      "Validation loss = 0.00045 | mse = 0.00040 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00020706362556666136 | Global gradient norm: 93.90\n",
      "Step 149460) Time = 130.286288\n",
      "Train loss = 0.00039 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00020665388728957623 | Global gradient norm: 93.93\n",
      "Step 149648) Time = 131.309455\n",
      "Train loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "Validation loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00020624494936782867 | Global gradient norm: 93.95\n",
      "Step 149836) Time = 128.076485\n",
      "Train loss = 0.00037 | mse = 0.00031 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00020583684090524912 | Global gradient norm: 93.95\n",
      "Step 150024) Time = 125.311008\n",
      "Train loss = 0.00058 | mse = 0.00053 | KL = 0.00005\n",
      "Validation loss = 0.00059 | mse = 0.00054 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00020542950369417667 | Global gradient norm: 93.97\n",
      "Step 150212) Time = 124.354544\n",
      "Train loss = 0.00047 | mse = 0.00042 | KL = 0.00006\n",
      "Validation loss = 0.00047 | mse = 0.00041 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00020502301049418747 | Global gradient norm: 93.99\n",
      "Step 150400) Time = 123.276316\n",
      "Train loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "Validation loss = 0.00043 | mse = 0.00038 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00020461731764953583 | Global gradient norm: 94.00\n",
      "Step 150588) Time = 120.475814\n",
      "Train loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "Validation loss = 0.00042 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00020421241060830653 | Global gradient norm: 94.02\n",
      "Step 150776) Time = 119.965253\n",
      "Train loss = 0.00068 | mse = 0.00063 | KL = 0.00005\n",
      "Validation loss = 0.00069 | mse = 0.00064 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00020380830392241478 | Global gradient norm: 94.03\n",
      "Step 150964) Time = 123.110847\n",
      "Train loss = 0.00043 | mse = 0.00038 | KL = 0.00005\n",
      "Validation loss = 0.00044 | mse = 0.00038 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0002034049975918606 | Global gradient norm: 94.04\n",
      "Step 151152) Time = 121.673642\n",
      "Train loss = 0.00039 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0002030025061685592 | Global gradient norm: 94.05\n",
      "Step 151340) Time = 122.413412\n",
      "Train loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00020260080054868013 | Global gradient norm: 94.07\n",
      "Step 151528) Time = 119.746336\n",
      "Train loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00020219989528413862 | Global gradient norm: 94.07\n",
      "Step 151716) Time = 121.270568\n",
      "Train loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00020179976127110422 | Global gradient norm: 94.09\n",
      "Step 151904) Time = 121.583004\n",
      "Train loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00020140045671723783 | Global gradient norm: 94.11\n",
      "Step 152092) Time = 119.206130\n",
      "Train loss = 0.00040 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00020100192341487855 | Global gradient norm: 94.13\n",
      "Step 152280) Time = 125.301758\n",
      "Train loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "Validation loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0002006041759159416 | Global gradient norm: 94.14\n",
      "Step 152468) Time = 124.505836\n",
      "Train loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00020020721422042698 | Global gradient norm: 94.15\n",
      "Step 152656) Time = 121.192101\n",
      "Train loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "Validation loss = 0.00043 | mse = 0.00038 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001998110383283347 | Global gradient norm: 94.17\n",
      "Step 152844) Time = 122.380770\n",
      "Train loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "Validation loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00019941564823966473 | Global gradient norm: 94.17\n",
      "Step 153032) Time = 118.478643\n",
      "Train loss = 0.00044 | mse = 0.00039 | KL = 0.00005\n",
      "Validation loss = 0.00043 | mse = 0.00038 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001990210439544171 | Global gradient norm: 94.20\n",
      "Step 153220) Time = 123.158653\n",
      "Train loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "Validation loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00019862719636876136 | Global gradient norm: 94.21\n",
      "Step 153408) Time = 125.235152\n",
      "Train loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00019823417824227363 | Global gradient norm: 94.23\n",
      "Step 153596) Time = 123.453455\n",
      "Train loss = 0.00039 | mse = 0.00032 | KL = 0.00006\n",
      "Validation loss = 0.00038 | mse = 0.00032 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00019784190226346254 | Global gradient norm: 94.26\n",
      "Step 153784) Time = 128.277234\n",
      "Train loss = 0.00042 | mse = 0.00037 | KL = 0.00006\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0001974504120880738 | Global gradient norm: 94.26\n",
      "Step 153972) Time = 123.193340\n",
      "Train loss = 0.00045 | mse = 0.00036 | KL = 0.00009\n",
      "Validation loss = 0.00043 | mse = 0.00034 | KL = 0.00009\n",
      "================================================\n",
      "Learning rate: 0.00019705969316419214 | Global gradient norm: 94.29\n",
      "Step 154160) Time = 118.682152\n",
      "Train loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00019666973093990237 | Global gradient norm: 94.30\n",
      "Step 154348) Time = 126.041157\n",
      "Train loss = 0.00046 | mse = 0.00041 | KL = 0.00005\n",
      "Validation loss = 0.00046 | mse = 0.00040 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00019628056907095015 | Global gradient norm: 94.33\n",
      "Step 154536) Time = 121.784188\n",
      "Train loss = 0.00048 | mse = 0.00043 | KL = 0.00005\n",
      "Validation loss = 0.00046 | mse = 0.00041 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001958921639015898 | Global gradient norm: 94.34\n",
      "Step 154724) Time = 115.906078\n",
      "Train loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00019550452998373657 | Global gradient norm: 94.36\n",
      "Step 154912) Time = 118.919080\n",
      "Train loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001951176527654752 | Global gradient norm: 94.37\n",
      "Step 155100) Time = 116.087087\n",
      "Train loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00040 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001947315759025514 | Global gradient norm: 94.39\n",
      "Step 155288) Time = 118.553197\n",
      "Train loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "Validation loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00019434622663538903 | Global gradient norm: 94.42\n",
      "Step 155476) Time = 118.174080\n",
      "Train loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00019396164861973375 | Global gradient norm: 94.43\n",
      "Step 155664) Time = 118.870171\n",
      "Train loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001935777981998399 | Global gradient norm: 94.44\n",
      "Step 155852) Time = 120.643150\n",
      "Train loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "Validation loss = 0.00042 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00019319477723911405 | Global gradient norm: 94.46\n",
      "Step 156040) Time = 119.251193\n",
      "Train loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00019281248387414962 | Global gradient norm: 94.47\n",
      "Step 156228) Time = 117.968129\n",
      "Train loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00019243093265686184 | Global gradient norm: 94.49\n",
      "Step 156416) Time = 119.412081\n",
      "Train loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "Validation loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001920501672429964 | Global gradient norm: 94.51\n",
      "Step 156604) Time = 118.028080\n",
      "Train loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00019167011487297714 | Global gradient norm: 94.52\n",
      "Step 156792) Time = 121.892082\n",
      "Train loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00019129083375446498 | Global gradient norm: 94.52\n",
      "Step 156980) Time = 124.968215\n",
      "Train loss = 0.00041 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001909123093355447 | Global gradient norm: 94.54\n",
      "Step 157168) Time = 125.237085\n",
      "Train loss = 0.00043 | mse = 0.00037 | KL = 0.00005\n",
      "Validation loss = 0.00044 | mse = 0.00039 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00019053452706430107 | Global gradient norm: 94.55\n",
      "Step 157356) Time = 122.553198\n",
      "Train loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00019015750149264932 | Global gradient norm: 94.56\n",
      "Step 157544) Time = 122.481246\n",
      "Train loss = 0.00040 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00018978120351675898 | Global gradient norm: 94.56\n",
      "Step 157732) Time = 118.262199\n",
      "Train loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00018940567679237574 | Global gradient norm: 94.58\n",
      "Step 157920) Time = 118.212080\n",
      "Train loss = 0.00045 | mse = 0.00040 | KL = 0.00005\n",
      "Validation loss = 0.00046 | mse = 0.00041 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001890308631118387 | Global gradient norm: 94.60\n",
      "Step 158108) Time = 116.685202\n",
      "Train loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00018865682068280876 | Global gradient norm: 94.61\n",
      "Step 158296) Time = 126.141086\n",
      "Train loss = 0.00046 | mse = 0.00040 | KL = 0.00005\n",
      "Validation loss = 0.00044 | mse = 0.00038 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00018828347674570978 | Global gradient norm: 94.62\n",
      "Step 158484) Time = 122.053194\n",
      "Train loss = 0.00048 | mse = 0.00043 | KL = 0.00006\n",
      "Validation loss = 0.00049 | mse = 0.00043 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00018791091861203313 | Global gradient norm: 94.64\n",
      "Step 158672) Time = 117.022224\n",
      "Train loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00018753907352220267 | Global gradient norm: 94.67\n",
      "Step 158860) Time = 118.405244\n",
      "Train loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "Validation loss = 0.00039 | mse = 0.00033 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.0001871679851319641 | Global gradient norm: 94.68\n",
      "Step 159048) Time = 120.852140\n",
      "Train loss = 0.00047 | mse = 0.00042 | KL = 0.00005\n",
      "Validation loss = 0.00047 | mse = 0.00041 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.000186797566129826 | Global gradient norm: 94.69\n",
      "Step 159236) Time = 116.028147\n",
      "Train loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00018642796203494072 | Global gradient norm: 94.72\n",
      "Step 159424) Time = 118.010496\n",
      "Train loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "Validation loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00018605904188007116 | Global gradient norm: 94.72\n",
      "Step 159612) Time = 120.848183\n",
      "Train loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00018569087842479348 | Global gradient norm: 94.75\n",
      "Step 159800) Time = 117.546608\n",
      "Train loss = 0.00046 | mse = 0.00040 | KL = 0.00005\n",
      "Validation loss = 0.00044 | mse = 0.00039 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00018532341346144676 | Global gradient norm: 94.76\n",
      "Step 159988) Time = 121.849357\n",
      "Train loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001849566906457767 | Global gradient norm: 94.78\n",
      "Step 160176) Time = 120.635396\n",
      "Train loss = 0.00044 | mse = 0.00038 | KL = 0.00005\n",
      "Validation loss = 0.00044 | mse = 0.00038 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00018459069542586803 | Global gradient norm: 94.79\n",
      "Step 160364) Time = 122.195023\n",
      "Train loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00018422544235363603 | Global gradient norm: 94.80\n",
      "Step 160552) Time = 121.059190\n",
      "Train loss = 0.00077 | mse = 0.00069 | KL = 0.00008\n",
      "Validation loss = 0.00077 | mse = 0.00069 | KL = 0.00008\n",
      "================================================\n",
      "Learning rate: 0.00018386087322141975 | Global gradient norm: 94.81\n",
      "Step 160740) Time = 120.899997\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001834970898926258 | Global gradient norm: 94.83\n",
      "Step 160928) Time = 129.735225\n",
      "Train loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00018313396140001714 | Global gradient norm: 94.83\n",
      "Step 161116) Time = 124.735618\n",
      "Train loss = 0.00044 | mse = 0.00039 | KL = 0.00005\n",
      "Validation loss = 0.00045 | mse = 0.00039 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00018277154595125467 | Global gradient norm: 94.85\n",
      "Step 161304) Time = 120.049132\n",
      "Train loss = 0.00047 | mse = 0.00042 | KL = 0.00005\n",
      "Validation loss = 0.00046 | mse = 0.00041 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00018240988720208406 | Global gradient norm: 94.87\n",
      "Step 161492) Time = 121.219672\n",
      "Train loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00018204892694484442 | Global gradient norm: 94.88\n",
      "Step 161680) Time = 117.757914\n",
      "Train loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00018168870883528143 | Global gradient norm: 94.90\n",
      "Step 161868) Time = 120.689337\n",
      "Train loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00018132917466573417 | Global gradient norm: 94.91\n",
      "Step 162056) Time = 131.412264\n",
      "Train loss = 0.00046 | mse = 0.00039 | KL = 0.00007\n",
      "Validation loss = 0.00044 | mse = 0.00037 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00018097033898811787 | Global gradient norm: 94.92\n",
      "Step 162244) Time = 131.052293\n",
      "Train loss = 0.00041 | mse = 0.00035 | KL = 0.00006\n",
      "Validation loss = 0.00042 | mse = 0.00036 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00018061226001009345 | Global gradient norm: 94.94\n",
      "Step 162432) Time = 125.430022\n",
      "Train loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001802548358682543 | Global gradient norm: 94.96\n",
      "Step 162620) Time = 120.275868\n",
      "Train loss = 0.00046 | mse = 0.00041 | KL = 0.00005\n",
      "Validation loss = 0.00046 | mse = 0.00041 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00017989818297792226 | Global gradient norm: 94.98\n",
      "Step 162808) Time = 119.034096\n",
      "Train loss = 0.00041 | mse = 0.00037 | KL = 0.00005\n",
      "Validation loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00017954217037186027 | Global gradient norm: 94.99\n",
      "Step 162996) Time = 124.397002\n",
      "Train loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00017918687080964446 | Global gradient norm: 95.02\n",
      "Step 163184) Time = 121.501269\n",
      "Train loss = 0.00037 | mse = 0.00032 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001788323133951053 | Global gradient norm: 95.03\n",
      "Step 163372) Time = 121.556463\n",
      "Train loss = 0.00043 | mse = 0.00038 | KL = 0.00005\n",
      "Validation loss = 0.00043 | mse = 0.00039 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00017847842536866665 | Global gradient norm: 95.05\n",
      "Step 163560) Time = 121.894499\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00017812525038607419 | Global gradient norm: 95.06\n",
      "Step 163748) Time = 120.229522\n",
      "Train loss = 0.00042 | mse = 0.00035 | KL = 0.00007\n",
      "Validation loss = 0.00042 | mse = 0.00034 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.0001777727884473279 | Global gradient norm: 95.09\n",
      "Step 163936) Time = 125.099343\n",
      "Train loss = 0.00045 | mse = 0.00040 | KL = 0.00005\n",
      "Validation loss = 0.00046 | mse = 0.00041 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00017742099589668214 | Global gradient norm: 95.10\n",
      "Step 164124) Time = 121.826710\n",
      "Train loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001770699309417978 | Global gradient norm: 95.13\n",
      "Step 164312) Time = 117.684150\n",
      "Train loss = 0.00039 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001767195062711835 | Global gradient norm: 95.16\n",
      "Step 164500) Time = 122.051031\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00017636982374824584 | Global gradient norm: 95.17\n",
      "Step 164688) Time = 123.900836\n",
      "Train loss = 0.00037 | mse = 0.00032 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00017602083971723914 | Global gradient norm: 95.19\n",
      "Step 164876) Time = 121.834645\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001756724959705025 | Global gradient norm: 95.22\n",
      "Step 165064) Time = 123.673025\n",
      "Train loss = 0.00039 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001753248943714425 | Global gradient norm: 95.23\n",
      "Step 165252) Time = 121.451082\n",
      "Train loss = 0.00040 | mse = 0.00036 | KL = 0.00004\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00017497793305665255 | Global gradient norm: 95.24\n",
      "Step 165440) Time = 121.282323\n",
      "Train loss = 0.00037 | mse = 0.00032 | KL = 0.00005\n",
      "Validation loss = 0.00037 | mse = 0.00032 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.000174631699337624 | Global gradient norm: 95.26\n",
      "Step 165628) Time = 118.003080\n",
      "Train loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00017428614955861121 | Global gradient norm: 95.26\n",
      "Step 165816) Time = 114.274153\n",
      "Train loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001739412546157837 | Global gradient norm: 95.29\n",
      "Step 166004) Time = 118.304279\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00017359702906105667 | Global gradient norm: 95.31\n",
      "Step 166192) Time = 119.023199\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001732535456540063 | Global gradient norm: 95.30\n",
      "Step 166380) Time = 116.230078\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00005\n",
      "Validation loss = 0.00036 | mse = 0.00031 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00017291070253122598 | Global gradient norm: 95.31\n",
      "Step 166568) Time = 118.257268\n",
      "Train loss = 0.00036 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00017256855790037662 | Global gradient norm: 95.33\n",
      "Step 166756) Time = 116.495081\n",
      "Train loss = 0.00049 | mse = 0.00044 | KL = 0.00004\n",
      "Validation loss = 0.00049 | mse = 0.00045 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00017222708265762776 | Global gradient norm: 95.35\n",
      "Step 166944) Time = 122.847208\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00017188623314723372 | Global gradient norm: 95.36\n",
      "Step 167132) Time = 120.091320\n",
      "Train loss = 0.00043 | mse = 0.00039 | KL = 0.00005\n",
      "Validation loss = 0.00043 | mse = 0.00039 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00017154612578451633 | Global gradient norm: 95.37\n",
      "Step 167320) Time = 117.227160\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00017120664415415376 | Global gradient norm: 95.39\n",
      "Step 167508) Time = 118.505153\n",
      "Train loss = 0.00038 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00017086790467146784 | Global gradient norm: 95.41\n",
      "Step 167696) Time = 120.687201\n",
      "Train loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "Validation loss = 0.00040 | mse = 0.00035 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00017052976181730628 | Global gradient norm: 95.42\n",
      "Step 167884) Time = 119.051156\n",
      "Train loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00017019228835124522 | Global gradient norm: 95.42\n",
      "Step 168072) Time = 119.685704\n",
      "Train loss = 0.00037 | mse = 0.00032 | KL = 0.00005\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016985555703286082 | Global gradient norm: 95.42\n",
      "Step 168260) Time = 121.034237\n",
      "Train loss = 0.00046 | mse = 0.00042 | KL = 0.00005\n",
      "Validation loss = 0.00045 | mse = 0.00040 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016951942234300077 | Global gradient norm: 95.42\n",
      "Step 168448) Time = 117.725195\n",
      "Train loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016918395704124123 | Global gradient norm: 95.44\n",
      "Step 168636) Time = 121.244207\n",
      "Train loss = 0.00047 | mse = 0.00042 | KL = 0.00005\n",
      "Validation loss = 0.00046 | mse = 0.00041 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016884919023141265 | Global gradient norm: 95.45\n",
      "Step 168824) Time = 118.289080\n",
      "Train loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016851506370585412 | Global gradient norm: 95.46\n",
      "Step 169012) Time = 119.886081\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016818163567222655 | Global gradient norm: 95.48\n",
      "Step 169200) Time = 122.466317\n",
      "Train loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016784880426712334 | Global gradient norm: 95.49\n",
      "Step 169388) Time = 119.673128\n",
      "Train loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001675166713539511 | Global gradient norm: 95.50\n",
      "Step 169576) Time = 124.463346\n",
      "Train loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "Validation loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016718519327696413 | Global gradient norm: 95.51\n",
      "Step 169764) Time = 121.671329\n",
      "Train loss = 0.00039 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00035 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016685434093233198 | Global gradient norm: 95.52\n",
      "Step 169952) Time = 117.889207\n",
      "Train loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016652420163154602 | Global gradient norm: 95.53\n",
      "Step 170140) Time = 119.230225\n",
      "Train loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "Validation loss = 0.00043 | mse = 0.00038 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016619465895928442 | Global gradient norm: 95.55\n",
      "Step 170328) Time = 116.723082\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00005\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016586578567512333 | Global gradient norm: 95.56\n",
      "Step 170516) Time = 118.616080\n",
      "Train loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016553758177906275 | Global gradient norm: 95.58\n",
      "Step 170704) Time = 119.325555\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016520998906344175 | Global gradient norm: 95.59\n",
      "Step 170892) Time = 118.020144\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00005\n",
      "Validation loss = 0.00036 | mse = 0.00031 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016488306573592126 | Global gradient norm: 95.61\n",
      "Step 171080) Time = 119.285567\n",
      "Train loss = 0.00044 | mse = 0.00039 | KL = 0.00004\n",
      "Validation loss = 0.00044 | mse = 0.00040 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016455681179650128 | Global gradient norm: 95.62\n",
      "Step 171268) Time = 121.713955\n",
      "Train loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016423116903752089 | Global gradient norm: 95.64\n",
      "Step 171456) Time = 117.872738\n",
      "Train loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "Validation loss = 0.00040 | mse = 0.00036 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00016390621021855623 | Global gradient norm: 95.66\n",
      "Step 171644) Time = 120.225274\n",
      "Train loss = 0.00042 | mse = 0.00038 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00037 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016358184802811593 | Global gradient norm: 95.68\n",
      "Step 171832) Time = 119.514456\n",
      "Train loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "Validation loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.0001632581406738609 | Global gradient norm: 95.68\n",
      "Step 172020) Time = 119.296379\n",
      "Train loss = 0.00038 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001629351027077064 | Global gradient norm: 95.70\n",
      "Step 172208) Time = 119.653131\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00016261267592199147 | Global gradient norm: 95.71\n",
      "Step 172396) Time = 117.475652\n",
      "Train loss = 0.00041 | mse = 0.00037 | KL = 0.00004\n",
      "Validation loss = 0.00041 | mse = 0.00037 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001622908894205466 | Global gradient norm: 95.72\n",
      "Step 172584) Time = 123.638311\n",
      "Train loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "Validation loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00016196977230720222 | Global gradient norm: 95.74\n",
      "Step 172772) Time = 120.288267\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00031 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00016164923727046698 | Global gradient norm: 95.76\n",
      "Step 172960) Time = 117.494777\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00016132938617374748 | Global gradient norm: 95.79\n",
      "Step 173148) Time = 121.477074\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00016101013170555234 | Global gradient norm: 95.81\n",
      "Step 173336) Time = 122.272594\n",
      "Train loss = 0.00040 | mse = 0.00036 | KL = 0.00004\n",
      "Validation loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00016069150296971202 | Global gradient norm: 95.83\n",
      "Step 173524) Time = 119.682253\n",
      "Train loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001603735436219722 | Global gradient norm: 95.85\n",
      "Step 173712) Time = 124.461516\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00016005618090275675 | Global gradient norm: 95.86\n",
      "Step 173900) Time = 122.467645\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015973947301972657 | Global gradient norm: 95.87\n",
      "Step 174088) Time = 123.435268\n",
      "Train loss = 0.00038 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.000159423376317136 | Global gradient norm: 95.89\n",
      "Step 174276) Time = 120.311930\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.000159107890794985 | Global gradient norm: 95.90\n",
      "Step 174464) Time = 118.305692\n",
      "Train loss = 0.00040 | mse = 0.00036 | KL = 0.00004\n",
      "Validation loss = 0.00041 | mse = 0.00037 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00015879306010901928 | Global gradient norm: 95.91\n",
      "Step 174652) Time = 120.873670\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00031 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015847884060349315 | Global gradient norm: 95.92\n",
      "Step 174840) Time = 121.417685\n",
      "Train loss = 0.00041 | mse = 0.00037 | KL = 0.00005\n",
      "Validation loss = 0.00042 | mse = 0.00037 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00015816523227840662 | Global gradient norm: 95.93\n",
      "Step 175028) Time = 118.422842\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001578522496856749 | Global gradient norm: 95.94\n",
      "Step 175216) Time = 124.266636\n",
      "Train loss = 0.00039 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00015753987827338278 | Global gradient norm: 95.95\n",
      "Step 175404) Time = 121.929205\n",
      "Train loss = 0.00043 | mse = 0.00039 | KL = 0.00005\n",
      "Validation loss = 0.00040 | mse = 0.00036 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015722816169727594 | Global gradient norm: 95.96\n",
      "Step 175592) Time = 123.222256\n",
      "Train loss = 0.00041 | mse = 0.00036 | KL = 0.00004\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015691702719777822 | Global gradient norm: 95.99\n",
      "Step 175780) Time = 125.732518\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001566065038787201 | Global gradient norm: 96.01\n",
      "Step 175968) Time = 123.616138\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015629662084393203 | Global gradient norm: 96.02\n",
      "Step 176156) Time = 122.834327\n",
      "Train loss = 0.00069 | mse = 0.00064 | KL = 0.00004\n",
      "Validation loss = 0.00068 | mse = 0.00064 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00015598733443766832 | Global gradient norm: 96.04\n",
      "Step 176344) Time = 117.519207\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015567868831567466 | Global gradient norm: 96.06\n",
      "Step 176532) Time = 116.489079\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015537059516645968 | Global gradient norm: 96.07\n",
      "Step 176720) Time = 119.455158\n",
      "Train loss = 0.00053 | mse = 0.00046 | KL = 0.00007\n",
      "Validation loss = 0.00054 | mse = 0.00047 | KL = 0.00007\n",
      "================================================\n",
      "Learning rate: 0.00015506315685342997 | Global gradient norm: 96.08\n",
      "Step 176908) Time = 118.233146\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015475631516892463 | Global gradient norm: 96.10\n",
      "Step 177096) Time = 117.938189\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015445008466485888 | Global gradient norm: 96.13\n",
      "Step 177284) Time = 121.615125\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015414447989314795 | Global gradient norm: 96.15\n",
      "Step 177472) Time = 118.447080\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015383941354230046 | Global gradient norm: 96.16\n",
      "Step 177660) Time = 127.981123\n",
      "Train loss = 0.00043 | mse = 0.00039 | KL = 0.00004\n",
      "Validation loss = 0.00044 | mse = 0.00039 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015353500202763826 | Global gradient norm: 96.18\n",
      "Step 177848) Time = 52.590156\n",
      "Train loss = 0.00042 | mse = 0.00038 | KL = 0.00004\n",
      "Validation loss = 0.00042 | mse = 0.00038 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015323120169341564 | Global gradient norm: 96.17\n",
      "Step 178036) Time = 118.647080\n",
      "Train loss = 0.00037 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015292796888388693 | Global gradient norm: 96.19\n",
      "Step 178224) Time = 117.079079\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015262536180671304 | Global gradient norm: 96.21\n",
      "Step 178412) Time = 128.268348\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001523233368061483 | Global gradient norm: 96.22\n",
      "Step 178600) Time = 119.700201\n",
      "Train loss = 0.00041 | mse = 0.00037 | KL = 0.00004\n",
      "Validation loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001520219084341079 | Global gradient norm: 96.23\n",
      "Step 178788) Time = 117.789217\n",
      "Train loss = 0.00041 | mse = 0.00037 | KL = 0.00004\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015172110579442233 | Global gradient norm: 96.24\n",
      "Step 178976) Time = 120.344081\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015142085612751544 | Global gradient norm: 96.26\n",
      "Step 179164) Time = 118.934295\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015112121764104813 | Global gradient norm: 96.27\n",
      "Step 179352) Time = 117.877192\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015082220488693565 | Global gradient norm: 96.28\n",
      "Step 179540) Time = 118.464211\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00015052373055368662 | Global gradient norm: 96.28\n",
      "Step 179728) Time = 116.282195\n",
      "Train loss = 0.00048 | mse = 0.00043 | KL = 0.00004\n",
      "Validation loss = 0.00047 | mse = 0.00043 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001502258819527924 | Global gradient norm: 96.29\n",
      "Step 179916) Time = 119.814317\n",
      "Train loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "Validation loss = 0.00039 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014992861542850733 | Global gradient norm: 96.30\n",
      "Step 180104) Time = 122.135284\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00014963193098083138 | Global gradient norm: 96.31\n",
      "Step 180292) Time = 119.414314\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001493358431616798 | Global gradient norm: 96.32\n",
      "Step 180480) Time = 118.044151\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014904033741913736 | Global gradient norm: 96.34\n",
      "Step 180668) Time = 116.235244\n",
      "Train loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "Validation loss = 0.00040 | mse = 0.00035 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014874541375320405 | Global gradient norm: 96.35\n",
      "Step 180856) Time = 119.897081\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014845107216387987 | Global gradient norm: 96.36\n",
      "Step 181044) Time = 124.780335\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014815731265116483 | Global gradient norm: 96.38\n",
      "Step 181232) Time = 119.311207\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001478641206631437 | Global gradient norm: 96.39\n",
      "Step 181420) Time = 121.301242\n",
      "Train loss = 0.00042 | mse = 0.00038 | KL = 0.00005\n",
      "Validation loss = 0.00043 | mse = 0.00038 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00014757152530364692 | Global gradient norm: 96.40\n",
      "Step 181608) Time = 127.711200\n",
      "Train loss = 0.00042 | mse = 0.00038 | KL = 0.00004\n",
      "Validation loss = 0.00041 | mse = 0.00037 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014727951202075928 | Global gradient norm: 96.41\n",
      "Step 181796) Time = 126.419086\n",
      "Train loss = 0.00041 | mse = 0.00037 | KL = 0.00004\n",
      "Validation loss = 0.00041 | mse = 0.00037 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014698808081448078 | Global gradient norm: 96.43\n",
      "Step 181984) Time = 120.217919\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014669720258098096 | Global gradient norm: 96.45\n",
      "Step 182172) Time = 118.602409\n",
      "Train loss = 0.00040 | mse = 0.00036 | KL = 0.00004\n",
      "Validation loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014640693552792072 | Global gradient norm: 96.47\n",
      "Step 182360) Time = 134.187673\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014611720689572394 | Global gradient norm: 96.48\n",
      "Step 182548) Time = 121.357363\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001458280603401363 | Global gradient norm: 96.51\n",
      "Step 182736) Time = 115.378621\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014553952496498823 | Global gradient norm: 96.52\n",
      "Step 182924) Time = 123.844852\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014525149890687317 | Global gradient norm: 96.54\n",
      "Step 183112) Time = 121.079959\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014496409858111292 | Global gradient norm: 96.55\n",
      "Step 183300) Time = 117.625996\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001446772221243009 | Global gradient norm: 96.56\n",
      "Step 183488) Time = 121.544108\n",
      "Train loss = 0.00040 | mse = 0.00035 | KL = 0.00005\n",
      "Validation loss = 0.00041 | mse = 0.00036 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.000144390927744098 | Global gradient norm: 96.57\n",
      "Step 183676) Time = 123.435374\n",
      "Train loss = 0.00041 | mse = 0.00037 | KL = 0.00004\n",
      "Validation loss = 0.00042 | mse = 0.00038 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014410522999241948 | Global gradient norm: 96.59\n",
      "Step 183864) Time = 120.090457\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001438200706616044 | Global gradient norm: 96.60\n",
      "Step 184052) Time = 120.921826\n",
      "Train loss = 0.00040 | mse = 0.00037 | KL = 0.00004\n",
      "Validation loss = 0.00041 | mse = 0.00037 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014353547885548323 | Global gradient norm: 96.62\n",
      "Step 184240) Time = 119.158793\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014325144002214074 | Global gradient norm: 96.65\n",
      "Step 184428) Time = 122.149363\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014296795416157693 | Global gradient norm: 96.67\n",
      "Step 184616) Time = 120.967692\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014268506492953748 | Global gradient norm: 96.69\n",
      "Step 184804) Time = 120.450132\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014240271411836147 | Global gradient norm: 96.71\n",
      "Step 184992) Time = 123.333570\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014212091627996415 | Global gradient norm: 96.72\n",
      "Step 185180) Time = 121.992951\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00040 | mse = 0.00036 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014183970051817596 | Global gradient norm: 96.73\n",
      "Step 185368) Time = 121.868760\n",
      "Train loss = 0.00059 | mse = 0.00055 | KL = 0.00004\n",
      "Validation loss = 0.00058 | mse = 0.00054 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014155899407342076 | Global gradient norm: 96.74\n",
      "Step 185556) Time = 124.214444\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014127889880910516 | Global gradient norm: 96.76\n",
      "Step 185744) Time = 119.475498\n",
      "Train loss = 0.00040 | mse = 0.00036 | KL = 0.00004\n",
      "Validation loss = 0.00040 | mse = 0.00036 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.000140999341965653 | Global gradient norm: 96.76\n",
      "Step 185932) Time = 126.609045\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014072030899114907 | Global gradient norm: 96.77\n",
      "Step 186120) Time = 123.415616\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00014044185809325427 | Global gradient norm: 96.76\n",
      "Step 186308) Time = 120.535062\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001401639310643077 | Global gradient norm: 96.77\n",
      "Step 186496) Time = 123.435587\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013988658611197025 | Global gradient norm: 96.79\n",
      "Step 186684) Time = 127.195561\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013960979413241148 | Global gradient norm: 96.80\n",
      "Step 186872) Time = 126.558657\n",
      "Train loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "Validation loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001393335114698857 | Global gradient norm: 96.81\n",
      "Step 187060) Time = 126.603881\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013905781088396907 | Global gradient norm: 96.82\n",
      "Step 187248) Time = 124.234236\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013878263416700065 | Global gradient norm: 96.83\n",
      "Step 187436) Time = 123.698563\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013850799587089568 | Global gradient norm: 96.84\n",
      "Step 187624) Time = 126.626086\n",
      "Train loss = 0.00038 | mse = 0.00035 | KL = 0.00004\n",
      "Validation loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013823393965139985 | Global gradient norm: 96.85\n",
      "Step 187812) Time = 125.379215\n",
      "Train loss = 0.00043 | mse = 0.00038 | KL = 0.00005\n",
      "Validation loss = 0.00044 | mse = 0.00039 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013796037819702178 | Global gradient norm: 96.86\n",
      "Step 188000) Time = 123.336083\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013768741337116808 | Global gradient norm: 96.88\n",
      "Step 188188) Time = 119.828081\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001374149287585169 | Global gradient norm: 96.89\n",
      "Step 188376) Time = 120.175154\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013714299711864442 | Global gradient norm: 96.90\n",
      "Step 188564) Time = 125.411085\n",
      "Train loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013687163300346583 | Global gradient norm: 96.90\n",
      "Step 188752) Time = 121.544082\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013660077820532024 | Global gradient norm: 96.91\n",
      "Step 188940) Time = 126.625266\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013633047637995332 | Global gradient norm: 96.92\n",
      "Step 189128) Time = 124.719084\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013606071297544986 | Global gradient norm: 96.93\n",
      "Step 189316) Time = 120.916191\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001357914588879794 | Global gradient norm: 96.93\n",
      "Step 189504) Time = 120.915158\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013552277232520282 | Global gradient norm: 96.95\n",
      "Step 189692) Time = 119.484081\n",
      "Train loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "Validation loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013525458052754402 | Global gradient norm: 96.96\n",
      "Step 189880) Time = 117.345142\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013498692715074867 | Global gradient norm: 96.98\n",
      "Step 190068) Time = 119.852129\n",
      "Train loss = 0.00040 | mse = 0.00037 | KL = 0.00004\n",
      "Validation loss = 0.00041 | mse = 0.00037 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013471984129864722 | Global gradient norm: 96.99\n",
      "Step 190256) Time = 119.323272\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013445323565974832 | Global gradient norm: 97.01\n",
      "Step 190444) Time = 118.587080\n",
      "Train loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "Validation loss = 0.00040 | mse = 0.00036 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001341871975455433 | Global gradient norm: 97.03\n",
      "Step 190632) Time = 124.557146\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013392163964454085 | Global gradient norm: 97.05\n",
      "Step 190820) Time = 119.673202\n",
      "Train loss = 0.00040 | mse = 0.00036 | KL = 0.00004\n",
      "Validation loss = 0.00040 | mse = 0.00037 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013365664926823229 | Global gradient norm: 97.05\n",
      "Step 191008) Time = 119.758213\n",
      "Train loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013339216820895672 | Global gradient norm: 97.07\n",
      "Step 191196) Time = 120.538081\n",
      "Train loss = 0.00039 | mse = 0.00036 | KL = 0.00004\n",
      "Validation loss = 0.00039 | mse = 0.00035 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013312819646671414 | Global gradient norm: 97.09\n",
      "Step 191384) Time = 118.482145\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001328647485934198 | Global gradient norm: 97.10\n",
      "Step 191572) Time = 121.782231\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013260186824481934 | Global gradient norm: 97.10\n",
      "Step 191760) Time = 122.874192\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001323394535575062 | Global gradient norm: 97.12\n",
      "Step 191948) Time = 121.016081\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013207759184297174 | Global gradient norm: 97.13\n",
      "Step 192136) Time = 123.783084\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013181621034163982 | Global gradient norm: 97.14\n",
      "Step 192324) Time = 119.635121\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013155536726117134 | Global gradient norm: 97.15\n",
      "Step 192512) Time = 122.231146\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013129506260156631 | Global gradient norm: 97.16\n",
      "Step 192700) Time = 121.668408\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013103523815516382 | Global gradient norm: 97.17\n",
      "Step 192888) Time = 119.877081\n",
      "Train loss = 0.00040 | mse = 0.00037 | KL = 0.00004\n",
      "Validation loss = 0.00040 | mse = 0.00037 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013077596668154 | Global gradient norm: 97.18\n",
      "Step 193076) Time = 122.761257\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013051717542111874 | Global gradient norm: 97.19\n",
      "Step 193264) Time = 130.312934\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013025889347773045 | Global gradient norm: 97.18\n",
      "Step 193452) Time = 132.258400\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00013000114995520562 | Global gradient norm: 97.19\n",
      "Step 193640) Time = 129.212560\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00012974388664588332 | Global gradient norm: 97.19\n",
      "Step 193828) Time = 125.540977\n",
      "Train loss = 0.00039 | mse = 0.00036 | KL = 0.00004\n",
      "Validation loss = 0.00040 | mse = 0.00036 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001294871763093397 | Global gradient norm: 97.20\n",
      "Step 194016) Time = 123.547652\n",
      "Train loss = 0.00042 | mse = 0.00038 | KL = 0.00004\n",
      "Validation loss = 0.00042 | mse = 0.00038 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00012923091708216816 | Global gradient norm: 97.21\n",
      "Step 194204) Time = 120.947835\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00032 | KL = 0.00005\n",
      "================================================\n",
      "Learning rate: 0.00012897518172394484 | Global gradient norm: 97.22\n",
      "Step 194392) Time = 121.098022\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00012871997023466974 | Global gradient norm: 97.23\n",
      "Step 194580) Time = 127.957423\n",
      "Train loss = 0.00034 | mse = 0.00030 | KL = 0.00004\n",
      "Validation loss = 0.00034 | mse = 0.00030 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00012846526806242764 | Global gradient norm: 97.24\n",
      "Step 194768) Time = 127.731874\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00012821104610338807 | Global gradient norm: 97.25\n",
      "Step 194956) Time = 125.967827\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00012795734801329672 | Global gradient norm: 97.26\n",
      "Step 195144) Time = 127.243575\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00012770414468832314 | Global gradient norm: 97.28\n",
      "Step 195332) Time = 124.881306\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00012745145068038255 | Global gradient norm: 97.29\n",
      "Step 195520) Time = 122.629059\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001271992368856445 | Global gradient norm: 97.31\n",
      "Step 195708) Time = 121.835974\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00012694753240793943 | Global gradient norm: 97.32\n",
      "Step 195896) Time = 120.318085\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00012669632269535214 | Global gradient norm: 97.35\n",
      "Step 196084) Time = 127.809402\n",
      "Train loss = 0.00034 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001264456077478826 | Global gradient norm: 97.37\n",
      "Step 196272) Time = 126.758201\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001261954166693613 | Global gradient norm: 97.37\n",
      "Step 196460) Time = 126.090549\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001259456912521273 | Global gradient norm: 97.40\n",
      "Step 196648) Time = 124.001731\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00012569647515192628 | Global gradient norm: 97.40\n",
      "Step 196836) Time = 125.276731\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00012544775381684303 | Global gradient norm: 97.41\n",
      "Step 197024) Time = 121.213005\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001251994981430471 | Global gradient norm: 97.43\n",
      "Step 197212) Time = 122.230548\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00012495173723436892 | Global gradient norm: 97.44\n",
      "Step 197400) Time = 119.274044\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00012470450019463897 | Global gradient norm: 97.45\n",
      "Step 197588) Time = 128.093128\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.0001244577142642811 | Global gradient norm: 97.47\n",
      "Step 197776) Time = 125.390283\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00012421145220287144 | Global gradient norm: 97.48\n",
      "Step 197964) Time = 121.696676\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.0001239656558027491 | Global gradient norm: 97.47\n",
      "Step 198152) Time = 122.444921\n",
      "Train loss = 0.00044 | mse = 0.00040 | KL = 0.00004\n",
      "Validation loss = 0.00045 | mse = 0.00041 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001237203396158293 | Global gradient norm: 97.48\n",
      "Step 198340) Time = 121.972095\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00012347553274594247 | Global gradient norm: 97.50\n",
      "Step 198528) Time = 118.080415\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00012323119153734297 | Global gradient norm: 97.52\n",
      "Step 198716) Time = 119.634084\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00012298734509386122 | Global gradient norm: 97.51\n",
      "Step 198904) Time = 118.432186\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00012274397886358202 | Global gradient norm: 97.52\n",
      "Step 199092) Time = 121.716149\n",
      "Train loss = 0.00046 | mse = 0.00042 | KL = 0.00004\n",
      "Validation loss = 0.00045 | mse = 0.00041 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00012250107829459012 | Global gradient norm: 97.53\n",
      "Step 199280) Time = 120.029226\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.0001222586870426312 | Global gradient norm: 97.54\n",
      "Step 199468) Time = 116.654201\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00012201674690004438 | Global gradient norm: 97.54\n",
      "Step 199656) Time = 123.034083\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00012177528697066009 | Global gradient norm: 97.55\n",
      "Step 199844) Time = 126.699214\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00012153432908235118 | Global gradient norm: 97.56\n",
      "Step 200032) Time = 122.255193\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00012129383685532957 | Global gradient norm: 97.57\n",
      "Step 200220) Time = 120.443128\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00012105383211746812 | Global gradient norm: 97.58\n",
      "Step 200408) Time = 123.225236\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00033 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00012081427121302113 | Global gradient norm: 97.59\n",
      "Step 200596) Time = 120.736206\n",
      "Train loss = 0.00034 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00012057519779773429 | Global gradient norm: 97.61\n",
      "Step 200784) Time = 122.505083\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001203366118716076 | Global gradient norm: 97.63\n",
      "Step 200972) Time = 120.429199\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.0001200984843308106 | Global gradient norm: 97.65\n",
      "Step 201160) Time = 121.224322\n",
      "Train loss = 0.00034 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011986084427917376 | Global gradient norm: 97.68\n",
      "Step 201348) Time = 120.260190\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.000119623655336909 | Global gradient norm: 97.68\n",
      "Step 201536) Time = 117.681317\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011938693205593154 | Global gradient norm: 97.69\n",
      "Step 201724) Time = 126.436207\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011915069626411423 | Global gradient norm: 97.71\n",
      "Step 201912) Time = 125.394211\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011891490430571139 | Global gradient norm: 97.73\n",
      "Step 202100) Time = 121.647253\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011867958528455347 | Global gradient norm: 97.74\n",
      "Step 202288) Time = 121.691323\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011844476102851331 | Global gradient norm: 97.75\n",
      "Step 202476) Time = 118.610145\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.0001182103660539724 | Global gradient norm: 97.77\n",
      "Step 202664) Time = 119.910336\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011797646584454924 | Global gradient norm: 97.78\n",
      "Step 202852) Time = 121.853160\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011774300946854055 | Global gradient norm: 97.80\n",
      "Step 203040) Time = 117.893269\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011751000420190394 | Global gradient norm: 97.79\n",
      "Step 203228) Time = 125.499167\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.0001172774937003851 | Global gradient norm: 97.80\n",
      "Step 203416) Time = 119.653191\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00011704540520440787 | Global gradient norm: 97.81\n",
      "Step 203604) Time = 115.297143\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.0001168138041975908 | Global gradient norm: 97.82\n",
      "Step 203792) Time = 120.073169\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011658264702418819 | Global gradient norm: 97.83\n",
      "Step 203980) Time = 119.463289\n",
      "Train loss = 0.00072 | mse = 0.00068 | KL = 0.00004\n",
      "Validation loss = 0.00072 | mse = 0.00069 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011635194096015766 | Global gradient norm: 97.85\n",
      "Step 204168) Time = 125.022313\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011612171510932967 | Global gradient norm: 97.85\n",
      "Step 204356) Time = 131.526379\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011589192581595853 | Global gradient norm: 97.86\n",
      "Step 204544) Time = 132.644208\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011566258763195947 | Global gradient norm: 97.86\n",
      "Step 204732) Time = 125.272636\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011543372238520533 | Global gradient norm: 97.87\n",
      "Step 204920) Time = 121.900300\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011520529369590804 | Global gradient norm: 97.87\n",
      "Step 205108) Time = 119.255692\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011497733794385567 | Global gradient norm: 97.88\n",
      "Step 205296) Time = 123.227109\n",
      "Train loss = 0.00041 | mse = 0.00037 | KL = 0.00003\n",
      "Validation loss = 0.00040 | mse = 0.00037 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011474981874926016 | Global gradient norm: 97.89\n",
      "Step 205484) Time = 121.675817\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011452273611212149 | Global gradient norm: 97.90\n",
      "Step 205672) Time = 120.501217\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011429612641222775 | Global gradient norm: 97.91\n",
      "Step 205860) Time = 122.499346\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011406994599383324 | Global gradient norm: 97.91\n",
      "Step 206048) Time = 119.657010\n",
      "Train loss = 0.00034 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011384423851268366 | Global gradient norm: 97.93\n",
      "Step 206236) Time = 122.871575\n",
      "Train loss = 0.00038 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.0001136189530370757 | Global gradient norm: 97.94\n",
      "Step 206424) Time = 120.866240\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011339411867083982 | Global gradient norm: 97.95\n",
      "Step 206612) Time = 119.639354\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011316974268993363 | Global gradient norm: 97.95\n",
      "Step 206800) Time = 129.478315\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011294579599052668 | Global gradient norm: 97.95\n",
      "Step 206988) Time = 124.308707\n",
      "Train loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "Validation loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.0001127223004004918 | Global gradient norm: 97.97\n",
      "Step 207176) Time = 119.067759\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011249923409195617 | Global gradient norm: 97.98\n",
      "Step 207364) Time = 120.433870\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00011227661889279261 | Global gradient norm: 97.99\n",
      "Step 207552) Time = 118.313875\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011205445480300114 | Global gradient norm: 98.00\n",
      "Step 207740) Time = 122.637307\n",
      "Train loss = 0.00040 | mse = 0.00037 | KL = 0.00003\n",
      "Validation loss = 0.00042 | mse = 0.00038 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00011183270544279367 | Global gradient norm: 98.01\n",
      "Step 207928) Time = 121.511061\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011161139991600066 | Global gradient norm: 98.01\n",
      "Step 208116) Time = 119.505274\n",
      "Train loss = 0.00047 | mse = 0.00043 | KL = 0.00003\n",
      "Validation loss = 0.00046 | mse = 0.00043 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011139056005049497 | Global gradient norm: 98.03\n",
      "Step 208304) Time = 130.445829\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011117012763861567 | Global gradient norm: 98.03\n",
      "Step 208492) Time = 122.664811\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011095015361206606 | Global gradient norm: 98.04\n",
      "Step 208680) Time = 117.049371\n",
      "Train loss = 0.00042 | mse = 0.00039 | KL = 0.00003\n",
      "Validation loss = 0.00043 | mse = 0.00039 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011073060159105808 | Global gradient norm: 98.04\n",
      "Step 208868) Time = 128.307779\n",
      "Train loss = 0.00044 | mse = 0.00038 | KL = 0.00006\n",
      "Validation loss = 0.00044 | mse = 0.00038 | KL = 0.00006\n",
      "================================================\n",
      "Learning rate: 0.00011051147157559171 | Global gradient norm: 98.05\n",
      "Step 209056) Time = 129.467242\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00011029280722141266 | Global gradient norm: 98.06\n",
      "Step 209244) Time = 126.237568\n",
      "Train loss = 0.00037 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00011007454304490238 | Global gradient norm: 98.07\n",
      "Step 209432) Time = 123.478222\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010985675180563703 | Global gradient norm: 98.08\n",
      "Step 209620) Time = 119.313390\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010963935346808285 | Global gradient norm: 98.09\n",
      "Step 209808) Time = 123.289083\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.0001094223844120279 | Global gradient norm: 98.10\n",
      "Step 209996) Time = 121.853083\n",
      "Train loss = 0.00033 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010920586646534503 | Global gradient norm: 98.12\n",
      "Step 210184) Time = 115.677076\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010898975597228855 | Global gradient norm: 98.14\n",
      "Step 210372) Time = 120.284283\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010877409658860415 | Global gradient norm: 98.15\n",
      "Step 210560) Time = 123.057194\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010855885193450376 | Global gradient norm: 98.17\n",
      "Step 210748) Time = 118.469206\n",
      "Train loss = 0.00034 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.000108344029285945 | Global gradient norm: 98.18\n",
      "Step 210936) Time = 125.884207\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.0001081296504708007 | Global gradient norm: 98.20\n",
      "Step 211124) Time = 126.490255\n",
      "Train loss = 0.00036 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010791567183332518 | Global gradient norm: 98.21\n",
      "Step 211312) Time = 125.453085\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010770210792543367 | Global gradient norm: 98.21\n",
      "Step 211500) Time = 121.283084\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010748901695478708 | Global gradient norm: 98.23\n",
      "Step 211688) Time = 116.863192\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010727630433393642 | Global gradient norm: 98.24\n",
      "Step 211876) Time = 118.885081\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010706402827054262 | Global gradient norm: 98.25\n",
      "Step 212064) Time = 117.480080\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010685215966077521 | Global gradient norm: 98.26\n",
      "Step 212252) Time = 117.224079\n",
      "Train loss = 0.00039 | mse = 0.00036 | KL = 0.00003\n",
      "Validation loss = 0.00041 | mse = 0.00037 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00010664070578059182 | Global gradient norm: 98.27\n",
      "Step 212440) Time = 122.925213\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010642969573382288 | Global gradient norm: 98.28\n",
      "Step 212628) Time = 118.268315\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00010621909314068034 | Global gradient norm: 98.29\n",
      "Step 212816) Time = 119.077080\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010600889072520658 | Global gradient norm: 98.30\n",
      "Step 213004) Time = 119.804081\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.0001057991394191049 | Global gradient norm: 98.31\n",
      "Step 213192) Time = 119.887081\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010558976646279916 | Global gradient norm: 98.32\n",
      "Step 213380) Time = 51.345035\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010538083733990788 | Global gradient norm: 98.32\n",
      "Step 213568) Time = 118.268210\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010517229384277016 | Global gradient norm: 98.33\n",
      "Step 213756) Time = 118.053080\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010496417962713167 | Global gradient norm: 98.34\n",
      "Step 213944) Time = 116.398225\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.0001047564874170348 | Global gradient norm: 98.35\n",
      "Step 214132) Time = 118.495080\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010454918083269149 | Global gradient norm: 98.37\n",
      "Step 214320) Time = 115.754142\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010434231080580503 | Global gradient norm: 98.39\n",
      "Step 214508) Time = 116.105143\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010413582640467212 | Global gradient norm: 98.39\n",
      "Step 214696) Time = 116.671079\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010392976400908083 | Global gradient norm: 98.41\n",
      "Step 214884) Time = 114.866187\n",
      "Train loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "Validation loss = 0.00039 | mse = 0.00036 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010372411634307355 | Global gradient norm: 98.42\n",
      "Step 215072) Time = 114.589128\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010351885430281982 | Global gradient norm: 98.43\n",
      "Step 215260) Time = 115.988198\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010331399971619248 | Global gradient norm: 98.44\n",
      "Step 215448) Time = 115.862087\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010310956713510677 | Global gradient norm: 98.46\n",
      "Step 215636) Time = 118.298690\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010290552745573223 | Global gradient norm: 98.47\n",
      "Step 215824) Time = 127.273964\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010270191705785692 | Global gradient norm: 98.48\n",
      "Step 216012) Time = 125.733943\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010249867773381993 | Global gradient norm: 98.49\n",
      "Step 216200) Time = 118.347883\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010229583858745173 | Global gradient norm: 98.50\n",
      "Step 216388) Time = 115.204880\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010209342872258276 | Global gradient norm: 98.51\n",
      "Step 216576) Time = 124.745393\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010189138993155211 | Global gradient norm: 98.53\n",
      "Step 216764) Time = 120.387887\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010168977314606309 | Global gradient norm: 98.54\n",
      "Step 216952) Time = 116.520893\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00004\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 0.00010148854926228523 | Global gradient norm: 98.55\n",
      "Step 217140) Time = 120.200315\n",
      "Train loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "Validation loss = 0.00039 | mse = 0.00036 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010128771100426093 | Global gradient norm: 98.55\n",
      "Step 217328) Time = 128.909958\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010108729475177824 | Global gradient norm: 98.56\n",
      "Step 217516) Time = 128.396376\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010088724957313389 | Global gradient norm: 98.57\n",
      "Step 217704) Time = 123.566470\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010068760457215831 | Global gradient norm: 98.58\n",
      "Step 217892) Time = 120.270557\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010048838157672435 | Global gradient norm: 98.59\n",
      "Step 218080) Time = 119.992433\n",
      "Train loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "Validation loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010028952965512872 | Global gradient norm: 98.59\n",
      "Step 218268) Time = 119.929706\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 0.00010009109246311709 | Global gradient norm: 98.60\n",
      "Step 218456) Time = 119.757487\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.989301906898618e-05 | Global gradient norm: 98.61\n",
      "Step 218644) Time = 124.680408\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.969533857656643e-05 | Global gradient norm: 98.62\n",
      "Step 218832) Time = 124.578272\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.949805826181546e-05 | Global gradient norm: 98.62\n",
      "Step 219020) Time = 124.363548\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.930117084877566e-05 | Global gradient norm: 98.63\n",
      "Step 219208) Time = 127.830406\n",
      "Train loss = 0.00035 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.910469088936225e-05 | Global gradient norm: 98.64\n",
      "Step 219396) Time = 126.302014\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.890856745187193e-05 | Global gradient norm: 98.65\n",
      "Step 219584) Time = 124.768948\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.871283691609278e-05 | Global gradient norm: 98.66\n",
      "Step 219772) Time = 126.040972\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.851752110989764e-05 | Global gradient norm: 98.67\n",
      "Step 219960) Time = 123.732511\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.832256910158321e-05 | Global gradient norm: 98.68\n",
      "Step 220148) Time = 125.152655\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.81279881671071e-05 | Global gradient norm: 98.68\n",
      "Step 220336) Time = 123.204294\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.7933821962215e-05 | Global gradient norm: 98.69\n",
      "Step 220524) Time = 119.318449\n",
      "Train loss = 0.00039 | mse = 0.00036 | KL = 0.00003\n",
      "Validation loss = 0.00039 | mse = 0.00036 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.7740012279246e-05 | Global gradient norm: 98.70\n",
      "Step 220712) Time = 121.672472\n",
      "Train loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.754661732586101e-05 | Global gradient norm: 98.71\n",
      "Step 220900) Time = 120.408264\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.735358617035672e-05 | Global gradient norm: 98.73\n",
      "Step 221088) Time = 116.541360\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.716094064060599e-05 | Global gradient norm: 98.73\n",
      "Step 221276) Time = 119.850119\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.696869528852403e-05 | Global gradient norm: 98.74\n",
      "Step 221464) Time = 118.538153\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.677679190644994e-05 | Global gradient norm: 98.75\n",
      "Step 221652) Time = 127.814086\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.658530325395986e-05 | Global gradient norm: 98.76\n",
      "Step 221840) Time = 120.542081\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.639417112339288e-05 | Global gradient norm: 98.76\n",
      "Step 222028) Time = 116.754275\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.620343189453706e-05 | Global gradient norm: 98.77\n",
      "Step 222216) Time = 120.602271\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.601306373951957e-05 | Global gradient norm: 98.78\n",
      "Step 222404) Time = 118.272262\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.582305938238278e-05 | Global gradient norm: 98.79\n",
      "Step 222592) Time = 117.084121\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.563346247887239e-05 | Global gradient norm: 98.80\n",
      "Step 222780) Time = 121.695082\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.544420754536986e-05 | Global gradient norm: 98.81\n",
      "Step 222968) Time = 119.116081\n",
      "Train loss = 0.00040 | mse = 0.00037 | KL = 0.00003\n",
      "Validation loss = 0.00041 | mse = 0.00038 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.525533823762089e-05 | Global gradient norm: 98.82\n",
      "Step 223156) Time = 121.387082\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.506684727966785e-05 | Global gradient norm: 98.83\n",
      "Step 223344) Time = 121.135082\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.487872739555314e-05 | Global gradient norm: 98.83\n",
      "Step 223532) Time = 118.564190\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.469097130931914e-05 | Global gradient norm: 98.84\n",
      "Step 223720) Time = 119.440151\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.450360084883869e-05 | Global gradient norm: 98.85\n",
      "Step 223908) Time = 122.290083\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.431658691028133e-05 | Global gradient norm: 98.86\n",
      "Step 224096) Time = 117.167200\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.412996587343514e-05 | Global gradient norm: 98.87\n",
      "Step 224284) Time = 121.200159\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.394369408255443e-05 | Global gradient norm: 98.88\n",
      "Step 224472) Time = 119.537081\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.375779336551204e-05 | Global gradient norm: 98.88\n",
      "Step 224660) Time = 121.173159\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.357227827422321e-05 | Global gradient norm: 98.89\n",
      "Step 224848) Time = 119.824147\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.338710515294224e-05 | Global gradient norm: 98.90\n",
      "Step 225036) Time = 116.787139\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.320231038145721e-05 | Global gradient norm: 98.91\n",
      "Step 225224) Time = 125.788205\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.301787213189527e-05 | Global gradient norm: 98.92\n",
      "Step 225412) Time = 125.724220\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.283380495617166e-05 | Global gradient norm: 98.93\n",
      "Step 225600) Time = 122.480332\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.26501234062016e-05 | Global gradient norm: 98.94\n",
      "Step 225788) Time = 124.419150\n",
      "Train loss = 0.00036 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.246676927432418e-05 | Global gradient norm: 98.95\n",
      "Step 225976) Time = 120.100184\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.228378621628508e-05 | Global gradient norm: 98.96\n",
      "Step 226164) Time = 118.621142\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.210118878399953e-05 | Global gradient norm: 98.97\n",
      "Step 226352) Time = 119.864081\n",
      "Train loss = 0.00040 | mse = 0.00038 | KL = 0.00003\n",
      "Validation loss = 0.00041 | mse = 0.00038 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.191894059767947e-05 | Global gradient norm: 98.97\n",
      "Step 226540) Time = 119.238652\n",
      "Train loss = 0.00046 | mse = 0.00043 | KL = 0.00003\n",
      "Validation loss = 0.00045 | mse = 0.00042 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.17370489332825e-05 | Global gradient norm: 98.98\n",
      "Step 226728) Time = 121.508085\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.155552106676623e-05 | Global gradient norm: 98.99\n",
      "Step 226916) Time = 127.950923\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.137433517025784e-05 | Global gradient norm: 98.99\n",
      "Step 227104) Time = 129.335474\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.119353489950299e-05 | Global gradient norm: 98.99\n",
      "Step 227292) Time = 127.489319\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.101306204684079e-05 | Global gradient norm: 98.99\n",
      "Step 227480) Time = 121.443120\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.083298937184736e-05 | Global gradient norm: 99.00\n",
      "Step 227668) Time = 119.623606\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.065323683898896e-05 | Global gradient norm: 99.00\n",
      "Step 227856) Time = 120.724903\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.047384082805365e-05 | Global gradient norm: 99.01\n",
      "Step 228044) Time = 119.081921\n",
      "Train loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "Validation loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.029482316691428e-05 | Global gradient norm: 99.02\n",
      "Step 228232) Time = 122.142590\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 9.011614019982517e-05 | Global gradient norm: 99.04\n",
      "Step 228420) Time = 119.781051\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.993781375465915e-05 | Global gradient norm: 99.04\n",
      "Step 228608) Time = 117.984054\n",
      "Train loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "Validation loss = 0.00041 | mse = 0.00037 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.975985110737383e-05 | Global gradient norm: 99.04\n",
      "Step 228796) Time = 121.108500\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.958222315413877e-05 | Global gradient norm: 99.05\n",
      "Step 228984) Time = 122.804159\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.940496627474204e-05 | Global gradient norm: 99.05\n",
      "Step 229172) Time = 122.769732\n",
      "Train loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.922804408939555e-05 | Global gradient norm: 99.06\n",
      "Step 229360) Time = 123.150431\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00030 | KL = 0.00004\n",
      "================================================\n",
      "Learning rate: 8.905146387405694e-05 | Global gradient norm: 99.06\n",
      "Step 229548) Time = 118.629697\n",
      "Train loss = 0.00039 | mse = 0.00036 | KL = 0.00003\n",
      "Validation loss = 0.00040 | mse = 0.00037 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.887527656042948e-05 | Global gradient norm: 99.07\n",
      "Step 229736) Time = 126.328371\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.869939483702183e-05 | Global gradient norm: 99.07\n",
      "Step 229924) Time = 123.660452\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.852387691149488e-05 | Global gradient norm: 99.08\n",
      "Step 230112) Time = 119.444005\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.83487009559758e-05 | Global gradient norm: 99.08\n",
      "Step 230300) Time = 120.483090\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.817386697046459e-05 | Global gradient norm: 99.10\n",
      "Step 230488) Time = 124.457846\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.799939678283408e-05 | Global gradient norm: 99.11\n",
      "Step 230676) Time = 122.163518\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.782526128925383e-05 | Global gradient norm: 99.12\n",
      "Step 230864) Time = 121.951723\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.765146048972383e-05 | Global gradient norm: 99.13\n",
      "Step 231052) Time = 123.811599\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.747803076403216e-05 | Global gradient norm: 99.14\n",
      "Step 231240) Time = 120.716613\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.730491390451789e-05 | Global gradient norm: 99.15\n",
      "Step 231428) Time = 122.329129\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.713216084288433e-05 | Global gradient norm: 99.16\n",
      "Step 231616) Time = 120.070458\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.69597279233858e-05 | Global gradient norm: 99.17\n",
      "Step 231804) Time = 122.124403\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.678765880176798e-05 | Global gradient norm: 99.18\n",
      "Step 231992) Time = 126.132573\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.661593165015802e-05 | Global gradient norm: 99.19\n",
      "Step 232180) Time = 121.878123\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.644452464068308e-05 | Global gradient norm: 99.20\n",
      "Step 232368) Time = 119.599150\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.627348142908886e-05 | Global gradient norm: 99.20\n",
      "Step 232556) Time = 122.920127\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.610275108367205e-05 | Global gradient norm: 99.20\n",
      "Step 232744) Time = 117.634196\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.59323627082631e-05 | Global gradient norm: 99.21\n",
      "Step 232932) Time = 124.005207\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.576233085477725e-05 | Global gradient norm: 99.22\n",
      "Step 233120) Time = 122.924124\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.559261914342642e-05 | Global gradient norm: 99.23\n",
      "Step 233308) Time = 121.006637\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.542324212612584e-05 | Global gradient norm: 99.23\n",
      "Step 233496) Time = 123.479084\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.525421435479075e-05 | Global gradient norm: 99.24\n",
      "Step 233684) Time = 121.440082\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.508549217367545e-05 | Global gradient norm: 99.25\n",
      "Step 233872) Time = 120.641082\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.491714106639847e-05 | Global gradient norm: 99.25\n",
      "Step 234060) Time = 124.452206\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.474909554934129e-05 | Global gradient norm: 99.26\n",
      "Step 234248) Time = 121.089082\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.458139200229198e-05 | Global gradient norm: 99.27\n",
      "Step 234436) Time = 119.823194\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.441403042525053e-05 | Global gradient norm: 99.28\n",
      "Step 234624) Time = 117.251201\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.424698899034411e-05 | Global gradient norm: 99.29\n",
      "Step 234812) Time = 121.682106\n",
      "Train loss = 0.00039 | mse = 0.00036 | KL = 0.00003\n",
      "Validation loss = 0.00041 | mse = 0.00038 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.408028224948794e-05 | Global gradient norm: 99.29\n",
      "Step 235000) Time = 120.238203\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.39139029267244e-05 | Global gradient norm: 99.30\n",
      "Step 235188) Time = 117.445186\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.374783647013828e-05 | Global gradient norm: 99.30\n",
      "Step 235376) Time = 119.687207\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.358212653547525e-05 | Global gradient norm: 99.30\n",
      "Step 235564) Time = 120.432311\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.341672946698964e-05 | Global gradient norm: 99.31\n",
      "Step 235752) Time = 118.409080\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.325165254063904e-05 | Global gradient norm: 99.32\n",
      "Step 235940) Time = 128.042124\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00038 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.308692486025393e-05 | Global gradient norm: 99.33\n",
      "Step 236128) Time = 121.901083\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.292251004604623e-05 | Global gradient norm: 99.34\n",
      "Step 236316) Time = 117.427187\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.275842992588878e-05 | Global gradient norm: 99.35\n",
      "Step 236504) Time = 119.037126\n",
      "Train loss = 0.00031 | mse = 0.00028 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.259466267190874e-05 | Global gradient norm: 99.35\n",
      "Step 236692) Time = 116.036104\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.243121556006372e-05 | Global gradient norm: 99.36\n",
      "Step 236880) Time = 118.451256\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.226810314226896e-05 | Global gradient norm: 99.37\n",
      "Step 237068) Time = 123.693084\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.21053035906516e-05 | Global gradient norm: 99.38\n",
      "Step 237256) Time = 121.995083\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.194284600904211e-05 | Global gradient norm: 99.39\n",
      "Step 237444) Time = 119.715132\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.178069401765242e-05 | Global gradient norm: 99.41\n",
      "Step 237632) Time = 120.517948\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.161885489244014e-05 | Global gradient norm: 99.42\n",
      "Step 237820) Time = 119.474805\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.145735773723572e-05 | Global gradient norm: 99.43\n",
      "Step 238008) Time = 125.916550\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.129615889629349e-05 | Global gradient norm: 99.43\n",
      "Step 238196) Time = 125.265671\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.113530202535912e-05 | Global gradient norm: 99.44\n",
      "Step 238384) Time = 122.833348\n",
      "Train loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "Validation loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.097473619272932e-05 | Global gradient norm: 99.44\n",
      "Step 238572) Time = 128.901094\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.081449777819216e-05 | Global gradient norm: 99.45\n",
      "Step 238760) Time = 129.278522\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.065459405770525e-05 | Global gradient norm: 99.46\n",
      "Step 238948) Time = 124.882326\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.049498137552291e-05 | Global gradient norm: 99.47\n",
      "Step 239136) Time = 122.239486\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.033569611143321e-05 | Global gradient norm: 99.48\n",
      "Step 239324) Time = 118.879079\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.017673826543614e-05 | Global gradient norm: 99.48\n",
      "Step 239512) Time = 122.076003\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 8.001807873370126e-05 | Global gradient norm: 99.48\n",
      "Step 239700) Time = 129.895467\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.985974662005901e-05 | Global gradient norm: 99.48\n",
      "Step 239888) Time = 129.814483\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.970171282067895e-05 | Global gradient norm: 99.48\n",
      "Step 240076) Time = 124.307899\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.95439918874763e-05 | Global gradient norm: 99.48\n",
      "Step 240264) Time = 120.633907\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.938659837236628e-05 | Global gradient norm: 99.48\n",
      "Step 240452) Time = 123.649548\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.922950317151845e-05 | Global gradient norm: 99.49\n",
      "Step 240640) Time = 121.292446\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.907273538876325e-05 | Global gradient norm: 99.49\n",
      "Step 240828) Time = 118.854802\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.8916251368355e-05 | Global gradient norm: 99.50\n",
      "Step 241016) Time = 121.709623\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.876007293816656e-05 | Global gradient norm: 99.50\n",
      "Step 241204) Time = 122.098061\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.860422920202836e-05 | Global gradient norm: 99.51\n",
      "Step 241392) Time = 120.851982\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.844869105610996e-05 | Global gradient norm: 99.52\n",
      "Step 241580) Time = 121.882440\n",
      "Train loss = 0.00035 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.829345122445375e-05 | Global gradient norm: 99.52\n",
      "Step 241768) Time = 118.466927\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.813852425897494e-05 | Global gradient norm: 99.53\n",
      "Step 241956) Time = 129.424972\n",
      "Train loss = 0.00031 | mse = 0.00028 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.798389560775831e-05 | Global gradient norm: 99.53\n",
      "Step 242144) Time = 126.512311\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.782959437463433e-05 | Global gradient norm: 99.54\n",
      "Step 242332) Time = 123.433244\n",
      "Train loss = 0.00035 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.767557690385729e-05 | Global gradient norm: 99.54\n",
      "Step 242520) Time = 123.269680\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.752187229925767e-05 | Global gradient norm: 99.55\n",
      "Step 242708) Time = 132.911384\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.736848056083545e-05 | Global gradient norm: 99.55\n",
      "Step 242896) Time = 131.222255\n",
      "Train loss = 0.00038 | mse = 0.00036 | KL = 0.00003\n",
      "Validation loss = 0.00038 | mse = 0.00036 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.721537258476019e-05 | Global gradient norm: 99.56\n",
      "Step 243084) Time = 127.708112\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.706257747486234e-05 | Global gradient norm: 99.56\n",
      "Step 243272) Time = 121.867083\n",
      "Train loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.691008067922667e-05 | Global gradient norm: 99.57\n",
      "Step 243460) Time = 122.840083\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.67578967497684e-05 | Global gradient norm: 99.57\n",
      "Step 243648) Time = 122.916274\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.660601113457233e-05 | Global gradient norm: 99.58\n",
      "Step 243836) Time = 121.480193\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.64544092817232e-05 | Global gradient norm: 99.58\n",
      "Step 244024) Time = 122.019121\n",
      "Train loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "Validation loss = 0.00039 | mse = 0.00037 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.630311301909387e-05 | Global gradient norm: 99.59\n",
      "Step 244212) Time = 118.285080\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.615213689859957e-05 | Global gradient norm: 99.60\n",
      "Step 244400) Time = 118.642081\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.600144454045221e-05 | Global gradient norm: 99.60\n",
      "Step 244588) Time = 123.131083\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.585105777252465e-05 | Global gradient norm: 99.60\n",
      "Step 244776) Time = 123.262198\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.570094749098644e-05 | Global gradient norm: 99.61\n",
      "Step 244964) Time = 119.263319\n",
      "Train loss = 0.00036 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.555115735158324e-05 | Global gradient norm: 99.60\n",
      "Step 245152) Time = 121.244198\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.540165825048462e-05 | Global gradient norm: 99.61\n",
      "Step 245340) Time = 118.318247\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.525244291173294e-05 | Global gradient norm: 99.61\n",
      "Step 245528) Time = 120.434138\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.510354043915868e-05 | Global gradient norm: 99.61\n",
      "Step 245716) Time = 126.299085\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.495492172893137e-05 | Global gradient norm: 99.62\n",
      "Step 245904) Time = 127.086159\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.480659405700862e-05 | Global gradient norm: 99.62\n",
      "Step 246092) Time = 123.067142\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.465857925126329e-05 | Global gradient norm: 99.62\n",
      "Step 246280) Time = 124.434084\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.45108409319073e-05 | Global gradient norm: 99.62\n",
      "Step 246468) Time = 122.880153\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.436338637489825e-05 | Global gradient norm: 99.63\n",
      "Step 246656) Time = 120.921286\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.421624468406662e-05 | Global gradient norm: 99.64\n",
      "Step 246844) Time = 118.670147\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.406937947962433e-05 | Global gradient norm: 99.64\n",
      "Step 247032) Time = 121.461252\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.392281986540183e-05 | Global gradient norm: 99.65\n",
      "Step 247220) Time = 129.684212\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.377652946161106e-05 | Global gradient norm: 99.65\n",
      "Step 247408) Time = 129.454087\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.363053009612486e-05 | Global gradient norm: 99.66\n",
      "Step 247596) Time = 123.274316\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.348484359681606e-05 | Global gradient norm: 99.67\n",
      "Step 247784) Time = 120.282081\n",
      "Train loss = 0.00031 | mse = 0.00028 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.333942630793899e-05 | Global gradient norm: 99.68\n",
      "Step 247972) Time = 117.848079\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00031 | mse = 0.00028 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.31943073333241e-05 | Global gradient norm: 99.69\n",
      "Step 248160) Time = 120.632190\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.304945756914094e-05 | Global gradient norm: 99.70\n",
      "Step 248348) Time = 117.826080\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.290490611921996e-05 | Global gradient norm: 99.71\n",
      "Step 248536) Time = 119.507617\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.276064570760354e-05 | Global gradient norm: 99.72\n",
      "Step 248724) Time = 123.449964\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.261666905833408e-05 | Global gradient norm: 99.72\n",
      "Step 248912) Time = 50.425562\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.247296161949635e-05 | Global gradient norm: 99.73\n",
      "Step 249100) Time = 119.153863\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00031 | mse = 0.00028 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.23295597708784e-05 | Global gradient norm: 99.74\n",
      "Step 249288) Time = 121.956380\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.218642713269219e-05 | Global gradient norm: 99.75\n",
      "Step 249476) Time = 129.882117\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.204359280876815e-05 | Global gradient norm: 99.75\n",
      "Step 249664) Time = 128.768871\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.190102769527584e-05 | Global gradient norm: 99.76\n",
      "Step 249852) Time = 119.797954\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.175874634413049e-05 | Global gradient norm: 99.76\n",
      "Step 250040) Time = 122.515153\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.161674875533208e-05 | Global gradient norm: 99.78\n",
      "Step 250228) Time = 120.526282\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.147503492888063e-05 | Global gradient norm: 99.79\n",
      "Step 250416) Time = 129.758348\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.133360486477613e-05 | Global gradient norm: 99.79\n",
      "Step 250604) Time = 128.340928\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.119243673514575e-05 | Global gradient norm: 99.80\n",
      "Step 250792) Time = 120.798048\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.105156691977754e-05 | Global gradient norm: 99.81\n",
      "Step 250980) Time = 123.664895\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.091097359079868e-05 | Global gradient norm: 99.81\n",
      "Step 251168) Time = 121.720110\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.077064947225153e-05 | Global gradient norm: 99.82\n",
      "Step 251356) Time = 121.090809\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.063060911605135e-05 | Global gradient norm: 99.83\n",
      "Step 251544) Time = 121.186021\n",
      "Train loss = 0.00035 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.049083797028288e-05 | Global gradient norm: 99.83\n",
      "Step 251732) Time = 118.759352\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.035135058686137e-05 | Global gradient norm: 99.83\n",
      "Step 251920) Time = 121.459221\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.021214696578681e-05 | Global gradient norm: 99.84\n",
      "Step 252108) Time = 120.164051\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 7.007319800322875e-05 | Global gradient norm: 99.84\n",
      "Step 252296) Time = 120.779731\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.993453280301765e-05 | Global gradient norm: 99.85\n",
      "Step 252484) Time = 123.085939\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.97961586411111e-05 | Global gradient norm: 99.85\n",
      "Step 252672) Time = 121.155395\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.965803186176345e-05 | Global gradient norm: 99.85\n",
      "Step 252860) Time = 116.172300\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.952020339667797e-05 | Global gradient norm: 99.86\n",
      "Step 253048) Time = 120.179650\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.93826368660666e-05 | Global gradient norm: 99.87\n",
      "Step 253236) Time = 118.620551\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.924533226992935e-05 | Global gradient norm: 99.87\n",
      "Step 253424) Time = 118.653753\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.910831143613905e-05 | Global gradient norm: 99.88\n",
      "Step 253612) Time = 116.183293\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.897155253682286e-05 | Global gradient norm: 99.89\n",
      "Step 253800) Time = 118.920614\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.883508467581123e-05 | Global gradient norm: 99.90\n",
      "Step 253988) Time = 119.165214\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.869886419735849e-05 | Global gradient norm: 99.90\n",
      "Step 254176) Time = 116.169309\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.856291292933747e-05 | Global gradient norm: 99.91\n",
      "Step 254364) Time = 122.444088\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.842724542366341e-05 | Global gradient norm: 99.91\n",
      "Step 254552) Time = 120.090171\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.829184712842107e-05 | Global gradient norm: 99.92\n",
      "Step 254740) Time = 117.431114\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.815669621573761e-05 | Global gradient norm: 99.92\n",
      "Step 254928) Time = 119.482123\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.802183634135872e-05 | Global gradient norm: 99.93\n",
      "Step 255116) Time = 120.142191\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.788723840145394e-05 | Global gradient norm: 99.93\n",
      "Step 255304) Time = 115.963224\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.775290967198089e-05 | Global gradient norm: 99.94\n",
      "Step 255492) Time = 124.272200\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.761882832506672e-05 | Global gradient norm: 99.95\n",
      "Step 255680) Time = 122.725135\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.748501618858427e-05 | Global gradient norm: 99.96\n",
      "Step 255868) Time = 121.417206\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.735148053849116e-05 | Global gradient norm: 99.97\n",
      "Step 256056) Time = 119.309081\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.721820682287216e-05 | Global gradient norm: 99.98\n",
      "Step 256244) Time = 117.430122\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.708520231768489e-05 | Global gradient norm: 99.98\n",
      "Step 256432) Time = 120.542082\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.695243791909888e-05 | Global gradient norm: 99.99\n",
      "Step 256620) Time = 119.114080\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.681995728285983e-05 | Global gradient norm: 100.00\n",
      "Step 256808) Time = 117.103237\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.668773858109489e-05 | Global gradient norm: 100.01\n",
      "Step 256996) Time = 120.017145\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.655576726188883e-05 | Global gradient norm: 100.02\n",
      "Step 257184) Time = 117.631237\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.642405787715688e-05 | Global gradient norm: 100.03\n",
      "Step 257372) Time = 128.224087\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.629263225477189e-05 | Global gradient norm: 100.05\n",
      "Step 257560) Time = 122.646082\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.616143946303055e-05 | Global gradient norm: 100.05\n",
      "Step 257748) Time = 118.268127\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.603053043363616e-05 | Global gradient norm: 100.06\n",
      "Step 257936) Time = 120.217164\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.589986151084304e-05 | Global gradient norm: 100.07\n",
      "Step 258124) Time = 122.571085\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.576946179848164e-05 | Global gradient norm: 100.07\n",
      "Step 258312) Time = 119.064205\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.563931674463674e-05 | Global gradient norm: 100.07\n",
      "Step 258500) Time = 119.613196\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.550941907335073e-05 | Global gradient norm: 100.08\n",
      "Step 258688) Time = 119.346081\n",
      "Train loss = 0.00035 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.537979061249644e-05 | Global gradient norm: 100.09\n",
      "Step 258876) Time = 117.290121\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.525042408611625e-05 | Global gradient norm: 100.10\n",
      "Step 259064) Time = 127.316212\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.512129766633734e-05 | Global gradient norm: 100.11\n",
      "Step 259252) Time = 127.541129\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.499244045699015e-05 | Global gradient norm: 100.11\n",
      "Step 259440) Time = 125.361755\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.486381607828662e-05 | Global gradient norm: 100.12\n",
      "Step 259628) Time = 125.215240\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.473546818597242e-05 | Global gradient norm: 100.12\n",
      "Step 259816) Time = 124.958120\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.460736767621711e-05 | Global gradient norm: 100.13\n",
      "Step 260004) Time = 122.091552\n",
      "Train loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.447951454902068e-05 | Global gradient norm: 100.14\n",
      "Step 260192) Time = 120.468561\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.435193790821359e-05 | Global gradient norm: 100.15\n",
      "Step 260380) Time = 118.628182\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.422459409805015e-05 | Global gradient norm: 100.16\n",
      "Step 260568) Time = 120.155397\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.409749767044559e-05 | Global gradient norm: 100.17\n",
      "Step 260756) Time = 118.474303\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.397067045327276e-05 | Global gradient norm: 100.18\n",
      "Step 260944) Time = 121.858676\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.38440833427012e-05 | Global gradient norm: 100.19\n",
      "Step 261132) Time = 119.152055\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.371775816660374e-05 | Global gradient norm: 100.20\n",
      "Step 261320) Time = 119.606322\n",
      "Train loss = 0.00045 | mse = 0.00042 | KL = 0.00003\n",
      "Validation loss = 0.00045 | mse = 0.00043 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.359165854519233e-05 | Global gradient norm: 100.20\n",
      "Step 261508) Time = 127.893273\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.346581358229741e-05 | Global gradient norm: 100.21\n",
      "Step 261696) Time = 123.989688\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.334023782983422e-05 | Global gradient norm: 100.22\n",
      "Step 261884) Time = 118.441489\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.321489490801468e-05 | Global gradient norm: 100.23\n",
      "Step 262072) Time = 130.157738\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.308979936875403e-05 | Global gradient norm: 100.24\n",
      "Step 262260) Time = 131.432274\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.296496576396748e-05 | Global gradient norm: 100.24\n",
      "Step 262448) Time = 126.384558\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.284036498982459e-05 | Global gradient norm: 100.25\n",
      "Step 262636) Time = 122.656223\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.271602615015581e-05 | Global gradient norm: 100.26\n",
      "Step 262824) Time = 120.256968\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.259191286517307e-05 | Global gradient norm: 100.26\n",
      "Step 263012) Time = 130.525337\n",
      "Train loss = 0.00036 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.246805423870683e-05 | Global gradient norm: 100.27\n",
      "Step 263200) Time = 133.026337\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.234444299479946e-05 | Global gradient norm: 100.28\n",
      "Step 263388) Time = 130.387446\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.222107913345098e-05 | Global gradient norm: 100.27\n",
      "Step 263576) Time = 129.877471\n",
      "Train loss = 0.00039 | mse = 0.00037 | KL = 0.00003\n",
      "Validation loss = 0.00039 | mse = 0.00036 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.209795537870377e-05 | Global gradient norm: 100.27\n",
      "Step 263764) Time = 130.682414\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.197507173055783e-05 | Global gradient norm: 100.28\n",
      "Step 263952) Time = 129.130818\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.185242091305554e-05 | Global gradient norm: 100.29\n",
      "Step 264140) Time = 123.442763\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.173003930598497e-05 | Global gradient norm: 100.29\n",
      "Step 264328) Time = 118.379955\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.160788325360045e-05 | Global gradient norm: 100.30\n",
      "Step 264516) Time = 121.479507\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.148596730781719e-05 | Global gradient norm: 100.31\n",
      "Step 264704) Time = 121.873254\n",
      "Train loss = 0.00037 | mse = 0.00035 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.136430602055043e-05 | Global gradient norm: 100.30\n",
      "Step 264892) Time = 119.470926\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.124287028796971e-05 | Global gradient norm: 100.31\n",
      "Step 265080) Time = 120.898850\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.112168921390548e-05 | Global gradient norm: 100.31\n",
      "Step 265268) Time = 118.573915\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.1000737332506105e-05 | Global gradient norm: 100.32\n",
      "Step 265456) Time = 115.005080\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.088002191972919e-05 | Global gradient norm: 100.32\n",
      "Step 265644) Time = 120.273299\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.075956116546877e-05 | Global gradient norm: 100.32\n",
      "Step 265832) Time = 118.282165\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00035 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.063932232791558e-05 | Global gradient norm: 100.32\n",
      "Step 266020) Time = 120.005200\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.051933814887889e-05 | Global gradient norm: 100.33\n",
      "Step 266208) Time = 121.654192\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.039957224857062e-05 | Global gradient norm: 100.33\n",
      "Step 266396) Time = 119.840189\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 6.028005009284243e-05 | Global gradient norm: 100.34\n",
      "Step 266584) Time = 125.000155\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.016077895765193e-05 | Global gradient norm: 100.34\n",
      "Step 266772) Time = 125.791319\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 6.004172610118985e-05 | Global gradient norm: 100.35\n",
      "Step 266960) Time = 124.515202\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.992292062728666e-05 | Global gradient norm: 100.36\n",
      "Step 267148) Time = 123.993264\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.9804337070090696e-05 | Global gradient norm: 100.36\n",
      "Step 267336) Time = 120.189081\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.9685993619496e-05 | Global gradient norm: 100.37\n",
      "Step 267524) Time = 120.885084\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.956789027550258e-05 | Global gradient norm: 100.38\n",
      "Step 267712) Time = 118.880142\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.945001248619519e-05 | Global gradient norm: 100.38\n",
      "Step 267900) Time = 117.244259\n",
      "Train loss = 0.00040 | mse = 0.00037 | KL = 0.00003\n",
      "Validation loss = 0.00040 | mse = 0.00037 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.933236752753146e-05 | Global gradient norm: 100.38\n",
      "Step 268088) Time = 122.073083\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.92149663134478e-05 | Global gradient norm: 100.39\n",
      "Step 268276) Time = 123.838276\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.909778337809257e-05 | Global gradient norm: 100.40\n",
      "Step 268464) Time = 120.887204\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.898084782529622e-05 | Global gradient norm: 100.41\n",
      "Step 268652) Time = 121.240081\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.88641305512283e-05 | Global gradient norm: 100.42\n",
      "Step 268840) Time = 120.244081\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.8747653383761644e-05 | Global gradient norm: 100.42\n",
      "Step 269028) Time = 116.217206\n",
      "Train loss = 0.00037 | mse = 0.00035 | KL = 0.00003\n",
      "Validation loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.8631405408959836e-05 | Global gradient norm: 100.43\n",
      "Step 269216) Time = 120.272081\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.851537935086526e-05 | Global gradient norm: 100.43\n",
      "Step 269404) Time = 116.269235\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.839959339937195e-05 | Global gradient norm: 100.43\n",
      "Step 269592) Time = 118.616079\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.828402572660707e-05 | Global gradient norm: 100.44\n",
      "Step 269780) Time = 119.425081\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.816869088448584e-05 | Global gradient norm: 100.44\n",
      "Step 269968) Time = 117.694296\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.8053588873008266e-05 | Global gradient norm: 100.44\n",
      "Step 270156) Time = 127.661286\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.7938708778237924e-05 | Global gradient norm: 100.44\n",
      "Step 270344) Time = 128.069086\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.782405423815362e-05 | Global gradient norm: 100.44\n",
      "Step 270532) Time = 124.819275\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.770963616669178e-05 | Global gradient norm: 100.45\n",
      "Step 270720) Time = 122.441303\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.759543273597956e-05 | Global gradient norm: 100.45\n",
      "Step 270908) Time = 119.033297\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.748147304984741e-05 | Global gradient norm: 100.45\n",
      "Step 271096) Time = 122.054225\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.736772436648607e-05 | Global gradient norm: 100.46\n",
      "Step 271284) Time = 120.299201\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.725419759983197e-05 | Global gradient norm: 100.46\n",
      "Step 271472) Time = 118.787376\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.7140907301800326e-05 | Global gradient norm: 100.47\n",
      "Step 271660) Time = 121.839061\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.702783528249711e-05 | Global gradient norm: 100.47\n",
      "Step 271848) Time = 121.489990\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.6914996093837544e-05 | Global gradient norm: 100.47\n",
      "Step 272036) Time = 119.777503\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.680236790794879e-05 | Global gradient norm: 100.47\n",
      "Step 272224) Time = 121.277562\n",
      "Train loss = 0.00036 | mse = 0.00034 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.6689958000788465e-05 | Global gradient norm: 100.47\n",
      "Step 272412) Time = 118.893456\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.65777845622506e-05 | Global gradient norm: 100.47\n",
      "Step 272600) Time = 118.483116\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.646582576446235e-05 | Global gradient norm: 100.47\n",
      "Step 272788) Time = 121.081096\n",
      "Train loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.635408524540253e-05 | Global gradient norm: 100.48\n",
      "Step 272976) Time = 119.806755\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.624258119496517e-05 | Global gradient norm: 100.48\n",
      "Step 273164) Time = 121.134512\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.6131280871341005e-05 | Global gradient norm: 100.48\n",
      "Step 273352) Time = 129.649248\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00031 | mse = 0.00028 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.6020213378360495e-05 | Global gradient norm: 100.49\n",
      "Step 273540) Time = 129.284324\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.590935325017199e-05 | Global gradient norm: 100.49\n",
      "Step 273728) Time = 125.020443\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.5798715038690716e-05 | Global gradient norm: 100.50\n",
      "Step 273916) Time = 120.935470\n",
      "Train loss = 0.00035 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.568830601987429e-05 | Global gradient norm: 100.50\n",
      "Step 274104) Time = 119.050803\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.5578108003828675e-05 | Global gradient norm: 100.51\n",
      "Step 274292) Time = 122.258969\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.546813190449029e-05 | Global gradient norm: 100.52\n",
      "Step 274480) Time = 121.673965\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.535836680792272e-05 | Global gradient norm: 100.52\n",
      "Step 274668) Time = 124.078379\n",
      "Train loss = 0.00039 | mse = 0.00036 | KL = 0.00003\n",
      "Validation loss = 0.00038 | mse = 0.00035 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.524882362806238e-05 | Global gradient norm: 100.53\n",
      "Step 274856) Time = 121.693755\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.513949872693047e-05 | Global gradient norm: 100.54\n",
      "Step 275044) Time = 119.244574\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.503037755261175e-05 | Global gradient norm: 100.54\n",
      "Step 275232) Time = 123.906761\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.4921485570957884e-05 | Global gradient norm: 100.55\n",
      "Step 275420) Time = 127.838946\n",
      "Train loss = 0.00040 | mse = 0.00037 | KL = 0.00003\n",
      "Validation loss = 0.00038 | mse = 0.00036 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.481281550601125e-05 | Global gradient norm: 100.55\n",
      "Step 275608) Time = 124.474112\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.4704345529899e-05 | Global gradient norm: 100.56\n",
      "Step 275796) Time = 125.045034\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.459609747049399e-05 | Global gradient norm: 100.57\n",
      "Step 275984) Time = 120.921608\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.448805677588098e-05 | Global gradient norm: 100.57\n",
      "Step 276172) Time = 120.618292\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.43802379979752e-05 | Global gradient norm: 100.58\n",
      "Step 276360) Time = 121.542381\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.4272630222840235e-05 | Global gradient norm: 100.58\n",
      "Step 276548) Time = 117.247387\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.416523345047608e-05 | Global gradient norm: 100.60\n",
      "Step 276736) Time = 121.482082\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.405805859481916e-05 | Global gradient norm: 100.61\n",
      "Step 276924) Time = 121.863082\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.395108019001782e-05 | Global gradient norm: 100.62\n",
      "Step 277112) Time = 118.074240\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 5.384431278798729e-05 | Global gradient norm: 100.63\n",
      "Step 277300) Time = 120.065081\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.3737781854579225e-05 | Global gradient norm: 100.64\n",
      "Step 277488) Time = 120.880287\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00003\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.363143645809032e-05 | Global gradient norm: 100.65\n",
      "Step 277676) Time = 119.666130\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.3525305702351034e-05 | Global gradient norm: 100.66\n",
      "Step 277864) Time = 121.072085\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.3419389587361366e-05 | Global gradient norm: 100.67\n",
      "Step 278052) Time = 118.055212\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.3313677199184895e-05 | Global gradient norm: 100.68\n",
      "Step 278240) Time = 120.884082\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.3208186727715656e-05 | Global gradient norm: 100.68\n",
      "Step 278428) Time = 124.457199\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.310289634508081e-05 | Global gradient norm: 100.69\n",
      "Step 278616) Time = 119.675081\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.2997809689259157e-05 | Global gradient norm: 100.70\n",
      "Step 278804) Time = 123.251151\n",
      "Train loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.2892948588123545e-05 | Global gradient norm: 100.71\n",
      "Step 278992) Time = 123.256210\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.27882766618859e-05 | Global gradient norm: 100.72\n",
      "Step 279180) Time = 120.067597\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.2683823014376685e-05 | Global gradient norm: 100.72\n",
      "Step 279368) Time = 122.846083\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.257956581772305e-05 | Global gradient norm: 100.73\n",
      "Step 279556) Time = 119.902129\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.2475523261819035e-05 | Global gradient norm: 100.74\n",
      "Step 279744) Time = 120.237151\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.237168807070702e-05 | Global gradient norm: 100.75\n",
      "Step 279932) Time = 119.297081\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.226804569247179e-05 | Global gradient norm: 100.76\n",
      "Step 280120) Time = 116.082241\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.2164625230943784e-05 | Global gradient norm: 100.77\n",
      "Step 280308) Time = 118.302290\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.206139758229256e-05 | Global gradient norm: 100.78\n",
      "Step 280496) Time = 126.066214\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.195837002247572e-05 | Global gradient norm: 100.78\n",
      "Step 280684) Time = 125.497252\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.185556801734492e-05 | Global gradient norm: 100.79\n",
      "Step 280872) Time = 121.233206\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.175294427317567e-05 | Global gradient norm: 100.79\n",
      "Step 281060) Time = 119.849257\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.165053153177723e-05 | Global gradient norm: 100.80\n",
      "Step 281248) Time = 115.716078\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.1548329793149605e-05 | Global gradient norm: 100.81\n",
      "Step 281436) Time = 118.080198\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.1446320867398754e-05 | Global gradient norm: 100.81\n",
      "Step 281624) Time = 117.035079\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.134452658239752e-05 | Global gradient norm: 100.82\n",
      "Step 281812) Time = 121.272211\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.1242921472294256e-05 | Global gradient norm: 100.83\n",
      "Step 282000) Time = 120.102235\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.114151645102538e-05 | Global gradient norm: 100.84\n",
      "Step 282188) Time = 120.279417\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.104032243252732e-05 | Global gradient norm: 100.85\n",
      "Step 282376) Time = 126.875494\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.093932122690603e-05 | Global gradient norm: 100.86\n",
      "Step 282564) Time = 122.878824\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.083852374809794e-05 | Global gradient norm: 100.87\n",
      "Step 282752) Time = 119.449840\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.0737922720145434e-05 | Global gradient norm: 100.88\n",
      "Step 282940) Time = 122.453116\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.06375145050697e-05 | Global gradient norm: 100.89\n",
      "Step 283128) Time = 121.276284\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.053732093074359e-05 | Global gradient norm: 100.90\n",
      "Step 283316) Time = 120.664582\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.043731289333664e-05 | Global gradient norm: 100.91\n",
      "Step 283504) Time = 122.442760\n",
      "Train loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.033750130678527e-05 | Global gradient norm: 100.92\n",
      "Step 283692) Time = 120.322892\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.023790436098352e-05 | Global gradient norm: 100.93\n",
      "Step 283880) Time = 123.458895\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.013848567614332e-05 | Global gradient norm: 100.94\n",
      "Step 284068) Time = 121.290785\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 5.003927435609512e-05 | Global gradient norm: 100.94\n",
      "Step 284256) Time = 119.276930\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.994025221094489e-05 | Global gradient norm: 100.94\n",
      "Step 284444) Time = 49.401095\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.9841426516650245e-05 | Global gradient norm: 100.94\n",
      "Step 284632) Time = 117.660625\n",
      "Train loss = 0.00036 | mse = 0.00034 | KL = 0.00002\n",
      "Validation loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.974280091118999e-05 | Global gradient norm: 100.95\n",
      "Step 284820) Time = 119.456686\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.9644371756585315e-05 | Global gradient norm: 100.95\n",
      "Step 285008) Time = 118.673203\n",
      "Train loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "Validation loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.954614269081503e-05 | Global gradient norm: 100.96\n",
      "Step 285196) Time = 120.926583\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.94480955239851e-05 | Global gradient norm: 100.96\n",
      "Step 285384) Time = 119.186526\n",
      "Train loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "Validation loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.935024117003195e-05 | Global gradient norm: 100.96\n",
      "Step 285572) Time = 121.898877\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.9252590542891994e-05 | Global gradient norm: 100.97\n",
      "Step 285760) Time = 124.568460\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.915512909065001e-05 | Global gradient norm: 100.98\n",
      "Step 285948) Time = 124.010561\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.905785317532718e-05 | Global gradient norm: 100.99\n",
      "Step 286136) Time = 119.202005\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.8960780986817554e-05 | Global gradient norm: 100.99\n",
      "Step 286324) Time = 118.927554\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.886389069724828e-05 | Global gradient norm: 101.00\n",
      "Step 286512) Time = 116.613356\n",
      "Train loss = 0.00036 | mse = 0.00034 | KL = 0.00002\n",
      "Validation loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.876720777247101e-05 | Global gradient norm: 101.00\n",
      "Step 286700) Time = 117.995429\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.867070310865529e-05 | Global gradient norm: 101.00\n",
      "Step 286888) Time = 117.080523\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.857439125771634e-05 | Global gradient norm: 101.01\n",
      "Step 287076) Time = 121.373488\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.847827949561179e-05 | Global gradient norm: 101.01\n",
      "Step 287264) Time = 121.167904\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.8382338718511164e-05 | Global gradient norm: 101.02\n",
      "Step 287452) Time = 120.037653\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.8286605306202546e-05 | Global gradient norm: 101.02\n",
      "Step 287640) Time = 127.646795\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.819105379283428e-05 | Global gradient norm: 101.02\n",
      "Step 287828) Time = 120.423707\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.809568781638518e-05 | Global gradient norm: 101.03\n",
      "Step 288016) Time = 117.149260\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.8000525566749275e-05 | Global gradient norm: 101.03\n",
      "Step 288204) Time = 118.512081\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.79055343021173e-05 | Global gradient norm: 101.03\n",
      "Step 288392) Time = 117.584080\n",
      "Train loss = 0.00037 | mse = 0.00034 | KL = 0.00002\n",
      "Validation loss = 0.00037 | mse = 0.00034 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 4.78107322123833e-05 | Global gradient norm: 101.03\n",
      "Step 288580) Time = 114.719078\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.771613021148369e-05 | Global gradient norm: 101.03\n",
      "Step 288768) Time = 116.168079\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.762170647154562e-05 | Global gradient norm: 101.04\n",
      "Step 288956) Time = 114.722077\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.752747554448433e-05 | Global gradient norm: 101.04\n",
      "Step 289144) Time = 128.816159\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.74334265163634e-05 | Global gradient norm: 101.05\n",
      "Step 289332) Time = 122.555083\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.733955938718282e-05 | Global gradient norm: 101.05\n",
      "Step 289520) Time = 117.386185\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.724588507087901e-05 | Global gradient norm: 101.06\n",
      "Step 289708) Time = 120.218152\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.715239265351556e-05 | Global gradient norm: 101.06\n",
      "Step 289896) Time = 123.951201\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.7059096687007695e-05 | Global gradient norm: 101.07\n",
      "Step 290084) Time = 121.098204\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.696597170550376e-05 | Global gradient norm: 101.07\n",
      "Step 290272) Time = 118.791143\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.687302862294018e-05 | Global gradient norm: 101.07\n",
      "Step 290460) Time = 116.985184\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.678027835325338e-05 | Global gradient norm: 101.08\n",
      "Step 290648) Time = 118.587080\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.6687706344528124e-05 | Global gradient norm: 101.08\n",
      "Step 290836) Time = 121.843214\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.659531987272203e-05 | Global gradient norm: 101.09\n",
      "Step 291024) Time = 117.854180\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.65031189378351e-05 | Global gradient norm: 101.09\n",
      "Step 291212) Time = 118.270082\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.6411092625930905e-05 | Global gradient norm: 101.09\n",
      "Step 291400) Time = 124.857123\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.631925912690349e-05 | Global gradient norm: 101.10\n",
      "Step 291588) Time = 124.596212\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.6227600250858814e-05 | Global gradient norm: 101.11\n",
      "Step 291776) Time = 121.003120\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.613611599779688e-05 | Global gradient norm: 101.11\n",
      "Step 291964) Time = 118.631210\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.604483547154814e-05 | Global gradient norm: 101.12\n",
      "Step 292152) Time = 117.382080\n",
      "Train loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.5953715016366914e-05 | Global gradient norm: 101.12\n",
      "Step 292340) Time = 121.735087\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.5862783736083657e-05 | Global gradient norm: 101.12\n",
      "Step 292528) Time = 118.239189\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.577202707878314e-05 | Global gradient norm: 101.13\n",
      "Step 292716) Time = 120.239191\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.568144504446536e-05 | Global gradient norm: 101.13\n",
      "Step 292904) Time = 120.200081\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.559105946100317e-05 | Global gradient norm: 101.14\n",
      "Step 293092) Time = 117.734080\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00036 | mse = 0.00034 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.55008412245661e-05 | Global gradient norm: 101.14\n",
      "Step 293280) Time = 119.496140\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.5410797611111775e-05 | Global gradient norm: 101.14\n",
      "Step 293468) Time = 121.405946\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.5320946810534224e-05 | Global gradient norm: 101.14\n",
      "Step 293656) Time = 120.936043\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.523125971900299e-05 | Global gradient norm: 101.15\n",
      "Step 293844) Time = 122.690245\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.5141754526412115e-05 | Global gradient norm: 101.15\n",
      "Step 294032) Time = 121.330649\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.505243123276159e-05 | Global gradient norm: 101.16\n",
      "Step 294220) Time = 123.773995\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.4963278924115e-05 | Global gradient norm: 101.16\n",
      "Step 294408) Time = 121.107774\n",
      "Train loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.487430851440877e-05 | Global gradient norm: 101.16\n",
      "Step 294596) Time = 118.745462\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.478550545172766e-05 | Global gradient norm: 101.17\n",
      "Step 294784) Time = 120.794349\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.469688792596571e-05 | Global gradient norm: 101.17\n",
      "Step 294972) Time = 123.754584\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.460843774722889e-05 | Global gradient norm: 101.18\n",
      "Step 295160) Time = 123.920195\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.452016582945362e-05 | Global gradient norm: 101.19\n",
      "Step 295348) Time = 120.909243\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.443207217263989e-05 | Global gradient norm: 101.19\n",
      "Step 295536) Time = 118.401822\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.434414586285129e-05 | Global gradient norm: 101.19\n",
      "Step 295724) Time = 125.836589\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.425639781402424e-05 | Global gradient norm: 101.19\n",
      "Step 295912) Time = 122.520138\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.416882075020112e-05 | Global gradient norm: 101.20\n",
      "Step 296100) Time = 117.904238\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.4081414671381935e-05 | Global gradient norm: 101.20\n",
      "Step 296288) Time = 119.557743\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.399419412948191e-05 | Global gradient norm: 101.21\n",
      "Step 296476) Time = 121.395250\n",
      "Train loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.39071336586494e-05 | Global gradient norm: 101.22\n",
      "Step 296664) Time = 119.292238\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.382024417282082e-05 | Global gradient norm: 101.22\n",
      "Step 296852) Time = 128.376294\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.3733540223911405e-05 | Global gradient norm: 101.23\n",
      "Step 297040) Time = 123.516829\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.364699270809069e-05 | Global gradient norm: 101.23\n",
      "Step 297228) Time = 117.851830\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.3560627091210335e-05 | Global gradient norm: 101.24\n",
      "Step 297416) Time = 119.430047\n",
      "Train loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 4.34744251833763e-05 | Global gradient norm: 101.24\n",
      "Step 297604) Time = 119.499477\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.3388397898525e-05 | Global gradient norm: 101.24\n",
      "Step 297792) Time = 122.437503\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.330254523665644e-05 | Global gradient norm: 101.25\n",
      "Step 297980) Time = 121.418038\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.32168526458554e-05 | Global gradient norm: 101.25\n",
      "Step 298168) Time = 118.675583\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.31313383160159e-05 | Global gradient norm: 101.25\n",
      "Step 298356) Time = 120.732331\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.304598769522272e-05 | Global gradient norm: 101.25\n",
      "Step 298544) Time = 122.177489\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 4.296080442145467e-05 | Global gradient norm: 101.26\n",
      "Step 298732) Time = 119.200016\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.2875795770669356e-05 | Global gradient norm: 101.26\n",
      "Step 298920) Time = 122.010875\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.279095082893036e-05 | Global gradient norm: 101.26\n",
      "Step 299108) Time = 119.669192\n",
      "Train loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.2706273234216496e-05 | Global gradient norm: 101.26\n",
      "Step 299296) Time = 121.640082\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.262176662450656e-05 | Global gradient norm: 101.27\n",
      "Step 299484) Time = 122.316083\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.2537427361821756e-05 | Global gradient norm: 101.27\n",
      "Step 299672) Time = 119.378203\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.245325908414088e-05 | Global gradient norm: 101.27\n",
      "Step 299860) Time = 119.095081\n",
      "Train loss = 0.00036 | mse = 0.00034 | KL = 0.00002\n",
      "Validation loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.2369243601569906e-05 | Global gradient norm: 101.27\n",
      "Step 300048) Time = 128.196267\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.228540274198167e-05 | Global gradient norm: 101.28\n",
      "Step 300236) Time = 126.431310\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 4.220173286739737e-05 | Global gradient norm: 101.28\n",
      "Step 300424) Time = 121.547206\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.211821942590177e-05 | Global gradient norm: 101.28\n",
      "Step 300612) Time = 119.764189\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.2034880607388914e-05 | Global gradient norm: 101.29\n",
      "Step 300800) Time = 127.641090\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.195169822196476e-05 | Global gradient norm: 101.29\n",
      "Step 300988) Time = 123.067083\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.186867590760812e-05 | Global gradient norm: 101.29\n",
      "Step 301176) Time = 119.633306\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.1785835492191836e-05 | Global gradient norm: 101.29\n",
      "Step 301364) Time = 120.983081\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.170314423390664e-05 | Global gradient norm: 101.30\n",
      "Step 301552) Time = 121.081082\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.1620616684667766e-05 | Global gradient norm: 101.30\n",
      "Step 301740) Time = 117.419237\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.153826739639044e-05 | Global gradient norm: 101.30\n",
      "Step 301928) Time = 120.308081\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.1456063627265394e-05 | Global gradient norm: 101.31\n",
      "Step 302116) Time = 119.864083\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.137403448112309e-05 | Global gradient norm: 101.31\n",
      "Step 302304) Time = 117.570144\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.1292158130090684e-05 | Global gradient norm: 101.32\n",
      "Step 302492) Time = 120.455081\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.12104454881046e-05 | Global gradient norm: 101.32\n",
      "Step 302680) Time = 117.757147\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.112890383112244e-05 | Global gradient norm: 101.32\n",
      "Step 302868) Time = 119.618203\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.104751496925019e-05 | Global gradient norm: 101.32\n",
      "Step 303056) Time = 122.066210\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.0966297092381865e-05 | Global gradient norm: 101.33\n",
      "Step 303244) Time = 117.599080\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.088522837264463e-05 | Global gradient norm: 101.33\n",
      "Step 303432) Time = 120.313169\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.0804316085996106e-05 | Global gradient norm: 101.33\n",
      "Step 303620) Time = 120.028147\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.072357842233032e-05 | Global gradient norm: 101.33\n",
      "Step 303808) Time = 117.639080\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.064299355377443e-05 | Global gradient norm: 101.33\n",
      "Step 303996) Time = 118.839080\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.056256511830725e-05 | Global gradient norm: 101.34\n",
      "Step 304184) Time = 116.282142\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00002\n",
      "Validation loss = 0.00036 | mse = 0.00033 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.048230402986519e-05 | Global gradient norm: 101.34\n",
      "Step 304372) Time = 120.423086\n",
      "Train loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "Validation loss = 0.00036 | mse = 0.00034 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.0402192098554224e-05 | Global gradient norm: 101.34\n",
      "Step 304560) Time = 127.437456\n",
      "Train loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "Validation loss = 0.00036 | mse = 0.00034 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.032225115224719e-05 | Global gradient norm: 101.34\n",
      "Step 304748) Time = 128.883857\n",
      "Train loss = 0.00036 | mse = 0.00034 | KL = 0.00002\n",
      "Validation loss = 0.00037 | mse = 0.00035 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.024245572509244e-05 | Global gradient norm: 101.35\n",
      "Step 304936) Time = 123.269997\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.016282400698401e-05 | Global gradient norm: 101.35\n",
      "Step 305124) Time = 129.027599\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.008335235994309e-05 | Global gradient norm: 101.35\n",
      "Step 305312) Time = 130.422013\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 4.000403350801207e-05 | Global gradient norm: 101.35\n",
      "Step 305500) Time = 124.422282\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.992487472714856e-05 | Global gradient norm: 101.36\n",
      "Step 305688) Time = 118.878936\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.9845865103416145e-05 | Global gradient norm: 101.36\n",
      "Step 305876) Time = 127.459785\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.976701555075124e-05 | Global gradient norm: 101.36\n",
      "Step 306064) Time = 122.598967\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.968832970713265e-05 | Global gradient norm: 101.37\n",
      "Step 306252) Time = 119.655130\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.960978938266635e-05 | Global gradient norm: 101.37\n",
      "Step 306440) Time = 121.865566\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.953140549128875e-05 | Global gradient norm: 101.38\n",
      "Step 306628) Time = 123.276847\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.945318530895747e-05 | Global gradient norm: 101.38\n",
      "Step 306816) Time = 121.221758\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.937511064577848e-05 | Global gradient norm: 101.38\n",
      "Step 307004) Time = 122.411327\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.92971996916458e-05 | Global gradient norm: 101.39\n",
      "Step 307192) Time = 120.119976\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.9219437894644216e-05 | Global gradient norm: 101.39\n",
      "Step 307380) Time = 117.439296\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.914182525477372e-05 | Global gradient norm: 101.40\n",
      "Step 307568) Time = 119.831677\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.906437268597074e-05 | Global gradient norm: 101.40\n",
      "Step 307756) Time = 119.089461\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.898706927429885e-05 | Global gradient norm: 101.41\n",
      "Step 307944) Time = 120.271469\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.8909929571673274e-05 | Global gradient norm: 101.41\n",
      "Step 308132) Time = 127.884496\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.883292811224237e-05 | Global gradient norm: 101.42\n",
      "Step 308320) Time = 127.857483\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.875608308590017e-05 | Global gradient norm: 101.42\n",
      "Step 308508) Time = 126.836664\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.867939449264668e-05 | Global gradient norm: 101.43\n",
      "Step 308696) Time = 120.705119\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.860285141854547e-05 | Global gradient norm: 101.43\n",
      "Step 308884) Time = 118.083679\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.852645750157535e-05 | Global gradient norm: 101.44\n",
      "Step 309072) Time = 124.883569\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.845022729365155e-05 | Global gradient norm: 101.44\n",
      "Step 309260) Time = 124.656010\n",
      "Train loss = 0.00036 | mse = 0.00033 | KL = 0.00003\n",
      "Validation loss = 0.00036 | mse = 0.00034 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.837413896690123e-05 | Global gradient norm: 101.45\n",
      "Step 309448) Time = 121.630380\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.829821071121842e-05 | Global gradient norm: 101.45\n",
      "Step 309636) Time = 128.123820\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.822242433670908e-05 | Global gradient norm: 101.45\n",
      "Step 309824) Time = 125.467061\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.8146779843373224e-05 | Global gradient norm: 101.46\n",
      "Step 310012) Time = 122.670274\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.80713063350413e-05 | Global gradient norm: 101.46\n",
      "Step 310200) Time = 120.671126\n",
      "Train loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.799596743192524e-05 | Global gradient norm: 101.47\n",
      "Step 310388) Time = 117.495123\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.792078132391907e-05 | Global gradient norm: 101.47\n",
      "Step 310576) Time = 122.652250\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.784574073506519e-05 | Global gradient norm: 101.47\n",
      "Step 310764) Time = 119.826197\n",
      "Train loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "Validation loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.7770845665363595e-05 | Global gradient norm: 101.48\n",
      "Step 310952) Time = 119.742081\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.769611066672951e-05 | Global gradient norm: 101.48\n",
      "Step 311140) Time = 119.839081\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.7621513911290094e-05 | Global gradient norm: 101.49\n",
      "Step 311328) Time = 117.033196\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.7547073588939384e-05 | Global gradient norm: 101.49\n",
      "Step 311516) Time = 126.486086\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.747277150978334e-05 | Global gradient norm: 101.49\n",
      "Step 311704) Time = 125.466085\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.7398614949779585e-05 | Global gradient norm: 101.50\n",
      "Step 311892) Time = 120.477192\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00027 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.7324611184885725e-05 | Global gradient norm: 101.51\n",
      "Step 312080) Time = 121.868147\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.725075293914415e-05 | Global gradient norm: 101.51\n",
      "Step 312268) Time = 121.485082\n",
      "Train loss = 0.00037 | mse = 0.00035 | KL = 0.00002\n",
      "Validation loss = 0.00036 | mse = 0.00034 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.7177043850533664e-05 | Global gradient norm: 101.51\n",
      "Step 312456) Time = 120.816089\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.710348028107546e-05 | Global gradient norm: 101.52\n",
      "Step 312644) Time = 122.502204\n",
      "Train loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "Validation loss = 0.00036 | mse = 0.00034 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.7030051316833124e-05 | Global gradient norm: 101.52\n",
      "Step 312832) Time = 119.448209\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.6956782423658296e-05 | Global gradient norm: 101.53\n",
      "Step 313020) Time = 119.901195\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.688364813569933e-05 | Global gradient norm: 101.53\n",
      "Step 313208) Time = 129.838088\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.681065936689265e-05 | Global gradient norm: 101.54\n",
      "Step 313396) Time = 130.326088\n",
      "Train loss = 0.00034 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.673782339319587e-05 | Global gradient norm: 101.54\n",
      "Step 313584) Time = 124.206232\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.6665125662693754e-05 | Global gradient norm: 101.55\n",
      "Step 313772) Time = 119.526154\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.659256981336512e-05 | Global gradient norm: 101.55\n",
      "Step 313960) Time = 115.672079\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.652015948318876e-05 | Global gradient norm: 101.56\n",
      "Step 314148) Time = 118.621080\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.644788739620708e-05 | Global gradient norm: 101.56\n",
      "Step 314336) Time = 118.078199\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.637577538029291e-05 | Global gradient norm: 101.57\n",
      "Step 314524) Time = 121.063082\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.6303790693636984e-05 | Global gradient norm: 101.57\n",
      "Step 314712) Time = 120.322082\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.6231947888154536e-05 | Global gradient norm: 101.58\n",
      "Step 314900) Time = 119.899189\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.616025423980318e-05 | Global gradient norm: 101.58\n",
      "Step 315088) Time = 122.075083\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.608869883464649e-05 | Global gradient norm: 101.59\n",
      "Step 315276) Time = 121.658082\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.601728894864209e-05 | Global gradient norm: 101.59\n",
      "Step 315464) Time = 120.463081\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.5946017305832356e-05 | Global gradient norm: 101.60\n",
      "Step 315652) Time = 125.096409\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.5874880268238485e-05 | Global gradient norm: 101.60\n",
      "Step 315840) Time = 120.670163\n",
      "Train loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.5803892387775704e-05 | Global gradient norm: 101.61\n",
      "Step 316028) Time = 120.424205\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.573304275050759e-05 | Global gradient norm: 101.62\n",
      "Step 316216) Time = 120.302605\n",
      "Train loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.566233499441296e-05 | Global gradient norm: 101.62\n",
      "Step 316404) Time = 118.888021\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.5591765481512994e-05 | Global gradient norm: 101.63\n",
      "Step 316592) Time = 121.269885\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.55213342118077e-05 | Global gradient norm: 101.64\n",
      "Step 316780) Time = 121.025892\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.5451048461254686e-05 | Global gradient norm: 101.65\n",
      "Step 316968) Time = 120.474679\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.538089367793873e-05 | Global gradient norm: 101.65\n",
      "Step 317156) Time = 122.274871\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.5310873499838635e-05 | Global gradient norm: 101.66\n",
      "Step 317344) Time = 122.515510\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.524100611684844e-05 | Global gradient norm: 101.67\n",
      "Step 317532) Time = 120.038752\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.5171273339074105e-05 | Global gradient norm: 101.67\n",
      "Step 317720) Time = 119.506037\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.510167880449444e-05 | Global gradient norm: 101.68\n",
      "Step 317908) Time = 117.436691\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.503221523715183e-05 | Global gradient norm: 101.69\n",
      "Step 318096) Time = 120.935390\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.496288991300389e-05 | Global gradient norm: 101.69\n",
      "Step 318284) Time = 125.817113\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.489370647002943e-05 | Global gradient norm: 101.70\n",
      "Step 318472) Time = 127.530453\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.482465763227083e-05 | Global gradient norm: 101.70\n",
      "Step 318660) Time = 125.451895\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.4755750675685704e-05 | Global gradient norm: 101.71\n",
      "Step 318848) Time = 121.085275\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.468697832431644e-05 | Global gradient norm: 101.72\n",
      "Step 319036) Time = 120.057792\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.461832966422662e-05 | Global gradient norm: 101.72\n",
      "Step 319224) Time = 121.868276\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.454983016126789e-05 | Global gradient norm: 101.73\n",
      "Step 319412) Time = 118.870747\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.448146162554622e-05 | Global gradient norm: 101.73\n",
      "Step 319600) Time = 122.889314\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.4413227695040405e-05 | Global gradient norm: 101.74\n",
      "Step 319788) Time = 122.685665\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.434513200772926e-05 | Global gradient norm: 101.75\n",
      "Step 319976) Time = 50.284954\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.427717092563398e-05 | Global gradient norm: 101.75\n",
      "Step 320164) Time = 118.898214\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.420933717279695e-05 | Global gradient norm: 101.76\n",
      "Step 320352) Time = 123.614028\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.4141645301133394e-05 | Global gradient norm: 101.76\n",
      "Step 320540) Time = 127.465588\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.40740880346857e-05 | Global gradient norm: 101.77\n",
      "Step 320728) Time = 126.711904\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.400665445951745e-05 | Global gradient norm: 101.78\n",
      "Step 320916) Time = 125.150449\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.393936640350148e-05 | Global gradient norm: 101.78\n",
      "Step 321104) Time = 123.370571\n",
      "Train loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.387221659068018e-05 | Global gradient norm: 101.79\n",
      "Step 321292) Time = 122.575130\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.3805175917223096e-05 | Global gradient norm: 101.79\n",
      "Step 321480) Time = 118.411199\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.37382844008971e-05 | Global gradient norm: 101.80\n",
      "Step 321668) Time = 115.725206\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.367152748978697e-05 | Global gradient norm: 101.80\n",
      "Step 321856) Time = 121.215082\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.360489790793508e-05 | Global gradient norm: 101.81\n",
      "Step 322044) Time = 118.111080\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.3538395655341446e-05 | Global gradient norm: 101.82\n",
      "Step 322232) Time = 115.255188\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.347202800796367e-05 | Global gradient norm: 101.82\n",
      "Step 322420) Time = 119.106081\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.340579860378057e-05 | Global gradient norm: 101.82\n",
      "Step 322608) Time = 118.763211\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.33396892528981e-05 | Global gradient norm: 101.83\n",
      "Step 322796) Time = 115.140126\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.32737217831891e-05 | Global gradient norm: 101.84\n",
      "Step 322984) Time = 117.965080\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.320788164273836e-05 | Global gradient norm: 101.84\n",
      "Step 323172) Time = 115.321077\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.314215791760944e-05 | Global gradient norm: 101.85\n",
      "Step 323360) Time = 118.359193\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.307658334961161e-05 | Global gradient norm: 101.85\n",
      "Step 323548) Time = 122.966149\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00035 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.3011128834914416e-05 | Global gradient norm: 101.85\n",
      "Step 323736) Time = 121.493825\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.294580164947547e-05 | Global gradient norm: 101.86\n",
      "Step 323924) Time = 120.086307\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.2880612707231194e-05 | Global gradient norm: 101.86\n",
      "Step 324112) Time = 120.466265\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.2815551094245166e-05 | Global gradient norm: 101.86\n",
      "Step 324300) Time = 117.614119\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.275062044849619e-05 | Global gradient norm: 101.87\n",
      "Step 324488) Time = 125.402125\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.268579894211143e-05 | Global gradient norm: 101.87\n",
      "Step 324676) Time = 124.817123\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.2621122954878956e-05 | Global gradient norm: 101.87\n",
      "Step 324864) Time = 120.522197\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.255658157286234e-05 | Global gradient norm: 101.88\n",
      "Step 325052) Time = 118.006225\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.2492142054252326e-05 | Global gradient norm: 101.88\n",
      "Step 325240) Time = 115.588120\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.24278516927734e-05 | Global gradient norm: 101.89\n",
      "Step 325428) Time = 121.065082\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.236368502257392e-05 | Global gradient norm: 101.89\n",
      "Step 325616) Time = 122.190211\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.229963476769626e-05 | Global gradient norm: 101.89\n",
      "Step 325804) Time = 119.642121\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.2235722756013274e-05 | Global gradient norm: 101.90\n",
      "Step 325992) Time = 121.311206\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.217194534954615e-05 | Global gradient norm: 101.90\n",
      "Step 326180) Time = 118.887310\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.210826980648562e-05 | Global gradient norm: 101.91\n",
      "Step 326368) Time = 119.082288\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.2044736144598573e-05 | Global gradient norm: 101.91\n",
      "Step 326556) Time = 119.009142\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.198133344994858e-05 | Global gradient norm: 101.92\n",
      "Step 326744) Time = 115.906186\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.191805080859922e-05 | Global gradient norm: 101.92\n",
      "Step 326932) Time = 119.752112\n",
      "Train loss = 0.00035 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.1854884582571685e-05 | Global gradient norm: 101.92\n",
      "Step 327120) Time = 119.812927\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.1791852961760014e-05 | Global gradient norm: 101.92\n",
      "Step 327308) Time = 116.926928\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.1728945032227784e-05 | Global gradient norm: 101.93\n",
      "Step 327496) Time = 122.482654\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.1666149880038574e-05 | Global gradient norm: 101.93\n",
      "Step 327684) Time = 121.017138\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.160348933306523e-05 | Global gradient norm: 101.94\n",
      "Step 327872) Time = 117.722430\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.154095611535013e-05 | Global gradient norm: 101.94\n",
      "Step 328060) Time = 122.044954\n",
      "Train loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.147853567497805e-05 | Global gradient norm: 101.94\n",
      "Step 328248) Time = 120.125911\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.141625347780064e-05 | Global gradient norm: 101.95\n",
      "Step 328436) Time = 121.370409\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.135409133392386e-05 | Global gradient norm: 101.95\n",
      "Step 328624) Time = 126.471545\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.1292031053453684e-05 | Global gradient norm: 101.96\n",
      "Step 328812) Time = 123.983227\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.123011629213579e-05 | Global gradient norm: 101.96\n",
      "Step 329000) Time = 122.507814\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.116831794613972e-05 | Global gradient norm: 101.97\n",
      "Step 329188) Time = 121.198181\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00003\n",
      "================================================\n",
      "Learning rate: 3.1106646929401904e-05 | Global gradient norm: 101.98\n",
      "Step 329376) Time = 119.848950\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.10450850520283e-05 | Global gradient norm: 101.98\n",
      "Step 329564) Time = 121.625420\n",
      "Train loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.098365414189175e-05 | Global gradient norm: 101.99\n",
      "Step 329752) Time = 118.325836\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.0922350561013445e-05 | Global gradient norm: 101.99\n",
      "Step 329940) Time = 124.343207\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.086114884354174e-05 | Global gradient norm: 102.00\n",
      "Step 330128) Time = 123.037172\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.080008536926471e-05 | Global gradient norm: 102.00\n",
      "Step 330316) Time = 119.754141\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.0739141948288307e-05 | Global gradient norm: 102.01\n",
      "Step 330504) Time = 123.640991\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.067830402869731e-05 | Global gradient norm: 102.01\n",
      "Step 330692) Time = 123.338784\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.061760435230099e-05 | Global gradient norm: 102.02\n",
      "Step 330880) Time = 118.270849\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.055701745324768e-05 | Global gradient norm: 102.02\n",
      "Step 331068) Time = 122.234404\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.04965451505268e-05 | Global gradient norm: 102.02\n",
      "Step 331256) Time = 120.241217\n",
      "Train loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.0436200177064165e-05 | Global gradient norm: 102.03\n",
      "Step 331444) Time = 118.232581\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.037597707589157e-05 | Global gradient norm: 102.03\n",
      "Step 331632) Time = 119.983725\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.0315874028019607e-05 | Global gradient norm: 102.03\n",
      "Step 331820) Time = 118.417917\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.0255874662543647e-05 | Global gradient norm: 102.04\n",
      "Step 332008) Time = 121.436189\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.0196006264304742e-05 | Global gradient norm: 102.04\n",
      "Step 332196) Time = 126.833963\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.0136259738355875e-05 | Global gradient norm: 102.04\n",
      "Step 332384) Time = 124.666067\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.0076618713792413e-05 | Global gradient norm: 102.05\n",
      "Step 332572) Time = 121.822052\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 3.00171050184872e-05 | Global gradient norm: 102.05\n",
      "Step 332760) Time = 118.609080\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.9957709557493217e-05 | Global gradient norm: 102.06\n",
      "Step 332948) Time = 116.281139\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.989841959788464e-05 | Global gradient norm: 102.06\n",
      "Step 333136) Time = 120.562082\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.9839258786523715e-05 | Global gradient norm: 102.06\n",
      "Step 333324) Time = 117.710314\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.9780219847452827e-05 | Global gradient norm: 102.07\n",
      "Step 333512) Time = 121.468082\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.972129368572496e-05 | Global gradient norm: 102.07\n",
      "Step 333700) Time = 125.555194\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.966246938740369e-05 | Global gradient norm: 102.07\n",
      "Step 333888) Time = 123.723207\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.960377787530888e-05 | Global gradient norm: 102.08\n",
      "Step 334076) Time = 126.842185\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.9545199140557088e-05 | Global gradient norm: 102.08\n",
      "Step 334264) Time = 125.258199\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.948672954516951e-05 | Global gradient norm: 102.09\n",
      "Step 334452) Time = 121.441230\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.9428385460050777e-05 | Global gradient norm: 102.09\n",
      "Step 334640) Time = 122.442083\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.9370154152275063e-05 | Global gradient norm: 102.09\n",
      "Step 334828) Time = 123.384513\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.9312024707905948e-05 | Global gradient norm: 102.10\n",
      "Step 335016) Time = 119.490288\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.9254024411784485e-05 | Global gradient norm: 102.10\n",
      "Step 335204) Time = 125.671260\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.919614053098485e-05 | Global gradient norm: 102.11\n",
      "Step 335392) Time = 125.226208\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.9138362151570618e-05 | Global gradient norm: 102.11\n",
      "Step 335580) Time = 123.452122\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.908070382545702e-05 | Global gradient norm: 102.12\n",
      "Step 335768) Time = 121.217303\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.9023161914665252e-05 | Global gradient norm: 102.12\n",
      "Step 335956) Time = 118.809194\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.8965732781216502e-05 | Global gradient norm: 102.13\n",
      "Step 336144) Time = 121.530082\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.8908409149153158e-05 | Global gradient norm: 102.13\n",
      "Step 336332) Time = 119.991081\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.885120738937985e-05 | Global gradient norm: 102.14\n",
      "Step 336520) Time = 117.283120\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.879412022593897e-05 | Global gradient norm: 102.14\n",
      "Step 336708) Time = 116.844079\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.8737134925904684e-05 | Global gradient norm: 102.15\n",
      "Step 336896) Time = 113.401076\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.8680269679171033e-05 | Global gradient norm: 102.15\n",
      "Step 337084) Time = 128.343699\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.8623522666748613e-05 | Global gradient norm: 102.16\n",
      "Step 337272) Time = 122.955321\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.856687206076458e-05 | Global gradient norm: 102.16\n",
      "Step 337460) Time = 117.106614\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.8510346965049393e-05 | Global gradient norm: 102.17\n",
      "Step 337648) Time = 120.529211\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.845393646566663e-05 | Global gradient norm: 102.17\n",
      "Step 337836) Time = 123.351084\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.8397622372722253e-05 | Global gradient norm: 102.18\n",
      "Step 338024) Time = 121.335210\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.8341430152067915e-05 | Global gradient norm: 102.18\n",
      "Step 338212) Time = 119.372288\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.8285352527746e-05 | Global gradient norm: 102.19\n",
      "Step 338400) Time = 123.440434\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.8229384042788297e-05 | Global gradient norm: 102.19\n",
      "Step 338588) Time = 120.686721\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.8173517421237193e-05 | Global gradient norm: 102.20\n",
      "Step 338776) Time = 122.837878\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.811776903399732e-05 | Global gradient norm: 102.20\n",
      "Step 338964) Time = 120.705241\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.806213524308987e-05 | Global gradient norm: 102.21\n",
      "Step 339152) Time = 120.596086\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.8006594220642e-05 | Global gradient norm: 102.21\n",
      "Step 339340) Time = 122.020711\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.7951178708462976e-05 | Global gradient norm: 102.22\n",
      "Step 339528) Time = 117.844218\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.789587051665876e-05 | Global gradient norm: 102.22\n",
      "Step 339716) Time = 127.524041\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.7840664188261144e-05 | Global gradient norm: 102.22\n",
      "Step 339904) Time = 130.835545\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.7785576094174758e-05 | Global gradient norm: 102.23\n",
      "Step 340092) Time = 128.862516\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.773059532046318e-05 | Global gradient norm: 102.23\n",
      "Step 340280) Time = 123.004626\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.76757145911688e-05 | Global gradient norm: 102.23\n",
      "Step 340468) Time = 118.242986\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.7620950277196243e-05 | Global gradient norm: 102.24\n",
      "Step 340656) Time = 121.994118\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.7566298740566708e-05 | Global gradient norm: 102.24\n",
      "Step 340844) Time = 121.212794\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.7511754524311982e-05 | Global gradient norm: 102.24\n",
      "Step 341032) Time = 118.881301\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.745730489550624e-05 | Global gradient norm: 102.24\n",
      "Step 341220) Time = 125.919561\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.7402975320001133e-05 | Global gradient norm: 102.25\n",
      "Step 341408) Time = 123.649106\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.7348753064870834e-05 | Global gradient norm: 102.25\n",
      "Step 341596) Time = 120.206953\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.7294627216178924e-05 | Global gradient norm: 102.25\n",
      "Step 341784) Time = 123.033388\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.7240621420787647e-05 | Global gradient norm: 102.26\n",
      "Step 341972) Time = 123.319538\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.7186721126781777e-05 | Global gradient norm: 102.26\n",
      "Step 342160) Time = 123.191718\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.7132913601235487e-05 | Global gradient norm: 102.26\n",
      "Step 342348) Time = 125.217098\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.707922612898983e-05 | Global gradient norm: 102.27\n",
      "Step 342536) Time = 119.265718\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.702564415812958e-05 | Global gradient norm: 102.27\n",
      "Step 342724) Time = 122.866055\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.6972156774718314e-05 | Global gradient norm: 102.28\n",
      "Step 342912) Time = 120.860923\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.6918789444607683e-05 | Global gradient norm: 102.28\n",
      "Step 343100) Time = 118.046558\n",
      "Train loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.686552397790365e-05 | Global gradient norm: 102.29\n",
      "Step 343288) Time = 121.821797\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.6812365831574425e-05 | Global gradient norm: 102.29\n",
      "Step 343476) Time = 120.292090\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.6759302272694185e-05 | Global gradient norm: 102.29\n",
      "Step 343664) Time = 117.867970\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.6706351491156965e-05 | Global gradient norm: 102.30\n",
      "Step 343852) Time = 126.431086\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.665350984898396e-05 | Global gradient norm: 102.30\n",
      "Step 344040) Time = 123.880202\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.660075915628113e-05 | Global gradient norm: 102.31\n",
      "Step 344228) Time = 125.121211\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.654812669788953e-05 | Global gradient norm: 102.31\n",
      "Step 344416) Time = 120.216081\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.649559610290453e-05 | Global gradient norm: 102.32\n",
      "Step 344604) Time = 116.105079\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.6443156457389705e-05 | Global gradient norm: 102.32\n",
      "Step 344792) Time = 120.054242\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.639083322719671e-05 | Global gradient norm: 102.32\n",
      "Step 344980) Time = 120.658205\n",
      "Train loss = 0.00029 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.633861186041031e-05 | Global gradient norm: 102.33\n",
      "Step 345168) Time = 117.884186\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.6286490538041107e-05 | Global gradient norm: 102.33\n",
      "Step 345356) Time = 119.670153\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.623447471705731e-05 | Global gradient norm: 102.34\n",
      "Step 345544) Time = 117.265080\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.618256439745892e-05 | Global gradient norm: 102.34\n",
      "Step 345732) Time = 125.331084\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.613075776025653e-05 | Global gradient norm: 102.35\n",
      "Step 345920) Time = 122.452082\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.6079042072524317e-05 | Global gradient norm: 102.35\n",
      "Step 346108) Time = 118.438150\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.6027439162135124e-05 | Global gradient norm: 102.36\n",
      "Step 346296) Time = 120.669082\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.5975939934141934e-05 | Global gradient norm: 102.36\n",
      "Step 346484) Time = 121.680082\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.592453165561892e-05 | Global gradient norm: 102.36\n",
      "Step 346672) Time = 117.432197\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.5873234335449524e-05 | Global gradient norm: 102.37\n",
      "Step 346860) Time = 120.071201\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.5822037059697323e-05 | Global gradient norm: 102.37\n",
      "Step 347048) Time = 119.125242\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.577093619038351e-05 | Global gradient norm: 102.38\n",
      "Step 347236) Time = 117.291209\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.5719942641444504e-05 | Global gradient norm: 102.38\n",
      "Step 347424) Time = 120.472081\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.5669050955912098e-05 | Global gradient norm: 102.38\n",
      "Step 347612) Time = 118.052200\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.5618246581871063e-05 | Global gradient norm: 102.39\n",
      "Step 347800) Time = 128.674087\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.556755680416245e-05 | Global gradient norm: 102.39\n",
      "Step 347988) Time = 121.497082\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.5516967070871033e-05 | Global gradient norm: 102.39\n",
      "Step 348176) Time = 117.014051\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.5466475563007407e-05 | Global gradient norm: 102.40\n",
      "Step 348364) Time = 121.075120\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.5416078642592765e-05 | Global gradient norm: 102.40\n",
      "Step 348552) Time = 121.278082\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.5365785404574126e-05 | Global gradient norm: 102.40\n",
      "Step 348740) Time = 118.892081\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.531559584895149e-05 | Global gradient norm: 102.41\n",
      "Step 348928) Time = 121.861385\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.5265491785830818e-05 | Global gradient norm: 102.41\n",
      "Step 349116) Time = 119.259245\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.5215498681063764e-05 | Global gradient norm: 102.41\n",
      "Step 349304) Time = 124.287658\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.51656038017245e-05 | Global gradient norm: 102.42\n",
      "Step 349492) Time = 125.048325\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.511580169084482e-05 | Global gradient norm: 102.42\n",
      "Step 349680) Time = 119.912034\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.5066105081350543e-05 | Global gradient norm: 102.42\n",
      "Step 349868) Time = 121.480816\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.5016506697284058e-05 | Global gradient norm: 102.43\n",
      "Step 350056) Time = 122.463151\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.4966995624708943e-05 | Global gradient norm: 102.43\n",
      "Step 350244) Time = 119.475870\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.4917591872508638e-05 | Global gradient norm: 102.43\n",
      "Step 350432) Time = 125.467086\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.486828998371493e-05 | Global gradient norm: 102.44\n",
      "Step 350620) Time = 125.483627\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.4819084501359612e-05 | Global gradient norm: 102.44\n",
      "Step 350808) Time = 121.071677\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.4769962692516856e-05 | Global gradient norm: 102.44\n",
      "Step 350996) Time = 121.697286\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.4720951842027716e-05 | Global gradient norm: 102.44\n",
      "Step 351184) Time = 118.252222\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.467203557898756e-05 | Global gradient norm: 102.45\n",
      "Step 351372) Time = 129.509961\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.462320844642818e-05 | Global gradient norm: 102.45\n",
      "Step 351560) Time = 126.407552\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.4574486815254204e-05 | Global gradient norm: 102.45\n",
      "Step 351748) Time = 121.307381\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.4525861590518616e-05 | Global gradient norm: 102.46\n",
      "Step 351936) Time = 123.286053\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.4477321858284995e-05 | Global gradient norm: 102.46\n",
      "Step 352124) Time = 123.262155\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.4428885808447376e-05 | Global gradient norm: 102.47\n",
      "Step 352312) Time = 121.879909\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.4380551622016355e-05 | Global gradient norm: 102.47\n",
      "Step 352500) Time = 124.639834\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.4332301109097898e-05 | Global gradient norm: 102.47\n",
      "Step 352688) Time = 119.485081\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.4284156097564846e-05 | Global gradient norm: 102.48\n",
      "Step 352876) Time = 120.283152\n",
      "Train loss = 0.00033 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.4236102035501972e-05 | Global gradient norm: 102.48\n",
      "Step 353064) Time = 119.638081\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.4188148017856292e-05 | Global gradient norm: 102.48\n",
      "Step 353252) Time = 119.438422\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.4140275854733773e-05 | Global gradient norm: 102.49\n",
      "Step 353440) Time = 123.116195\n",
      "Train loss = 0.00029 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.409250919299666e-05 | Global gradient norm: 102.49\n",
      "Step 353628) Time = 120.627238\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.4044838937697932e-05 | Global gradient norm: 102.49\n",
      "Step 353816) Time = 118.108175\n",
      "Train loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.399725235591177e-05 | Global gradient norm: 102.50\n",
      "Step 354004) Time = 121.254092\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.3949767637532204e-05 | Global gradient norm: 102.50\n",
      "Step 354192) Time = 121.080286\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.3902379325591028e-05 | Global gradient norm: 102.50\n",
      "Step 354380) Time = 119.095312\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.3855074687162414e-05 | Global gradient norm: 102.51\n",
      "Step 354568) Time = 122.244195\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.3807873731129803e-05 | Global gradient norm: 102.51\n",
      "Step 354756) Time = 118.832141\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.376076372456737e-05 | Global gradient norm: 102.52\n",
      "Step 354944) Time = 120.129158\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.37137392105069e-05 | Global gradient norm: 102.52\n",
      "Step 355132) Time = 126.852386\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.366681474086363e-05 | Global gradient norm: 102.52\n",
      "Step 355320) Time = 125.873085\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.3619986677658744e-05 | Global gradient norm: 102.53\n",
      "Step 355508) Time = 51.482035\n",
      "Train loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.3573253201902844e-05 | Global gradient norm: 102.53\n",
      "Step 355696) Time = 118.923081\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.3526596123701893e-05 | Global gradient norm: 102.54\n",
      "Step 355884) Time = 121.165206\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.348004636587575e-05 | Global gradient norm: 102.54\n",
      "Step 356072) Time = 119.542185\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.343358391954098e-05 | Global gradient norm: 102.54\n",
      "Step 356260) Time = 120.746187\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.338720514671877e-05 | Global gradient norm: 102.55\n",
      "Step 356448) Time = 119.340207\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.3340931875281967e-05 | Global gradient norm: 102.55\n",
      "Step 356636) Time = 118.664124\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.3294747734325938e-05 | Global gradient norm: 102.56\n",
      "Step 356824) Time = 121.688633\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.3248645447893068e-05 | Global gradient norm: 102.56\n",
      "Step 357012) Time = 119.438132\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.320264138688799e-05 | Global gradient norm: 102.56\n",
      "Step 357200) Time = 119.020082\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.3156731913331896e-05 | Global gradient norm: 102.57\n",
      "Step 357388) Time = 121.648082\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.3110900656320155e-05 | Global gradient norm: 102.57\n",
      "Step 357576) Time = 118.124248\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.3065173081704415e-05 | Global gradient norm: 102.58\n",
      "Step 357764) Time = 121.203082\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.3019536456558853e-05 | Global gradient norm: 102.58\n",
      "Step 357952) Time = 129.980565\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.2973985323915258e-05 | Global gradient norm: 102.58\n",
      "Step 358140) Time = 128.113125\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.2928517864784226e-05 | Global gradient norm: 102.59\n",
      "Step 358328) Time = 123.213710\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.2883148631080985e-05 | Global gradient norm: 102.59\n",
      "Step 358516) Time = 119.613052\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.2837868527858518e-05 | Global gradient norm: 102.60\n",
      "Step 358704) Time = 122.412495\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.2792672098148614e-05 | Global gradient norm: 102.60\n",
      "Step 358892) Time = 120.348776\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.2747572074877098e-05 | Global gradient norm: 102.60\n",
      "Step 359080) Time = 120.197926\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.270256300107576e-05 | Global gradient norm: 102.60\n",
      "Step 359268) Time = 122.924468\n",
      "Train loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.265763032482937e-05 | Global gradient norm: 102.61\n",
      "Step 359456) Time = 121.431257\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.2612797693000175e-05 | Global gradient norm: 102.61\n",
      "Step 359644) Time = 117.876408\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.2568054191651754e-05 | Global gradient norm: 102.61\n",
      "Step 359832) Time = 120.092060\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.252339072583709e-05 | Global gradient norm: 102.61\n",
      "Step 360020) Time = 121.018674\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.2478823666460812e-05 | Global gradient norm: 102.62\n",
      "Step 360208) Time = 121.957952\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.243434573756531e-05 | Global gradient norm: 102.62\n",
      "Step 360396) Time = 122.787911\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.2389955120161176e-05 | Global gradient norm: 102.62\n",
      "Step 360584) Time = 118.758343\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.234564090031199e-05 | Global gradient norm: 102.62\n",
      "Step 360772) Time = 130.159034\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.2301428543869406e-05 | Global gradient norm: 102.63\n",
      "Step 360960) Time = 134.058414\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.2257299860939384e-05 | Global gradient norm: 102.63\n",
      "Step 361148) Time = 128.112193\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.2213249394553714e-05 | Global gradient norm: 102.63\n",
      "Step 361336) Time = 124.129154\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.2169297153595835e-05 | Global gradient norm: 102.63\n",
      "Step 361524) Time = 117.728334\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.2125430405139923e-05 | Global gradient norm: 102.64\n",
      "Step 361712) Time = 117.177079\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.208164005423896e-05 | Global gradient norm: 102.64\n",
      "Step 361900) Time = 121.584338\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.2037949747755192e-05 | Global gradient norm: 102.64\n",
      "Step 362088) Time = 120.994484\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.199434493377339e-05 | Global gradient norm: 102.64\n",
      "Step 362276) Time = 124.964272\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1950823793304153e-05 | Global gradient norm: 102.64\n",
      "Step 362464) Time = 122.523286\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1907379050389864e-05 | Global gradient norm: 102.65\n",
      "Step 362652) Time = 119.246205\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1864030713913962e-05 | Global gradient norm: 102.65\n",
      "Step 362840) Time = 118.414121\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1820767869940028e-05 | Global gradient norm: 102.65\n",
      "Step 363028) Time = 123.696084\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.177758506149985e-05 | Global gradient norm: 102.65\n",
      "Step 363216) Time = 124.579262\n",
      "Train loss = 0.00035 | mse = 0.00033 | KL = 0.00002\n",
      "Validation loss = 0.00036 | mse = 0.00034 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1734491383540444e-05 | Global gradient norm: 102.66\n",
      "Step 363404) Time = 120.260232\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1691486836061813e-05 | Global gradient norm: 102.66\n",
      "Step 363592) Time = 116.881202\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1648556867148727e-05 | Global gradient norm: 102.66\n",
      "Step 363780) Time = 124.439084\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1605721485684626e-05 | Global gradient norm: 102.66\n",
      "Step 363968) Time = 119.424155\n",
      "Train loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1562969777733088e-05 | Global gradient norm: 102.66\n",
      "Step 364156) Time = 116.373180\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1520294467336498e-05 | Global gradient norm: 102.67\n",
      "Step 364344) Time = 121.012082\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1477713744388893e-05 | Global gradient norm: 102.67\n",
      "Step 364532) Time = 121.457182\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1435214875964448e-05 | Global gradient norm: 102.67\n",
      "Step 364720) Time = 119.986601\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.139280150004197e-05 | Global gradient norm: 102.67\n",
      "Step 364908) Time = 124.323636\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1350462702685036e-05 | Global gradient norm: 102.67\n",
      "Step 365096) Time = 120.625925\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1308218492777087e-05 | Global gradient norm: 102.67\n",
      "Step 365284) Time = 121.743082\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1266056137392297e-05 | Global gradient norm: 102.67\n",
      "Step 365472) Time = 124.307718\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.122396654158365e-05 | Global gradient norm: 102.67\n",
      "Step 365660) Time = 120.834178\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1181971533223987e-05 | Global gradient norm: 102.67\n",
      "Step 365848) Time = 122.485240\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1140060198376887e-05 | Global gradient norm: 102.68\n",
      "Step 366036) Time = 123.725507\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.1098219804116525e-05 | Global gradient norm: 102.68\n",
      "Step 366224) Time = 119.589019\n",
      "Train loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.105647581629455e-05 | Global gradient norm: 102.68\n",
      "Step 366412) Time = 122.284984\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.101481004501693e-05 | Global gradient norm: 102.68\n",
      "Step 366600) Time = 122.875499\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.0973218852304853e-05 | Global gradient norm: 102.68\n",
      "Step 366788) Time = 118.889135\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.0931718609062955e-05 | Global gradient norm: 102.68\n",
      "Step 366976) Time = 122.842083\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.089030203933362e-05 | Global gradient norm: 102.68\n",
      "Step 367164) Time = 118.426204\n",
      "Train loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.0848963686148636e-05 | Global gradient norm: 102.68\n",
      "Step 367352) Time = 121.611122\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.080770536849741e-05 | Global gradient norm: 102.69\n",
      "Step 367540) Time = 120.183081\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.076653254334815e-05 | Global gradient norm: 102.69\n",
      "Step 367728) Time = 118.702193\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.0725441572722048e-05 | Global gradient norm: 102.69\n",
      "Step 367916) Time = 120.486081\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.068442336167209e-05 | Global gradient norm: 102.69\n",
      "Step 368104) Time = 119.006248\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.0643496100092307e-05 | Global gradient norm: 102.69\n",
      "Step 368292) Time = 116.156079\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.060264887404628e-05 | Global gradient norm: 102.70\n",
      "Step 368480) Time = 118.888081\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.0561874407576397e-05 | Global gradient norm: 102.70\n",
      "Step 368668) Time = 118.288141\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.0521189071587287e-05 | Global gradient norm: 102.70\n",
      "Step 368856) Time = 121.780082\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.0480583771131933e-05 | Global gradient norm: 102.70\n",
      "Step 369044) Time = 119.016192\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.0440049411263317e-05 | Global gradient norm: 102.70\n",
      "Step 369232) Time = 117.756080\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.0399606000864878e-05 | Global gradient norm: 102.70\n",
      "Step 369420) Time = 126.637131\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.0359242626000196e-05 | Global gradient norm: 102.71\n",
      "Step 369608) Time = 121.987156\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.0318955648690462e-05 | Global gradient norm: 102.71\n",
      "Step 369796) Time = 117.628080\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.027874143095687e-05 | Global gradient norm: 102.71\n",
      "Step 369984) Time = 128.779191\n",
      "Train loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.0238616343704052e-05 | Global gradient norm: 102.71\n",
      "Step 370172) Time = 128.021151\n",
      "Train loss = 0.00032 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.019857129198499e-05 | Global gradient norm: 102.71\n",
      "Step 370360) Time = 124.240084\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.0158597180852666e-05 | Global gradient norm: 102.71\n",
      "Step 370548) Time = 123.335123\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.0118710381211713e-05 | Global gradient norm: 102.71\n",
      "Step 370736) Time = 123.438335\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.007889997912571e-05 | Global gradient norm: 102.71\n",
      "Step 370924) Time = 128.087087\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 2.0039162336615846e-05 | Global gradient norm: 102.71\n",
      "Step 371112) Time = 124.659153\n",
      "Train loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.999951018660795e-05 | Global gradient norm: 102.71\n",
      "Step 371300) Time = 119.375096\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.9959936253144406e-05 | Global gradient norm: 102.71\n",
      "Step 371488) Time = 122.464083\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.9920435079257004e-05 | Global gradient norm: 102.72\n",
      "Step 371676) Time = 120.169081\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.9881017578882165e-05 | Global gradient norm: 102.72\n",
      "Step 371864) Time = 115.720078\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.9841680114041083e-05 | Global gradient norm: 102.72\n",
      "Step 372052) Time = 118.035188\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.9802420865744352e-05 | Global gradient norm: 102.73\n",
      "Step 372240) Time = 118.827563\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.9763228920055553e-05 | Global gradient norm: 102.73\n",
      "Step 372428) Time = 123.445369\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.9724124285858124e-05 | Global gradient norm: 102.73\n",
      "Step 372616) Time = 123.328241\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.9685096049215645e-05 | Global gradient norm: 102.74\n",
      "Step 372804) Time = 122.059054\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.96461369341705e-05 | Global gradient norm: 102.74\n",
      "Step 372992) Time = 124.166556\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.960726331162732e-05 | Global gradient norm: 102.74\n",
      "Step 373180) Time = 125.065089\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.956846608663909e-05 | Global gradient norm: 102.75\n",
      "Step 373368) Time = 120.700657\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.9529736164258793e-05 | Global gradient norm: 102.75\n",
      "Step 373556) Time = 123.742586\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.9491093553369865e-05 | Global gradient norm: 102.75\n",
      "Step 373744) Time = 122.324630\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.9452527340035886e-05 | Global gradient norm: 102.75\n",
      "Step 373932) Time = 122.979671\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.9414028429309838e-05 | Global gradient norm: 102.76\n",
      "Step 374120) Time = 122.013882\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.9375615011085756e-05 | Global gradient norm: 102.76\n",
      "Step 374308) Time = 119.643361\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.933727617142722e-05 | Global gradient norm: 102.76\n",
      "Step 374496) Time = 126.485628\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.929901191033423e-05 | Global gradient norm: 102.77\n",
      "Step 374684) Time = 122.660604\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.926082040881738e-05 | Global gradient norm: 102.77\n",
      "Step 374872) Time = 119.606208\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.9222708942834288e-05 | Global gradient norm: 102.77\n",
      "Step 375060) Time = 121.268443\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.918467205541674e-05 | Global gradient norm: 102.77\n",
      "Step 375248) Time = 119.655541\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.9146704289596528e-05 | Global gradient norm: 102.78\n",
      "Step 375436) Time = 123.428381\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.9108818378299475e-05 | Global gradient norm: 102.78\n",
      "Step 375624) Time = 123.827400\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.907100886455737e-05 | Global gradient norm: 102.78\n",
      "Step 375812) Time = 122.453766\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.9033264834433794e-05 | Global gradient norm: 102.78\n",
      "Step 376000) Time = 123.874259\n",
      "Train loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.899560447782278e-05 | Global gradient norm: 102.78\n",
      "Step 376188) Time = 121.262106\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8958018699777313e-05 | Global gradient norm: 102.79\n",
      "Step 376376) Time = 121.762171\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.892049658636097e-05 | Global gradient norm: 102.79\n",
      "Step 376564) Time = 125.921485\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.888305996544659e-05 | Global gradient norm: 102.79\n",
      "Step 376752) Time = 124.049519\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8845696104108356e-05 | Global gradient norm: 102.79\n",
      "Step 376940) Time = 118.611123\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8808406821335666e-05 | Global gradient norm: 102.79\n",
      "Step 377128) Time = 121.619236\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.87711812031921e-05 | Global gradient norm: 102.79\n",
      "Step 377316) Time = 119.804123\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.87340410775505e-05 | Global gradient norm: 102.79\n",
      "Step 377504) Time = 120.856082\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8696970073506236e-05 | Global gradient norm: 102.79\n",
      "Step 377692) Time = 120.318081\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8659966372069903e-05 | Global gradient norm: 102.79\n",
      "Step 377880) Time = 117.883079\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8623046344146132e-05 | Global gradient norm: 102.80\n",
      "Step 378068) Time = 120.065188\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.85861972568091e-05 | Global gradient norm: 102.80\n",
      "Step 378256) Time = 119.847081\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8549413653090596e-05 | Global gradient norm: 102.80\n",
      "Step 378444) Time = 116.854188\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8512708265916444e-05 | Global gradient norm: 102.80\n",
      "Step 378632) Time = 118.857135\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8476077457307838e-05 | Global gradient norm: 102.80\n",
      "Step 378820) Time = 116.416198\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.843951213231776e-05 | Global gradient norm: 102.81\n",
      "Step 379008) Time = 120.916082\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8403026842861436e-05 | Global gradient norm: 102.81\n",
      "Step 379196) Time = 125.598124\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8366610675002448e-05 | Global gradient norm: 102.81\n",
      "Step 379384) Time = 125.105201\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8330269085709006e-05 | Global gradient norm: 102.81\n",
      "Step 379572) Time = 122.876084\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.829399298003409e-05 | Global gradient norm: 102.82\n",
      "Step 379760) Time = 119.472688\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8257793271914124e-05 | Global gradient norm: 102.82\n",
      "Step 379948) Time = 119.019701\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8221668142359704e-05 | Global gradient norm: 102.82\n",
      "Step 380136) Time = 121.872930\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8185606677434407e-05 | Global gradient norm: 102.83\n",
      "Step 380324) Time = 119.706878\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8149623429053463e-05 | Global gradient norm: 102.83\n",
      "Step 380512) Time = 121.453435\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8113709302269854e-05 | Global gradient norm: 102.83\n",
      "Step 380700) Time = 122.293972\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8077860659104772e-05 | Global gradient norm: 102.84\n",
      "Step 380888) Time = 119.473081\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.8042092051473446e-05 | Global gradient norm: 102.84\n",
      "Step 381076) Time = 120.426082\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.800639074645005e-05 | Global gradient norm: 102.84\n",
      "Step 381264) Time = 119.500136\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7970754925045185e-05 | Global gradient norm: 102.84\n",
      "Step 381452) Time = 117.260263\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7935195501195267e-05 | Global gradient norm: 102.85\n",
      "Step 381640) Time = 118.482127\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7899707017932087e-05 | Global gradient norm: 102.85\n",
      "Step 381828) Time = 123.218155\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7864287656266242e-05 | Global gradient norm: 102.85\n",
      "Step 382016) Time = 122.517083\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7828933778218925e-05 | Global gradient norm: 102.85\n",
      "Step 382204) Time = 123.281326\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7793656297726557e-05 | Global gradient norm: 102.85\n",
      "Step 382392) Time = 119.839335\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7758447938831523e-05 | Global gradient norm: 102.85\n",
      "Step 382580) Time = 121.682340\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7723299606586806e-05 | Global gradient norm: 102.86\n",
      "Step 382768) Time = 121.430193\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7688231309875846e-05 | Global gradient norm: 102.86\n",
      "Step 382956) Time = 118.484207\n",
      "Train loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7653233953751624e-05 | Global gradient norm: 102.86\n",
      "Step 383144) Time = 120.283194\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.761829662427772e-05 | Global gradient norm: 102.86\n",
      "Step 383332) Time = 121.425209\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.758343387336936e-05 | Global gradient norm: 102.86\n",
      "Step 383520) Time = 116.475185\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7548642063047737e-05 | Global gradient norm: 102.87\n",
      "Step 383708) Time = 127.912294\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7513912098365836e-05 | Global gradient norm: 102.87\n",
      "Step 383896) Time = 128.693087\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.747925671224948e-05 | Global gradient norm: 102.87\n",
      "Step 384084) Time = 126.446213\n",
      "Train loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.744467044773046e-05 | Global gradient norm: 102.87\n",
      "Step 384272) Time = 121.277159\n",
      "Train loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7410155123798177e-05 | Global gradient norm: 102.87\n",
      "Step 384460) Time = 116.871188\n",
      "Train loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7375696188537404e-05 | Global gradient norm: 102.88\n",
      "Step 384648) Time = 117.825141\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7341317288810387e-05 | Global gradient norm: 102.88\n",
      "Step 384836) Time = 121.068202\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.730700023472309e-05 | Global gradient norm: 102.88\n",
      "Step 385024) Time = 116.901133\n",
      "Train loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7272748664254323e-05 | Global gradient norm: 102.88\n",
      "Step 385212) Time = 120.902150\n",
      "Train loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7238573491340503e-05 | Global gradient norm: 102.88\n",
      "Step 385400) Time = 118.828163\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7204461983055808e-05 | Global gradient norm: 102.89\n",
      "Step 385588) Time = 119.866081\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7170414139400236e-05 | Global gradient norm: 102.89\n",
      "Step 385776) Time = 120.516201\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7136437236331403e-05 | Global gradient norm: 102.89\n",
      "Step 385964) Time = 116.632211\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.710253127384931e-05 | Global gradient norm: 102.89\n",
      "Step 386152) Time = 120.475214\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.706868533801753e-05 | Global gradient norm: 102.89\n",
      "Step 386340) Time = 119.469179\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.703491034277249e-05 | Global gradient norm: 102.90\n",
      "Step 386528) Time = 116.874512\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.7001202650135383e-05 | Global gradient norm: 102.90\n",
      "Step 386716) Time = 120.698345\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.696756407909561e-05 | Global gradient norm: 102.90\n",
      "Step 386904) Time = 123.294540\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.6933981896727346e-05 | Global gradient norm: 102.90\n",
      "Step 387092) Time = 121.883447\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.6900474292924628e-05 | Global gradient norm: 102.90\n",
      "Step 387280) Time = 121.617518\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.6867032172740437e-05 | Global gradient norm: 102.90\n",
      "Step 387468) Time = 119.328040\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.6833651898195967e-05 | Global gradient norm: 102.90\n",
      "Step 387656) Time = 122.674854\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.680034438322764e-05 | Global gradient norm: 102.91\n",
      "Step 387844) Time = 127.041487\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.676710235187784e-05 | Global gradient norm: 102.91\n",
      "Step 388032) Time = 125.874213\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.6733914890210144e-05 | Global gradient norm: 102.91\n",
      "Step 388220) Time = 121.677093\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.6700807464076206e-05 | Global gradient norm: 102.91\n",
      "Step 388408) Time = 118.875149\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.6667760064592585e-05 | Global gradient norm: 102.91\n",
      "Step 388596) Time = 116.591202\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.6634774510748684e-05 | Global gradient norm: 102.91\n",
      "Step 388784) Time = 119.917081\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.660185807850212e-05 | Global gradient norm: 102.92\n",
      "Step 388972) Time = 118.671157\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.656900712987408e-05 | Global gradient norm: 102.92\n",
      "Step 389160) Time = 119.062081\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.653622166486457e-05 | Global gradient norm: 102.92\n",
      "Step 389348) Time = 121.878082\n",
      "Train loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.6503496226505376e-05 | Global gradient norm: 102.92\n",
      "Step 389536) Time = 117.467079\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.647084172873292e-05 | Global gradient norm: 102.92\n",
      "Step 389724) Time = 123.700083\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.643825089558959e-05 | Global gradient norm: 102.93\n",
      "Step 389912) Time = 119.023195\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.640571645111777e-05 | Global gradient norm: 102.93\n",
      "Step 390100) Time = 114.518121\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.637325476622209e-05 | Global gradient norm: 102.93\n",
      "Step 390288) Time = 123.464307\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.6340858564944938e-05 | Global gradient norm: 102.93\n",
      "Step 390476) Time = 125.845495\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.6308518752339296e-05 | Global gradient norm: 102.93\n",
      "Step 390664) Time = 125.517309\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.627624806133099e-05 | Global gradient norm: 102.93\n",
      "Step 390852) Time = 122.267227\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.624404285394121e-05 | Global gradient norm: 102.94\n",
      "Step 391040) Time = 48.389033\n",
      "Train loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.621189949219115e-05 | Global gradient norm: 102.94\n",
      "Step 391228) Time = 121.041082\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.617981615709141e-05 | Global gradient norm: 102.94\n",
      "Step 391416) Time = 124.797144\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.61478001245996e-05 | Global gradient norm: 102.94\n",
      "Step 391604) Time = 120.503222\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.611585139471572e-05 | Global gradient norm: 102.94\n",
      "Step 391792) Time = 117.799198\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.6083955415524542e-05 | Global gradient norm: 102.95\n",
      "Step 391980) Time = 115.836032\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.6052130376920104e-05 | Global gradient norm: 102.95\n",
      "Step 392168) Time = 115.751121\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.6020367183955386e-05 | Global gradient norm: 102.95\n",
      "Step 392356) Time = 115.021142\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.598866037966218e-05 | Global gradient norm: 102.95\n",
      "Step 392544) Time = 118.186118\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5957022696966305e-05 | Global gradient norm: 102.95\n",
      "Step 392732) Time = 117.748203\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5925448678899556e-05 | Global gradient norm: 102.96\n",
      "Step 392920) Time = 127.990211\n",
      "Train loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5893931049504317e-05 | Global gradient norm: 102.96\n",
      "Step 393108) Time = 120.099081\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5862482541706413e-05 | Global gradient norm: 102.96\n",
      "Step 393296) Time = 117.370079\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.583109587954823e-05 | Global gradient norm: 102.96\n",
      "Step 393484) Time = 119.288198\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5799769244040363e-05 | Global gradient norm: 102.96\n",
      "Step 393672) Time = 119.716081\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5768502635182813e-05 | Global gradient norm: 102.97\n",
      "Step 393860) Time = 118.086081\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.573730150994379e-05 | Global gradient norm: 102.97\n",
      "Step 394048) Time = 121.019082\n",
      "Train loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.570616223034449e-05 | Global gradient norm: 102.97\n",
      "Step 394236) Time = 119.008264\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5675077520427294e-05 | Global gradient norm: 102.97\n",
      "Step 394424) Time = 117.127255\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5644061932107434e-05 | Global gradient norm: 102.97\n",
      "Step 394612) Time = 122.746244\n",
      "Train loss = 0.00029 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.561310637043789e-05 | Global gradient norm: 102.98\n",
      "Step 394800) Time = 120.914213\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5582207197439857e-05 | Global gradient norm: 102.98\n",
      "Step 394988) Time = 120.855200\n",
      "Train loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.555137350806035e-05 | Global gradient norm: 102.98\n",
      "Step 395176) Time = 117.299079\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.552060348330997e-05 | Global gradient norm: 102.99\n",
      "Step 395364) Time = 114.364143\n",
      "Train loss = 0.00029 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.548988620925229e-05 | Global gradient norm: 102.99\n",
      "Step 395552) Time = 120.443081\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5459236237802543e-05 | Global gradient norm: 102.99\n",
      "Step 395740) Time = 124.008193\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5428646293003112e-05 | Global gradient norm: 102.99\n",
      "Step 395928) Time = 121.944166\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5398118193843402e-05 | Global gradient norm: 102.99\n",
      "Step 396116) Time = 118.568246\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5367642845376395e-05 | Global gradient norm: 103.00\n",
      "Step 396304) Time = 115.110078\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5337236618506722e-05 | Global gradient norm: 103.00\n",
      "Step 396492) Time = 119.566081\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5306888599297963e-05 | Global gradient norm: 103.00\n",
      "Step 396680) Time = 125.809717\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5276593330781907e-05 | Global gradient norm: 103.00\n",
      "Step 396868) Time = 127.252639\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5246368093357887e-05 | Global gradient norm: 103.00\n",
      "Step 397056) Time = 124.959338\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5216199244605377e-05 | Global gradient norm: 103.00\n",
      "Step 397244) Time = 127.518956\n",
      "Train loss = 0.00033 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00032 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5186084056040272e-05 | Global gradient norm: 103.01\n",
      "Step 397432) Time = 124.257950\n",
      "Train loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5156035260588396e-05 | Global gradient norm: 103.01\n",
      "Step 397620) Time = 124.666582\n",
      "Train loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5126046491786838e-05 | Global gradient norm: 103.01\n",
      "Step 397808) Time = 121.436862\n",
      "Train loss = 0.00029 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5096111383172683e-05 | Global gradient norm: 103.01\n",
      "Step 397996) Time = 121.198425\n",
      "Train loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5066241758177057e-05 | Global gradient norm: 103.02\n",
      "Step 398184) Time = 125.047539\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5036429431347642e-05 | Global gradient norm: 103.02\n",
      "Step 398372) Time = 121.017853\n",
      "Train loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.5006677131168544e-05 | Global gradient norm: 103.02\n",
      "Step 398560) Time = 123.670009\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.4976976672187448e-05 | Global gradient norm: 103.02\n",
      "Step 398748) Time = 122.399822\n",
      "Train loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00031 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.4947341696824878e-05 | Global gradient norm: 103.02\n",
      "Step 398936) Time = 120.483977\n",
      "Train loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.4917766748112626e-05 | Global gradient norm: 103.02\n",
      "Step 399124) Time = 121.911636\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.4888241821608972e-05 | Global gradient norm: 103.03\n",
      "Step 399312) Time = 120.140293\n",
      "Train loss = 0.00031 | mse = 0.00029 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.4858782378723845e-05 | Global gradient norm: 103.03\n",
      "Step 399500) Time = 127.015032\n",
      "Train loss = 0.00034 | mse = 0.00033 | KL = 0.00002\n",
      "Validation loss = 0.00034 | mse = 0.00033 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.4829381143499631e-05 | Global gradient norm: 103.03\n",
      "Step 399688) Time = 121.780250\n",
      "Train loss = 0.00031 | mse = 0.00030 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.480003265896812e-05 | Global gradient norm: 103.03\n",
      "Step 399876) Time = 119.772259\n",
      "Train loss = 0.00032 | mse = 0.00031 | KL = 0.00002\n",
      "Validation loss = 0.00032 | mse = 0.00030 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.4770747839065734e-05 | Global gradient norm: 103.03\n",
      "Step 400064) Time = 123.569360\n",
      "Train loss = 0.00029 | mse = 0.00027 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00029 | KL = 0.00002\n",
      "================================================\n",
      "Learning rate: 1.4741522136318963e-05 | Global gradient norm: 103.03\n",
      "Step 400252) Time = 123.405447\n",
      "Train loss = 0.00029 | mse = 0.00028 | KL = 0.00002\n",
      "Validation loss = 0.00030 | mse = 0.00028 | KL = 0.00002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m summary_writer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mcreate_file_writer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/path/to/log_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m summary_writer\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (x_seq, y_seq, m_seq) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tf_x_train_miss\u001b[38;5;241m.\u001b[39mtake(num_steps)):\n\u001b[0;32m     26\u001b[0m         loss \u001b[38;5;241m=\u001b[39m train_step(x_seq, y_seq, m_seq, model, optimizer)\n\u001b[0;32m     27\u001b[0m         losses_train\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:747\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    746\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    748\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:730\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 730\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    735\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:2573\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   2572\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2573\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2574\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIteratorGetNext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2575\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   2577\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "Learning rate: 0.00078568299068138 | Global gradient norm: 127.05\n",
      "Step 22892) Time = 586.584012\n",
      "Train loss = 0.00203 | mse = 0.00134 | KL = 0.00069\n",
      "Validation loss = 0.00184 | mse = 0.00117 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007837318116798997 | Global gradient norm: 127.10\n",
      "Step 23128) Time = 584.468661\n",
      "Train loss = 0.00134 | mse = 0.00066 | KL = 0.00068\n",
      "Validation loss = 0.00118 | mse = 0.00051 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007817854639142752 | Global gradient norm: 127.15\n",
      "Step 23364) Time = 585.365923\n",
      "Train loss = 0.00136 | mse = 0.00068 | KL = 0.00068\n",
      "Validation loss = 0.00117 | mse = 0.00050 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007798439473845065 | Global gradient norm: 127.21\n",
      "Step 23600) Time = 584.075753\n",
      "Train loss = 0.00132 | mse = 0.00064 | KL = 0.00068\n",
      "Validation loss = 0.00114 | mse = 0.00047 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007779072620905936 | Global gradient norm: 127.27\n",
      "Step 23836) Time = 580.405240\n",
      "Train loss = 0.00132 | mse = 0.00064 | KL = 0.00068\n",
      "Validation loss = 0.00113 | mse = 0.00046 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007759754662401974 | Global gradient norm: 127.33\n",
      "Step 24072) Time = 609.463936\n",
      "Train loss = 0.00134 | mse = 0.00066 | KL = 0.00068\n",
      "Validation loss = 0.00114 | mse = 0.00048 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007740483852103353 | Global gradient norm: 127.38\n",
      "Step 24308) Time = 553.832830\n",
      "Train loss = 0.00132 | mse = 0.00065 | KL = 0.00068\n",
      "Validation loss = 0.00113 | mse = 0.00047 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007721261354163289 | Global gradient norm: 127.44\n",
      "Step 24544) Time = 524.508750\n",
      "Train loss = 0.00133 | mse = 0.00065 | KL = 0.00068\n",
      "Validation loss = 0.00114 | mse = 0.00047 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007702086004428566 | Global gradient norm: 127.50\n",
      "Step 24780) Time = 572.961956\n",
      "Train loss = 0.00132 | mse = 0.00064 | KL = 0.00068\n",
      "Validation loss = 0.00114 | mse = 0.00047 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007682957802899182 | Global gradient norm: 127.54\n",
      "Step 25016) Time = 587.118385\n",
      "Train loss = 0.00132 | mse = 0.00064 | KL = 0.00068\n",
      "Validation loss = 0.00114 | mse = 0.00047 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007663878495804965 | Global gradient norm: 127.60\n",
      "Step 25252) Time = 579.437119\n",
      "Train loss = 0.00132 | mse = 0.00064 | KL = 0.00068\n",
      "Validation loss = 0.00114 | mse = 0.00047 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.000764484575483948 | Global gradient norm: 127.66\n",
      "Step 25488) Time = 593.966016\n",
      "Train loss = 0.00132 | mse = 0.00064 | KL = 0.00068\n",
      "Validation loss = 0.00114 | mse = 0.00048 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007625860162079334 | Global gradient norm: 127.69\n",
      "Step 25724) Time = 587.739375\n",
      "Train loss = 0.00131 | mse = 0.00063 | KL = 0.00068\n",
      "Validation loss = 0.00115 | mse = 0.00048 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007606922299601138 | Global gradient norm: 127.75\n",
      "Step 25960) Time = 602.363719\n",
      "Train loss = 0.00132 | mse = 0.00065 | KL = 0.00068\n",
      "Validation loss = 0.00114 | mse = 0.00048 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007588031003251672 | Global gradient norm: 127.81\n",
      "Step 26196) Time = 561.924501\n",
      "Train loss = 0.00135 | mse = 0.00067 | KL = 0.00068\n",
      "Validation loss = 0.00115 | mse = 0.00049 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007569186273030937 | Global gradient norm: 127.86\n",
      "Step 26432) Time = 559.834189\n",
      "Train loss = 0.00129 | mse = 0.00061 | KL = 0.00068\n",
      "Validation loss = 0.00113 | mse = 0.00046 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007550389273092151 | Global gradient norm: 127.92\n",
      "Step 26668) Time = 556.167774\n",
      "Train loss = 0.00132 | mse = 0.00064 | KL = 0.00068\n",
      "Validation loss = 0.00115 | mse = 0.00048 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007531637675128877 | Global gradient norm: 127.97\n",
      "Step 26904) Time = 532.538564\n",
      "Train loss = 0.00139 | mse = 0.00071 | KL = 0.00068\n",
      "Validation loss = 0.00120 | mse = 0.00053 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007512933807447553 | Global gradient norm: 128.03\n",
      "Step 27140) Time = 586.473220\n",
      "Train loss = 0.00130 | mse = 0.00063 | KL = 0.00068\n",
      "Validation loss = 0.00112 | mse = 0.00046 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007494276505894959 | Global gradient norm: 128.08\n",
      "Step 27376) Time = 583.579106\n",
      "Train loss = 0.00133 | mse = 0.00065 | KL = 0.00068\n",
      "Validation loss = 0.00116 | mse = 0.00049 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007475664606317878 | Global gradient norm: 128.14\n",
      "Step 27612) Time = 581.621736\n",
      "Train loss = 0.00132 | mse = 0.00064 | KL = 0.00068\n",
      "Validation loss = 0.00112 | mse = 0.00046 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007457099272869527 | Global gradient norm: 128.19\n",
      "Step 27848) Time = 583.339363\n",
      "Train loss = 0.00130 | mse = 0.00062 | KL = 0.00068\n",
      "Validation loss = 0.00112 | mse = 0.00046 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007438580505549908 | Global gradient norm: 128.25\n",
      "Step 28084) Time = 584.037937\n",
      "Train loss = 0.00133 | mse = 0.00066 | KL = 0.00067\n",
      "Validation loss = 0.00114 | mse = 0.00047 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.000742010772228241 | Global gradient norm: 128.30\n",
      "Step 28320) Time = 599.155843\n",
      "Train loss = 0.00133 | mse = 0.00066 | KL = 0.00068\n",
      "Validation loss = 0.00114 | mse = 0.00047 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007401679758913815 | Global gradient norm: 128.35\n",
      "Step 28556) Time = 562.657040\n",
      "Train loss = 0.00131 | mse = 0.00063 | KL = 0.00068\n",
      "Validation loss = 0.00113 | mse = 0.00046 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007383298361673951 | Global gradient norm: 128.40\n",
      "Step 28792) Time = 546.321193\n",
      "Train loss = 0.00131 | mse = 0.00063 | KL = 0.00067\n",
      "Validation loss = 0.00112 | mse = 0.00046 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007364962948486209 | Global gradient norm: 128.46\n",
      "Step 29028) Time = 557.605049\n",
      "Train loss = 0.00131 | mse = 0.00064 | KL = 0.00067\n",
      "Validation loss = 0.00114 | mse = 0.00047 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007346672937273979 | Global gradient norm: 128.51\n",
      "Step 29264) Time = 551.021057\n",
      "Train loss = 0.00131 | mse = 0.00064 | KL = 0.00068\n",
      "Validation loss = 0.00113 | mse = 0.00047 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007328427745960653 | Global gradient norm: 128.56\n",
      "Step 29500) Time = 580.408635\n",
      "Train loss = 0.00132 | mse = 0.00065 | KL = 0.00068\n",
      "Validation loss = 0.00113 | mse = 0.00047 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007310227956622839 | Global gradient norm: 128.61\n",
      "Step 29736) Time = 584.255360\n",
      "Train loss = 0.00133 | mse = 0.00066 | KL = 0.00068\n",
      "Validation loss = 0.00115 | mse = 0.00049 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007292073569260538 | Global gradient norm: 128.66\n",
      "Step 29972) Time = 597.361985\n",
      "Train loss = 0.00132 | mse = 0.00065 | KL = 0.00067\n",
      "Validation loss = 0.00113 | mse = 0.00047 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007273964583873749 | Global gradient norm: 128.72\n",
      "Step 30208) Time = 578.889204\n",
      "Train loss = 0.00135 | mse = 0.00067 | KL = 0.00068\n",
      "Validation loss = 0.00117 | mse = 0.00051 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007255900418385863 | Global gradient norm: 128.77\n",
      "Step 30444) Time = 589.876625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.00132 | mse = 0.00064 | KL = 0.00067\n",
      "Validation loss = 0.00114 | mse = 0.00047 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007237881072796881 | Global gradient norm: 128.82\n",
      "Step 30680) Time = 603.148640\n",
      "Train loss = 0.00132 | mse = 0.00064 | KL = 0.00068\n",
      "Validation loss = 0.00115 | mse = 0.00048 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007219905382953584 | Global gradient norm: 128.87\n",
      "Step 30916) Time = 556.050590\n",
      "Train loss = 0.00131 | mse = 0.00064 | KL = 0.00067\n",
      "Validation loss = 0.00113 | mse = 0.00047 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007201976259239018 | Global gradient norm: 128.92\n",
      "Step 31152) Time = 547.728974\n",
      "Train loss = 0.00131 | mse = 0.00063 | KL = 0.00067\n",
      "Validation loss = 0.00113 | mse = 0.00046 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007184090791270137 | Global gradient norm: 128.98\n",
      "Step 31388) Time = 548.024381\n",
      "Train loss = 0.00136 | mse = 0.00069 | KL = 0.00067\n",
      "Validation loss = 0.00119 | mse = 0.00053 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.000716624956112355 | Global gradient norm: 129.00\n",
      "Step 31624) Time = 550.497437\n",
      "Train loss = 0.00130 | mse = 0.00062 | KL = 0.00068\n",
      "Validation loss = 0.00112 | mse = 0.00045 | KL = 0.00067\n",
      "================================================\n",
      "Learning rate: 0.0007148452568799257 | Global gradient norm: 129.05\n",
      "Step 31860) Time = 553.755980\n",
      "Train loss = 0.00133 | mse = 0.00066 | KL = 0.00067\n",
      "Validation loss = 0.00117 | mse = 0.00050 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007130699814297259 | Global gradient norm: 129.10\n",
      "Step 32096) Time = 542.942646\n",
      "Train loss = 0.00132 | mse = 0.00064 | KL = 0.00067\n",
      "Validation loss = 0.00113 | mse = 0.00047 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007112991297617555 | Global gradient norm: 129.15\n",
      "Step 32332) Time = 556.842921\n",
      "Train loss = 0.00133 | mse = 0.00066 | KL = 0.00067\n",
      "Validation loss = 0.00116 | mse = 0.00050 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007095326436683536 | Global gradient norm: 129.20\n",
      "Step 32568) Time = 549.038081\n",
      "Train loss = 0.00138 | mse = 0.00071 | KL = 0.00067\n",
      "Validation loss = 0.00120 | mse = 0.00054 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007077705813571811 | Global gradient norm: 129.25\n",
      "Step 32804) Time = 550.747153\n",
      "Train loss = 0.00128 | mse = 0.00061 | KL = 0.00067\n",
      "Validation loss = 0.00111 | mse = 0.00044 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.000706012942828238 | Global gradient norm: 129.30\n",
      "Step 33040) Time = 552.876409\n",
      "Train loss = 0.00132 | mse = 0.00064 | KL = 0.00067\n",
      "Validation loss = 0.00113 | mse = 0.00047 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007042596116662025 | Global gradient norm: 129.35\n",
      "Step 33276) Time = 558.938993\n",
      "Train loss = 0.00130 | mse = 0.00063 | KL = 0.00067\n",
      "Validation loss = 0.00112 | mse = 0.00046 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007025105878710747 | Global gradient norm: 129.39\n",
      "Step 33512) Time = 528.860641\n",
      "Train loss = 0.00133 | mse = 0.00065 | KL = 0.00067\n",
      "Validation loss = 0.00114 | mse = 0.00047 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0007007659878581762 | Global gradient norm: 129.44\n",
      "Step 33748) Time = 581.119629\n",
      "Train loss = 0.00129 | mse = 0.00061 | KL = 0.00067\n",
      "Validation loss = 0.00110 | mse = 0.00044 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0006990256370045245 | Global gradient norm: 129.49\n",
      "Step 33984) Time = 590.040094\n",
      "Train loss = 0.00130 | mse = 0.00063 | KL = 0.00067\n",
      "Validation loss = 0.00113 | mse = 0.00046 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0006972897099331021 | Global gradient norm: 129.54\n",
      "Step 34220) Time = 578.683061\n",
      "Train loss = 0.00132 | mse = 0.00065 | KL = 0.00067\n",
      "Validation loss = 0.00114 | mse = 0.00047 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0006955580320209265 | Global gradient norm: 129.58\n",
      "Step 34456) Time = 589.905820\n",
      "Train loss = 0.00129 | mse = 0.00062 | KL = 0.00067\n",
      "Validation loss = 0.00113 | mse = 0.00047 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0006938306614756584 | Global gradient norm: 129.63\n",
      "Step 34692) Time = 582.818930\n",
      "Train loss = 0.00129 | mse = 0.00062 | KL = 0.00067\n",
      "Validation loss = 0.00112 | mse = 0.00046 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0006921076565049589 | Global gradient norm: 129.68\n",
      "Step 34928) Time = 591.394912\n",
      "Train loss = 0.00132 | mse = 0.00064 | KL = 0.00067\n",
      "Validation loss = 0.00113 | mse = 0.00047 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.0006903888424858451 | Global gradient norm: 129.72\n",
      "Step 35164) Time = 544.211528\n",
      "Train loss = 0.00131 | mse = 0.00064 | KL = 0.00067\n",
      "Validation loss = 0.00112 | mse = 0.00045 | KL = 0.00066\n",
      "================================================\n",
      "Learning rate: 0.000688674277625978 | Global gradient norm: 129.77\n",
      "Step 35400) Time = 546.781361\n",
      "Train loss = 0.00139 | mse = 0.00072 | KL = 0.00067\n",
      "Validation loss = 0.00121 | mse = 0.00054 | KL = 0.00066\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function for a single training step\n",
    "@tf.function\n",
    "def train_step(x_seq, y_seq, m_seq, model, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = model.compute_loss(x_seq, y_seq, m_mask=m_seq)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Gradient Clipping\n",
    "    clipped_grads = [tf.clip_by_value(grad, -gradient_clip, gradient_clip) for grad in gradients]\n",
    "    optimizer.apply_gradients(zip(clipped_grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# To store losses\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "\n",
    "# For best model saving\n",
    "val_loss_check = 0.001\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# For TensorBoard visualization\n",
    "summary_writer = tf.summary.create_file_writer(\"/path/to/log_dir\")\n",
    "\n",
    "with summary_writer.as_default():\n",
    "    for i, (x_seq, y_seq, m_seq) in enumerate(tf_x_train_miss.take(num_steps)):\n",
    "        loss = train_step(x_seq, y_seq, m_seq, model, optimizer)\n",
    "        losses_train.append(loss.numpy())\n",
    "\n",
    "        if i % print_interval == 0:\n",
    "            print(\"================================================\")\n",
    "            print(\"Learning rate: {} | Global gradient norm: {:.2f}\".format(\n",
    "                optimizer._decayed_lr('float32'), \n",
    "                tf.linalg.global_norm(model.trainable_variables))\n",
    "            )\n",
    "            print(\"Step {}) Time = {:2f}\".format(i, time.time() - t0))\n",
    "            loss, mse, kl = model.compute_loss(x_seq, y_seq, m_mask=m_seq, return_parts=True)\n",
    "            print(\"Train loss = {:.5f} | mse = {:.5f} | KL = {:.5f}\".format(loss, mse, kl))\n",
    "            \n",
    "            tf.summary.scalar(\"loss_train\", loss, step=i)\n",
    "            tf.summary.scalar(\"kl_train\", kl, step=i)\n",
    "            tf.summary.scalar(\"mse_train\", mse, step=i)\n",
    "\n",
    "            # Validation\n",
    "            random_indices = np.random.choice(len(Val_X), size=batch_size, replace=False)\n",
    "            random_batch_X = Val_X[random_indices]\n",
    "            random_batch_Y = Val_Y[random_indices]\n",
    "            random_batch_m = m_val_miss[random_indices]\n",
    "            val_loss, val_mse, val_kl = model.compute_loss(random_batch_X, random_batch_Y, m_mask=random_batch_m, return_parts=True)\n",
    "            losses_val.append(val_loss.numpy())\n",
    "            \n",
    "            print(\"Validation loss = {:.5f} | mse = {:.5f} | KL = {:.5f}\".format(val_loss, val_mse, val_kl))\n",
    "\n",
    "            tf.summary.scalar(\"loss_val\", val_loss, step=i)\n",
    "            tf.summary.scalar(\"kl_val\", val_kl, step=i)\n",
    "            tf.summary.scalar(\"mse_val\", val_mse, step=i)\n",
    "\n",
    "            if val_loss_check > val_loss:\n",
    "                val_loss_check = val_loss\n",
    "                model.encoder.net.save_weights(outdir+'encoder_hivae_val'+str(val_loss.numpy())+'_valmse'+str(val_mse.numpy())+'.hdf5')\n",
    "                model.decoder.net.save_weights(outdir+'decoder_hivae_val'+str(val_loss.numpy())+'_valmse'+str(val_mse.numpy())+'.hdf5')\n",
    "#                 # Stop training if val_mse goes below 0.00043\n",
    "#                 if val_mse < 0.00043:\n",
    "#                     print(\"Stopping training as val_mse reached below 0.00043.\")\n",
    "#                     break\n",
    "\n",
    "            t0 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24bc8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc540525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b767ac8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea302cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46081684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
