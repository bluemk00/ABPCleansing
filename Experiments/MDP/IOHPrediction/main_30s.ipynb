{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4acfb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "def load_and_prepare_data(sec, data, name, dataset, dtw_version, config):\n",
    "    data_root = config['data_root']\n",
    "    paths = config['paths']\n",
    "    dtw_suffix = '_withoutDTW' if dtw_version == 'without_DTW' else ''\n",
    "    dtw_prefix = '' if dtw_version == 'without_DTW' else 'DTW_'\n",
    "\n",
    "    if name == 'uncleaned':\n",
    "        base_path = paths['uncleaned']['base_path'].format(data_root=data_root, dataset=dataset)\n",
    "        hypo_path = os.path.join(base_path, paths['uncleaned']['file_pattern']['hypo'].format(data=data, sec=sec))\n",
    "        nonhypo_path = os.path.join(base_path, paths['uncleaned']['file_pattern']['nonhypo'].format(data=data, sec=sec))\n",
    "        normed = False\n",
    "    else:\n",
    "        base_path = paths['others']['base_path'].format(data_root=data_root, dataset=dataset, dtw_suffix=dtw_suffix)\n",
    "        hypo_path = os.path.join(base_path, paths['others']['file_pattern']['hypo'].format(name=name, data=data, sec=sec, dtw_prefix=dtw_prefix))\n",
    "        nonhypo_path = os.path.join(base_path, paths['others']['file_pattern']['nonhypo'].format(name=name, data=data, sec=sec, dtw_prefix=dtw_prefix))\n",
    "        normed = True\n",
    "\n",
    "    hypo_in = np.load(hypo_path)[:500]\n",
    "    nonhypo_in = np.load(nonhypo_path)[:500]\n",
    "\n",
    "    if not normed:\n",
    "        if dataset == 'vitaldb' and name == 'uncleaned':\n",
    "            hypo_in = hypo_in * 180.0 / 200.0\n",
    "            nonhypo_in = nonhypo_in * 180.0 / 200.0\n",
    "        else:\n",
    "            hypo_in = (hypo_in - 20.0) / 200.0\n",
    "            nonhypo_in = (nonhypo_in - 20.0) / 200.0\n",
    "\n",
    "    if data == 'Noise':\n",
    "        if name == 'uncleaned':\n",
    "            noise_base_path = paths['uncleaned']['noise']['base_path'].format(data_root=data_root, dataset=dataset)\n",
    "            noise_hypo_path = os.path.join(noise_base_path, paths['uncleaned']['noise']['file_pattern']['hypo'].format(sec=sec))\n",
    "            noise_nonhypo_path = os.path.join(noise_base_path, paths['uncleaned']['noise']['file_pattern']['nonhypo'].format(sec=sec))\n",
    "            normed = False\n",
    "        else:\n",
    "            noise_base_path = paths['others']['noise']['base_path'].format(data_root=data_root, dataset=dataset)\n",
    "            noise_hypo_path = os.path.join(noise_base_path, paths['others']['noise']['file_pattern'][dtw_version]['hypo'].format(name=name, sec=sec, dtw_prefix=dtw_prefix))\n",
    "            noise_nonhypo_path = os.path.join(noise_base_path, paths['others']['noise']['file_pattern'][dtw_version]['nonhypo'].format(name=name, sec=sec, dtw_prefix=dtw_prefix))\n",
    "\n",
    "        noise_hypo_in = np.load(noise_hypo_path)\n",
    "        noise_nonhypo_in = np.load(noise_nonhypo_path)\n",
    "\n",
    "        if not normed:\n",
    "            noise_hypo_in = (noise_hypo_in - 20.0) / 200.0\n",
    "            noise_nonhypo_in = (noise_nonhypo_in - 20.0) / 200.0\n",
    "\n",
    "        if sec != '30s':\n",
    "            idx_base_path = paths['uncleaned' if name == 'uncleaned' else 'others']['idx']['base_path'].format(data_root=data_root, dataset=dataset)\n",
    "            idx_hypo_path = os.path.join(idx_base_path, paths['uncleaned' if name == 'uncleaned' else 'others']['idx']['file_pattern']['hypo'].format(ids=paths['uncleaned' if name == 'uncleaned' else 'others']['idx']['ids'][dataset], sec=sec))\n",
    "            idx_nonhypo_path = os.path.join(idx_base_path, paths['uncleaned' if name == 'uncleaned' else 'others']['idx']['file_pattern']['nonhypo'].format(ids=paths['uncleaned' if name == 'uncleaned' else 'others']['idx']['ids'][dataset], sec=sec))\n",
    "\n",
    "            idx_hypo = np.load(idx_hypo_path)\n",
    "            idx_nonhypo = np.load(idx_nonhypo_path)\n",
    "\n",
    "            selected_noise_hypo_in = noise_hypo_in[idx_hypo]\n",
    "            selected_noise_nonhypo_in = noise_nonhypo_in[idx_nonhypo]\n",
    "        else:\n",
    "            selected_noise_hypo_in = noise_hypo_in\n",
    "            selected_noise_nonhypo_in = noise_nonhypo_in\n",
    "\n",
    "        All_hypo_in = np.concatenate((hypo_in, selected_noise_hypo_in), axis=0)\n",
    "        All_nonhypo_in = np.concatenate((nonhypo_in, selected_noise_nonhypo_in), axis=0)\n",
    "    else:\n",
    "        All_hypo_in = hypo_in\n",
    "        All_nonhypo_in = nonhypo_in\n",
    "\n",
    "    hypo_out = np.ones(All_hypo_in.shape[0])\n",
    "    nonhypo_out = np.zeros(All_nonhypo_in.shape[0])\n",
    "\n",
    "    All_hypo_in = np.clip(All_hypo_in, 0.0, 1.0)\n",
    "    All_nonhypo_in = np.clip(All_nonhypo_in, 0.0, 1.0)\n",
    "\n",
    "    return All_hypo_in, All_nonhypo_in, hypo_out, nonhypo_out\n",
    "\n",
    "def shuffle_and_combine_data(All_hypo_in, All_nonhypo_in, hypo_out, nonhypo_out):\n",
    "    hypo_pairs = list(zip(All_hypo_in, hypo_out))\n",
    "    nonhypo_pairs = list(zip(All_nonhypo_in, nonhypo_out))\n",
    "    combined_pairs = hypo_pairs + nonhypo_pairs\n",
    "    np.random.shuffle(combined_pairs)\n",
    "    shuffled_in = np.array([pair[0] for pair in combined_pairs])\n",
    "    shuffled_out = np.array([pair[1] for pair in combined_pairs])\n",
    "    return shuffled_in, shuffled_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2dab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from data_utils import load_config, load_and_prepare_data, shuffle_and_combine_data\n",
    "\n",
    "def evaluate_model(sec, data, name, dataset, dtw_version, config):\n",
    "    hypo_in, nonhypo_in, hypo_out, nonhypo_out = load_and_prepare_data(sec, data, name, dataset, dtw_version, config)\n",
    "    shuffled_in, shuffled_out = shuffle_and_combine_data(hypo_in, nonhypo_in, hypo_out, nonhypo_out)\n",
    "\n",
    "    if sec == '30s':\n",
    "        shuffled_in = shuffled_in[:, -3000:]\n",
    "    elif sec == '60s':\n",
    "        shuffled_in = shuffled_in[:, -6000:]\n",
    "\n",
    "    shuffled_in = np.expand_dims(shuffled_in, axis=-1)\n",
    "\n",
    "    model_paths = config['model']['paths'][sec]\n",
    "    all_model_auc_scores = []\n",
    "\n",
    "    for path in model_paths:\n",
    "        loaded_model = load_model(path)\n",
    "        model_auc_scores = []\n",
    "\n",
    "        for i in range(10):\n",
    "            y_pred = loaded_model.predict(shuffled_in)\n",
    "            auc = roc_auc_score(shuffled_out, y_pred)\n",
    "            model_auc_scores.append(auc)\n",
    "\n",
    "        avg_model_auc = np.mean(model_auc_scores)\n",
    "        all_model_auc_scores.append(avg_model_auc)\n",
    "\n",
    "    avg_auc_all_models = np.mean(all_model_auc_scores)\n",
    "    return avg_auc_all_models\n",
    "\n",
    "def save_results_to_csv(results, output_path):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    output_file = os.path.join(output_path, 'auc_results.csv')\n",
    "\n",
    "    sorted_results = sorted(results, key=lambda x: (x[4], x[3]))\n",
    "\n",
    "    with open(output_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        current_dtw = None\n",
    "        current_dataset = None\n",
    "\n",
    "        for result in sorted_results:\n",
    "            model, sec, data, dataset, dtw, auc = result\n",
    "\n",
    "            if dtw != current_dtw:\n",
    "                current_dtw = dtw\n",
    "                writer.writerow([])\n",
    "                writer.writerow([f'DTW Version: {dtw}'])\n",
    "\n",
    "            if dataset != current_dataset:\n",
    "                current_dataset = dataset\n",
    "                writer.writerow([f'DataSet: {dataset}'])\n",
    "\n",
    "            writer.writerow([model, sec, data, auc])\n",
    "\n",
    "    print(f'Results saved to: {output_file}')\n",
    "\n",
    "def main():\n",
    "    config_path = 'config.yaml'\n",
    "    config = load_config(config_path)\n",
    "\n",
    "    results = []\n",
    "    for sec in config['sec']:\n",
    "        for data in config['data_type']:\n",
    "            for model in config['name_list']:\n",
    "                model_results = []\n",
    "                for dataset in config['dataset']:\n",
    "                    for dtw_version in config['dtw_version']:\n",
    "                        avg_auc = evaluate_model(sec, data, model, dataset, dtw_version, config)\n",
    "                        result = [model, sec, data, dataset, dtw_version, avg_auc]\n",
    "                        model_results.append(result)\n",
    "                results.extend(model_results)\n",
    "\n",
    "    output_path = './results/'\n",
    "    save_results_to_csv(results, output_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd901f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c81a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a67c1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfa862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613654e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e88e096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn import metrics\n",
    "import yaml\n",
    "import sys\n",
    "sys.path.append('./utils/')\n",
    "from model_utils_90s import create_model \n",
    "import csv\n",
    "\n",
    "# with open('config_90s.yaml', 'r') as file:\n",
    "#     config = yaml.safe_load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45ad9ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = '90s'\n",
    "data_types = ['HighQual','LowQual']\n",
    "model_names = ['Baseline','DI','DI_D','DI_A','HIVAE','GPVAE','SNM','BDC']\n",
    "datasets = ['VitalDB','MIMIC3']\n",
    "\n",
    "model_path = './BestModels/90s_IOH.hdf5'\n",
    "input_layer_name = 'Input'\n",
    "output_layer_name = 'BinOut'\n",
    "input_shape = 9000\n",
    "model = create_model(input_shape) \n",
    "model.load_weights(model_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5f096a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suffle_combine_data(hypo_input, nonhypo_input, hypo_output, nonhypo_output):\n",
    "    combined_input = np.concatenate((hypo_input, nonhypo_input), axis=0)\n",
    "    combined_output = np.concatenate((hypo_output, nonhypo_output), axis=0)\n",
    "    \n",
    "    combined_indices = np.arange(len(combined_input))\n",
    "    np.random.shuffle(combined_indices)\n",
    "    abp_input = combined_input[combined_indices]\n",
    "    abp_output = combined_output[combined_indices]\n",
    "    \n",
    "    abp_input = np.clip(abp_input, 0, 1)\n",
    "    \n",
    "    return abp_input, abp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "234e39de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(abp_input):\n",
    "    pred = model.predict(abp_input[:, -9000:], batch_size=1000, verbose=1)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6686f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(abp_output, pred):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(abp_output[:], pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    confusion_matrix = metrics.confusion_matrix(abp_output, np.where(np.reshape(pred, -1) > 0.5, 1, 0))\n",
    "    return auc, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05487cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 388ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n"
     ]
    }
   ],
   "source": [
    "results = [['Dataset','Input Length','Quality', 'Model', 'AUROC']]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for data_type in data_types:\n",
    "        for model_name in model_names:\n",
    "            hypo_input = np.load(f'./ProcessedData/Evaluation/{sec}_IOH/{dataset}/{model_name}_{data_type}_Hypo.npy')\n",
    "            hypo_output = np.ones(hypo_input.shape[0])\n",
    "            nonhypo_input = np.load(f'./ProcessedData/Evaluation/{sec}_IOH/{dataset}/{model_name}_{data_type}_NonHypo.npy')\n",
    "            nonhypo_output = np.zeros(nonhypo_input.shape[0])\n",
    "\n",
    "            abp_input, abp_output = suffle_combine_data(hypo_input, nonhypo_input, hypo_output, nonhypo_output)\n",
    "            \n",
    "            pred = predict(abp_input)\n",
    "            \n",
    "            AUROC, _ = evaluate(abp_output, pred)\n",
    "            \n",
    "            results.append([dataset, sec, data_type, model_name, AUROC])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "082fc95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Dataset', 'Input Length', 'Quality', 'Model', 'AUROC'],\n",
       " ['VitalDB', '90s', 'HighQual', 'Baseline', 0.885092],\n",
       " ['VitalDB', '90s', 'HighQual', 'DI', 0.8922919999999999],\n",
       " ['VitalDB', '90s', 'HighQual', 'DI_D', 0.8906799999999999],\n",
       " ['VitalDB', '90s', 'HighQual', 'DI_A', 0.8890520000000001],\n",
       " ['VitalDB', '90s', 'HighQual', 'HIVAE', 0.87148],\n",
       " ['VitalDB', '90s', 'HighQual', 'GPVAE', 0.880716],\n",
       " ['VitalDB', '90s', 'HighQual', 'SNM', 0.877648],\n",
       " ['VitalDB', '90s', 'HighQual', 'BDC', 0.8867039999999999],\n",
       " ['VitalDB', '90s', 'LowQual', 'Baseline', 0.8200330578512396],\n",
       " ['VitalDB', '90s', 'LowQual', 'DI', 0.8546214876033058],\n",
       " ['VitalDB', '90s', 'LowQual', 'DI_D', 0.8589752066115703],\n",
       " ['VitalDB', '90s', 'LowQual', 'DI_A', 0.8603834710743802],\n",
       " ['VitalDB', '90s', 'LowQual', 'HIVAE', 0.8286396694214876],\n",
       " ['VitalDB', '90s', 'LowQual', 'GPVAE', 0.8192528925619834],\n",
       " ['VitalDB', '90s', 'LowQual', 'SNM', 0.8579371900826445],\n",
       " ['VitalDB', '90s', 'LowQual', 'BDC', 0.8210181818181819],\n",
       " ['MIMIC3', '90s', 'HighQual', 'Baseline', 0.982384],\n",
       " ['MIMIC3', '90s', 'HighQual', 'DI', 0.9819200000000001],\n",
       " ['MIMIC3', '90s', 'HighQual', 'DI_D', 0.9827840000000001],\n",
       " ['MIMIC3', '90s', 'HighQual', 'DI_A', 0.9832000000000001],\n",
       " ['MIMIC3', '90s', 'HighQual', 'HIVAE', 0.983136],\n",
       " ['MIMIC3', '90s', 'HighQual', 'GPVAE', 0.9830720000000001],\n",
       " ['MIMIC3', '90s', 'HighQual', 'SNM', 0.984896],\n",
       " ['MIMIC3', '90s', 'HighQual', 'BDC', 0.983168],\n",
       " ['MIMIC3', '90s', 'LowQual', 'Baseline', 0.9353954081632653],\n",
       " ['MIMIC3', '90s', 'LowQual', 'DI', 0.9816454081632653],\n",
       " ['MIMIC3', '90s', 'LowQual', 'DI_D', 0.9783673469387756],\n",
       " ['MIMIC3', '90s', 'LowQual', 'DI_A', 0.988890306122449],\n",
       " ['MIMIC3', '90s', 'LowQual', 'HIVAE', 0.95125],\n",
       " ['MIMIC3', '90s', 'LowQual', 'GPVAE', 0.9441071428571428],\n",
       " ['MIMIC3', '90s', 'LowQual', 'SNM', 0.9872193877551019],\n",
       " ['MIMIC3', '90s', 'LowQual', 'BDC', 0.9430867346938776]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd672df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_names[0]\n",
    "data_type = data_types[0]\n",
    "dataset = datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cca20556",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = f'./ProcessedData/Evaluation/{sec}_IOH/{dataset}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec2b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo_pattern = f'{model_name}_{data_type}_Hypo.npy'\n",
    "nonhypo_pattern = f'{model_name}_{data_type}_NonHypo.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd8a9b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './BestModels/90s_IOH.hdf5'\n",
    "input_layer_name = 'Input'\n",
    "output_layer_name = 'BinOut'\n",
    "input_shape = 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c068c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(input_shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45427a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d23eebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = './Results/'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Model', 'MAE', 'MKLD'])\n",
    "    writer.writerows(results)\n",
    "\n",
    "save_results_to_csv(results, output_path + f'/{sec}_IOH_AUROC_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ef9d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfe1a491",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo_input = np.load('./ProcessedData/Evaluation/90s_IOH/MIMIC3/Baseline_HighQual_Hypo.npy')\n",
    "hypo_output = np.ones(hypo_input.shape[0])\n",
    "nonhypo_input = np.load('./ProcessedData/Evaluation/90s_IOH/MIMIC3/Baseline_HighQual_NonHypo.npy')\n",
    "nonhypo_output = np.zeros(nonhypo_input.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ca3b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "abp_input, abp_output = suffle_combine_data(hypo_input, nonhypo_input, hypo_output, nonhypo_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa83027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "216268ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    }
   ],
   "source": [
    "pred = predict(abp_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739dd4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a26bcd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc, confusion_matrix = evaluate(abp_output, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfbe1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append([sec, model_name, data_type, dataset, auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc57a9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['90s', 'Baseline', 'HighQual', 'MIMIC3', 0.982384]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "885c1323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('HighQual', 'Baseline', 'MIMIC3', '90s')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type, model_name, dataset, sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7118a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(name, data_type, dataset, dtw_version):\n",
    "    hypo_input_path, nonhypo_input_path, hypo_input_paths, nonhypo_input_paths, hypo_index_path, nonhypo_index_path = generate_data_paths(name, data_type, sec, dataset, dtw_version)\n",
    "    \n",
    "    hypo_input = np.load(hypo_input_path)\n",
    "    nonhypo_input = np.load(nonhypo_input_path)\n",
    "    hypo_output = np.ones(hypo_input.shape[0])\n",
    "    nonhypo_output = np.zeros(nonhypo_input.shape[0])\n",
    "    \n",
    "    if data_type == 'Noise':\n",
    "        add_hypo_input = np.concatenate([np.load(path) for path in hypo_input_paths], axis=0)\n",
    "        add_nonhypo_input = np.concatenate([np.load(path) for path in nonhypo_input_paths], axis=0)\n",
    "        add_hypo_output = np.ones(add_hypo_input.shape[0])\n",
    "        add_nonhypo_output = np.zeros(add_nonhypo_input.shape[0])\n",
    "        \n",
    "        idx_hypo = np.load(hypo_index_path)\n",
    "        idx_nonhypo = np.load(nonhypo_index_path)\n",
    "        \n",
    "        if dataset == 'vitaldb' and name == 'uncleaned':\n",
    "            add_hypo_input = (add_hypo_input - 20) / 200\n",
    "            add_nonhypo_input = (add_nonhypo_input - 20) / 200\n",
    "        \n",
    "        hypo_input = np.concatenate((hypo_input, add_hypo_input[idx_hypo]), axis=0)\n",
    "        nonhypo_input = np.concatenate((nonhypo_input, add_nonhypo_input[idx_nonhypo]), axis=0)\n",
    "        hypo_output = np.concatenate((hypo_output, add_hypo_output), axis=0)\n",
    "        nonhypo_output = np.concatenate((nonhypo_output, add_nonhypo_output), axis=0)\n",
    "    \n",
    "    if name == 'uncleaned':\n",
    "        if dataset == 'vitaldb':\n",
    "            hypo_input = hypo_input * 180.0 / 200.0\n",
    "            nonhypo_input = nonhypo_input * 180.0 / 200.0\n",
    "        else:\n",
    "            hypo_input = (hypo_input - 20) / 200\n",
    "            nonhypo_input = (nonhypo_input - 20) / 200\n",
    "        \n",
    "    return hypo_input, nonhypo_input, hypo_output, nonhypo_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2ab3144",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a62126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7feff4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44accabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0794665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 12000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abp_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef8ef4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abp_output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
